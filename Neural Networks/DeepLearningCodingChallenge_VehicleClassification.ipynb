{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgsMhNCoqT/pv5iAYIXM4K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevAggarwal01/Machine-Learning/blob/main/Neural%20Networks/DeepLearningCodingChallenge_VehicleClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM6CA9WgsyRl"
      },
      "outputs": [],
      "source": [
        "!unzip vehicle_classification.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur-wG8mF1njy",
        "outputId": "7e962933-206c-4cb2-b90c-027f4e3175d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: utils in /usr/local/lib/python3.8/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image as mp_image\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
      ],
      "metadata": {
        "id": "xscWjSRp2GdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d4dd04-c085-4192-f03b-63a8a5579fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported - ready to use PyTorch 1.13.1+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_folder = '/content/vehicle_classification'\n",
        "img_size = (64, 64) # image size is 64 by 64\n",
        "\n",
        "classes = sorted(os.listdir(train_folder))\n",
        "print(classes)"
      ],
      "metadata": {
        "id": "LwTwSNdM7fO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a02bf326-722b-4971-a00e-3dec560346f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Bicycle', 'Bus', 'Car', 'Motorcycle', 'NonVehicles', 'Taxi', 'Truck', 'Van']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(data_path):\n",
        "    # Load all the images\n",
        "    transformation = transforms.Compose([\n",
        "        # Randomly augment the image data TO REDUCE OVERFITTING\n",
        "            # Random horizontal flip\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "            # Random vertical flip\n",
        "        transforms.RandomVerticalFlip(0.3),\n",
        "        # transform to tensors\n",
        "        transforms.ToTensor(),\n",
        "        # Normalize the pixel values (in R, G, and B channels)\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Load all of the images, transforming them\n",
        "    full_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=data_path,\n",
        "        transform=transformation\n",
        "    )\n",
        "    \n",
        "    \n",
        "    # Split into training (80% and testing (20%) datasets)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    \n",
        "    # use torch.utils.data.random_split for training/test split\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "    \n",
        "    # define a loader for the training data we can iterate through in 50-image batches\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=50,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    # define a loader for the testing data we can iterate through in 50-image batches\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=50,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "        \n",
        "    return train_loader, test_loader\n"
      ],
      "metadata": {
        "id": "ZMMFL3hF744F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = load_dataset(train_folder)\n",
        "batch_size = train_loader.batch_size\n",
        "print(\"Data loaders ready to read\", train_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5Lg9TTt8W0c",
        "outputId": "61bf50db-97ba-4cfe-f3b5-add87e0404fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaders ready to read /content/vehicle_classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a neural net class\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    \n",
        "    # Defining the Constructor\n",
        "    def __init__(self, num_classes=8):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # define each layer we will use in our model\n",
        "        \n",
        "        # images are RGB, so input channels = 3. \n",
        "        # apply 12 filters in the 1st convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        # 2nd convolutional layer takes 12 input channels, and generates 24 outputs\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        # apply max pooling with a kernel size of 2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
        "        self.drop = nn.Dropout2d(p=0.2)\n",
        "        \n",
        "        # 64x64 image tensors pooled twice with a kernel size of 2. 64/2/2 is 16.\n",
        "        # This means that our feature tensors are now 16 x 16, and we've generated 24 of them\n",
        "        \n",
        "        # We need to flatten these in order to feed them to a fully-connected layer\n",
        "        self.fc = nn.Linear(in_features=16 * 16 * 24, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # In the forward function, pass the data through the layers we defined in the init function\n",
        "        \n",
        "        # ReLU activation function after layer 1 (convolution 1 and pool)\n",
        "        x = F.relu(self.pool(self.conv1(x))) \n",
        "        \n",
        "        # ReLU activation function after layer 2\n",
        "        x = F.relu(self.pool(self.conv2(x)))  \n",
        "        \n",
        "        # drop some features to prevent overfitting (only drop during training)\n",
        "        x = F.dropout(self.drop(x), training=self.training)\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(-1, 16 * 16 * 24)\n",
        "        # Feed to fully-connected layer to predict class\n",
        "        x = self.fc(x)\n",
        "        # Return class probabilities via a log_softmax function \n",
        "        return torch.log_softmax(x, dim=1)\n",
        "    \n",
        "device = \"cpu\"\n",
        "if (torch.cuda.is_available()):\n",
        "    # if GPU available, use cuda\n",
        "    device = \"cuda\"\n",
        "\n",
        "# Create an instance of the model class and allocate it to the device\n",
        "model = Net(num_classes=len(classes)).to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4mOp7VL8puj",
        "outputId": "f9834fa9-dd30-4a57-ecdf-7d44ff8f16ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (drop): Dropout2d(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=6144, out_features=8, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  # training mode\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  print('Epoch', epoch)\n",
        "  # Process images in batches\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Reset optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Push data forward through model layers\n",
        "    output = model(data)\n",
        "\n",
        "    # Get loss\n",
        "    loss = loss_criteria(output, target)\n",
        "\n",
        "    # Keep a running total\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # Backpropoagate\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print metrics to see progress\n",
        "    print('\\tTraining batch {} loss: {:.6f}'.format(batch_idx+1, loss.item()))\n",
        "\n",
        "  # return average loss for the epoch\n",
        "  avg_loss = train_loss/(batch_idx+1)\n",
        "  print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "  return avg_loss\n"
      ],
      "metadata": {
        "id": "NbAfpynq8qVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "  # evaluation mode (to avoid backpropagation or drop)\n",
        "  model.eval()\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    batch_count = 0\n",
        "    for data, target in test_loader:\n",
        "      batch_count += 1\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      \n",
        "      # Get predictions \n",
        "      output = model(data)\n",
        "\n",
        "      # Calculate loss\n",
        "      test_loss += loss_criteria(output, target).item()\n",
        "\n",
        "      # Calculate the accuracy for this batch\n",
        "      _, predicted = torch.max(output.data, 1)\n",
        "      correct += torch.sum(target==predicted).item()\n",
        "\n",
        "  avg_loss = test_loss / batch_count\n",
        "  print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        avg_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "  return avg_loss"
      ],
      "metadata": {
        "id": "XvXR7St1_K56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam optimizer used to adjust weights\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# loss criteria\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "epochs = 10\n",
        "print('Training on', device)\n",
        "for epoch in range(1, epochs+1):\n",
        "  train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "  test_loss = test(model, device, test_loader)\n",
        "  epoch_nums.append(epoch)\n",
        "  training_loss.append(train_loss)\n",
        "  validation_loss.append(test_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDb_-g31AScJ",
        "outputId": "ee526e5b-5739-4cfc-e849-fd11376d4199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cpu\n",
            "Epoch 1\n",
            "\tTraining batch 1 loss: 0.893332\n",
            "\tTraining batch 2 loss: 4.821102\n",
            "\tTraining batch 3 loss: 1.590001\n",
            "\tTraining batch 4 loss: 2.726447\n",
            "\tTraining batch 5 loss: 2.194105\n",
            "\tTraining batch 6 loss: 2.013006\n",
            "\tTraining batch 7 loss: 1.668697\n",
            "\tTraining batch 8 loss: 1.596057\n",
            "\tTraining batch 9 loss: 1.738689\n",
            "\tTraining batch 10 loss: 1.566988\n",
            "\tTraining batch 11 loss: 1.479391\n",
            "\tTraining batch 12 loss: 1.876391\n",
            "\tTraining batch 13 loss: 1.802105\n",
            "\tTraining batch 14 loss: 1.554852\n",
            "\tTraining batch 15 loss: 1.873144\n",
            "\tTraining batch 16 loss: 1.491308\n",
            "\tTraining batch 17 loss: 1.276625\n",
            "\tTraining batch 18 loss: 1.792692\n",
            "\tTraining batch 19 loss: 1.502860\n",
            "\tTraining batch 20 loss: 1.541547\n",
            "\tTraining batch 21 loss: 1.548941\n",
            "\tTraining batch 22 loss: 1.371725\n",
            "\tTraining batch 23 loss: 1.176152\n",
            "\tTraining batch 24 loss: 1.202083\n",
            "\tTraining batch 25 loss: 1.626641\n",
            "\tTraining batch 26 loss: 1.493586\n",
            "\tTraining batch 27 loss: 1.163534\n",
            "\tTraining batch 28 loss: 1.556481\n",
            "\tTraining batch 29 loss: 1.542900\n",
            "\tTraining batch 30 loss: 1.242400\n",
            "\tTraining batch 31 loss: 1.377889\n",
            "\tTraining batch 32 loss: 1.438060\n",
            "\tTraining batch 33 loss: 1.519571\n",
            "\tTraining batch 34 loss: 1.034144\n",
            "\tTraining batch 35 loss: 1.249124\n",
            "\tTraining batch 36 loss: 1.911429\n",
            "\tTraining batch 37 loss: 1.461434\n",
            "\tTraining batch 38 loss: 1.209908\n",
            "\tTraining batch 39 loss: 1.558065\n",
            "\tTraining batch 40 loss: 1.618381\n",
            "\tTraining batch 41 loss: 1.126753\n",
            "\tTraining batch 42 loss: 1.389571\n",
            "\tTraining batch 43 loss: 1.183241\n",
            "\tTraining batch 44 loss: 1.312655\n",
            "\tTraining batch 45 loss: 1.337385\n",
            "\tTraining batch 46 loss: 1.282349\n",
            "\tTraining batch 47 loss: 1.538965\n",
            "\tTraining batch 48 loss: 1.348584\n",
            "\tTraining batch 49 loss: 1.481446\n",
            "\tTraining batch 50 loss: 0.982524\n",
            "\tTraining batch 51 loss: 1.229438\n",
            "\tTraining batch 52 loss: 1.434530\n",
            "\tTraining batch 53 loss: 0.953248\n",
            "\tTraining batch 54 loss: 1.042739\n",
            "\tTraining batch 55 loss: 1.250516\n",
            "\tTraining batch 56 loss: 1.123175\n",
            "\tTraining batch 57 loss: 1.504782\n",
            "\tTraining batch 58 loss: 1.131678\n",
            "\tTraining batch 59 loss: 1.845536\n",
            "\tTraining batch 60 loss: 1.327515\n",
            "\tTraining batch 61 loss: 1.167721\n",
            "\tTraining batch 62 loss: 1.386527\n",
            "\tTraining batch 63 loss: 1.255041\n",
            "\tTraining batch 64 loss: 1.349219\n",
            "\tTraining batch 65 loss: 1.455024\n",
            "\tTraining batch 66 loss: 1.211641\n",
            "\tTraining batch 67 loss: 0.794254\n",
            "\tTraining batch 68 loss: 1.185676\n",
            "\tTraining batch 69 loss: 1.349281\n",
            "\tTraining batch 70 loss: 1.045361\n",
            "\tTraining batch 71 loss: 1.387183\n",
            "\tTraining batch 72 loss: 0.968127\n",
            "\tTraining batch 73 loss: 1.074132\n",
            "\tTraining batch 74 loss: 1.246219\n",
            "\tTraining batch 75 loss: 1.130094\n",
            "\tTraining batch 76 loss: 1.238549\n",
            "\tTraining batch 77 loss: 1.525892\n",
            "\tTraining batch 78 loss: 1.630744\n",
            "\tTraining batch 79 loss: 1.573529\n",
            "\tTraining batch 80 loss: 1.135380\n",
            "\tTraining batch 81 loss: 1.531203\n",
            "\tTraining batch 82 loss: 1.155234\n",
            "\tTraining batch 83 loss: 1.231391\n",
            "\tTraining batch 84 loss: 1.506528\n",
            "\tTraining batch 85 loss: 1.014413\n",
            "\tTraining batch 86 loss: 1.111168\n",
            "\tTraining batch 87 loss: 1.310831\n",
            "\tTraining batch 88 loss: 1.494939\n",
            "\tTraining batch 89 loss: 1.324402\n",
            "\tTraining batch 90 loss: 1.242466\n",
            "\tTraining batch 91 loss: 1.094805\n",
            "\tTraining batch 92 loss: 0.995202\n",
            "\tTraining batch 93 loss: 1.204357\n",
            "\tTraining batch 94 loss: 1.116975\n",
            "\tTraining batch 95 loss: 1.201565\n",
            "\tTraining batch 96 loss: 1.221434\n",
            "\tTraining batch 97 loss: 1.695503\n",
            "\tTraining batch 98 loss: 1.253595\n",
            "\tTraining batch 99 loss: 1.136218\n",
            "\tTraining batch 100 loss: 1.250545\n",
            "\tTraining batch 101 loss: 1.374911\n",
            "\tTraining batch 102 loss: 1.088288\n",
            "\tTraining batch 103 loss: 1.167627\n",
            "\tTraining batch 104 loss: 1.288357\n",
            "\tTraining batch 105 loss: 1.296678\n",
            "\tTraining batch 106 loss: 1.203127\n",
            "\tTraining batch 107 loss: 1.272040\n",
            "\tTraining batch 108 loss: 1.336723\n",
            "\tTraining batch 109 loss: 1.180818\n",
            "\tTraining batch 110 loss: 1.272200\n",
            "\tTraining batch 111 loss: 1.225871\n",
            "\tTraining batch 112 loss: 1.162862\n",
            "\tTraining batch 113 loss: 1.339810\n",
            "\tTraining batch 114 loss: 1.158549\n",
            "\tTraining batch 115 loss: 1.004177\n",
            "\tTraining batch 116 loss: 1.137790\n",
            "\tTraining batch 117 loss: 1.443258\n",
            "\tTraining batch 118 loss: 1.012812\n",
            "\tTraining batch 119 loss: 1.445599\n",
            "\tTraining batch 120 loss: 1.172648\n",
            "\tTraining batch 121 loss: 1.086828\n",
            "\tTraining batch 122 loss: 1.245026\n",
            "\tTraining batch 123 loss: 1.057942\n",
            "\tTraining batch 124 loss: 1.062916\n",
            "\tTraining batch 125 loss: 1.085731\n",
            "\tTraining batch 126 loss: 1.055078\n",
            "\tTraining batch 127 loss: 1.195220\n",
            "\tTraining batch 128 loss: 1.265022\n",
            "\tTraining batch 129 loss: 1.176663\n",
            "\tTraining batch 130 loss: 1.269557\n",
            "\tTraining batch 131 loss: 1.237498\n",
            "\tTraining batch 132 loss: 0.964994\n",
            "\tTraining batch 133 loss: 1.151622\n",
            "\tTraining batch 134 loss: 1.351510\n",
            "\tTraining batch 135 loss: 1.356136\n",
            "\tTraining batch 136 loss: 1.182935\n",
            "\tTraining batch 137 loss: 1.282898\n",
            "\tTraining batch 138 loss: 0.981780\n",
            "\tTraining batch 139 loss: 1.218437\n",
            "\tTraining batch 140 loss: 1.115513\n",
            "\tTraining batch 141 loss: 1.091321\n",
            "\tTraining batch 142 loss: 1.047843\n",
            "\tTraining batch 143 loss: 1.188780\n",
            "\tTraining batch 144 loss: 1.146207\n",
            "\tTraining batch 145 loss: 1.275126\n",
            "\tTraining batch 146 loss: 1.382515\n",
            "\tTraining batch 147 loss: 1.484277\n",
            "\tTraining batch 148 loss: 1.035551\n",
            "\tTraining batch 149 loss: 1.046160\n",
            "\tTraining batch 150 loss: 1.003792\n",
            "\tTraining batch 151 loss: 0.831806\n",
            "\tTraining batch 152 loss: 1.142413\n",
            "\tTraining batch 153 loss: 1.187503\n",
            "\tTraining batch 154 loss: 1.201200\n",
            "\tTraining batch 155 loss: 0.858086\n",
            "\tTraining batch 156 loss: 1.157821\n",
            "\tTraining batch 157 loss: 0.930969\n",
            "\tTraining batch 158 loss: 1.116756\n",
            "\tTraining batch 159 loss: 0.843800\n",
            "\tTraining batch 160 loss: 1.090560\n",
            "\tTraining batch 161 loss: 0.993859\n",
            "\tTraining batch 162 loss: 1.090046\n",
            "\tTraining batch 163 loss: 1.259808\n",
            "\tTraining batch 164 loss: 1.187422\n",
            "\tTraining batch 165 loss: 0.927931\n",
            "\tTraining batch 166 loss: 0.949033\n",
            "\tTraining batch 167 loss: 1.012770\n",
            "\tTraining batch 168 loss: 1.296346\n",
            "\tTraining batch 169 loss: 1.094512\n",
            "\tTraining batch 170 loss: 1.056391\n",
            "\tTraining batch 171 loss: 1.183542\n",
            "\tTraining batch 172 loss: 1.142191\n",
            "\tTraining batch 173 loss: 1.259255\n",
            "\tTraining batch 174 loss: 1.099881\n",
            "\tTraining batch 175 loss: 1.278424\n",
            "\tTraining batch 176 loss: 1.116681\n",
            "\tTraining batch 177 loss: 1.306004\n",
            "\tTraining batch 178 loss: 1.045151\n",
            "\tTraining batch 179 loss: 1.369263\n",
            "\tTraining batch 180 loss: 1.326071\n",
            "\tTraining batch 181 loss: 1.160782\n",
            "\tTraining batch 182 loss: 1.254459\n",
            "\tTraining batch 183 loss: 1.207313\n",
            "\tTraining batch 184 loss: 1.472260\n",
            "\tTraining batch 185 loss: 0.711901\n",
            "\tTraining batch 186 loss: 1.224703\n",
            "\tTraining batch 187 loss: 1.248963\n",
            "\tTraining batch 188 loss: 0.811191\n",
            "\tTraining batch 189 loss: 0.827292\n",
            "\tTraining batch 190 loss: 1.400669\n",
            "\tTraining batch 191 loss: 1.185570\n",
            "\tTraining batch 192 loss: 1.124270\n",
            "\tTraining batch 193 loss: 1.063308\n",
            "\tTraining batch 194 loss: 0.766379\n",
            "\tTraining batch 195 loss: 1.185956\n",
            "\tTraining batch 196 loss: 1.601747\n",
            "\tTraining batch 197 loss: 1.381653\n",
            "\tTraining batch 198 loss: 1.102564\n",
            "\tTraining batch 199 loss: 1.316189\n",
            "\tTraining batch 200 loss: 0.983590\n",
            "\tTraining batch 201 loss: 1.545777\n",
            "\tTraining batch 202 loss: 1.556797\n",
            "\tTraining batch 203 loss: 1.650446\n",
            "\tTraining batch 204 loss: 1.078279\n",
            "\tTraining batch 205 loss: 1.206269\n",
            "\tTraining batch 206 loss: 1.199121\n",
            "\tTraining batch 207 loss: 1.237338\n",
            "\tTraining batch 208 loss: 1.194894\n",
            "\tTraining batch 209 loss: 1.195665\n",
            "\tTraining batch 210 loss: 1.167287\n",
            "\tTraining batch 211 loss: 1.332393\n",
            "\tTraining batch 212 loss: 1.402083\n",
            "\tTraining batch 213 loss: 1.317098\n",
            "\tTraining batch 214 loss: 1.218581\n",
            "\tTraining batch 215 loss: 1.295893\n",
            "\tTraining batch 216 loss: 0.982821\n",
            "\tTraining batch 217 loss: 1.109306\n",
            "\tTraining batch 218 loss: 1.240399\n",
            "\tTraining batch 219 loss: 1.117095\n",
            "\tTraining batch 220 loss: 1.103809\n",
            "\tTraining batch 221 loss: 0.965432\n",
            "\tTraining batch 222 loss: 1.032601\n",
            "\tTraining batch 223 loss: 1.252366\n",
            "\tTraining batch 224 loss: 1.071137\n",
            "\tTraining batch 225 loss: 1.441390\n",
            "\tTraining batch 226 loss: 1.075225\n",
            "\tTraining batch 227 loss: 1.571103\n",
            "\tTraining batch 228 loss: 0.932982\n",
            "\tTraining batch 229 loss: 1.085142\n",
            "\tTraining batch 230 loss: 1.062354\n",
            "\tTraining batch 231 loss: 1.284035\n",
            "\tTraining batch 232 loss: 1.374017\n",
            "\tTraining batch 233 loss: 1.148101\n",
            "\tTraining batch 234 loss: 1.097961\n",
            "\tTraining batch 235 loss: 1.236978\n",
            "\tTraining batch 236 loss: 0.931347\n",
            "\tTraining batch 237 loss: 0.844541\n",
            "\tTraining batch 238 loss: 1.003632\n",
            "\tTraining batch 239 loss: 1.144723\n",
            "\tTraining batch 240 loss: 1.017509\n",
            "\tTraining batch 241 loss: 1.344545\n",
            "\tTraining batch 242 loss: 1.069273\n",
            "\tTraining batch 243 loss: 1.019934\n",
            "\tTraining batch 244 loss: 0.849609\n",
            "\tTraining batch 245 loss: 0.978505\n",
            "\tTraining batch 246 loss: 0.988982\n",
            "\tTraining batch 247 loss: 1.167131\n",
            "\tTraining batch 248 loss: 1.133889\n",
            "\tTraining batch 249 loss: 1.126786\n",
            "\tTraining batch 250 loss: 0.845094\n",
            "\tTraining batch 251 loss: 0.992215\n",
            "\tTraining batch 252 loss: 1.091206\n",
            "\tTraining batch 253 loss: 0.920668\n",
            "\tTraining batch 254 loss: 1.133634\n",
            "\tTraining batch 255 loss: 0.997722\n",
            "\tTraining batch 256 loss: 1.251806\n",
            "\tTraining batch 257 loss: 0.972537\n",
            "\tTraining batch 258 loss: 0.985489\n",
            "\tTraining batch 259 loss: 1.072567\n",
            "\tTraining batch 260 loss: 0.964681\n",
            "\tTraining batch 261 loss: 1.001220\n",
            "\tTraining batch 262 loss: 1.099153\n",
            "\tTraining batch 263 loss: 0.891764\n",
            "\tTraining batch 264 loss: 1.399423\n",
            "\tTraining batch 265 loss: 1.126515\n",
            "\tTraining batch 266 loss: 1.165599\n",
            "\tTraining batch 267 loss: 1.077066\n",
            "\tTraining batch 268 loss: 1.215676\n",
            "\tTraining batch 269 loss: 1.234436\n",
            "\tTraining batch 270 loss: 0.814710\n",
            "\tTraining batch 271 loss: 1.123680\n",
            "\tTraining batch 272 loss: 1.305006\n",
            "\tTraining batch 273 loss: 0.818337\n",
            "\tTraining batch 274 loss: 1.382226\n",
            "\tTraining batch 275 loss: 0.942588\n",
            "\tTraining batch 276 loss: 1.337701\n",
            "\tTraining batch 277 loss: 0.835799\n",
            "\tTraining batch 278 loss: 1.441404\n",
            "\tTraining batch 279 loss: 0.859772\n",
            "\tTraining batch 280 loss: 1.077050\n",
            "\tTraining batch 281 loss: 1.091308\n",
            "\tTraining batch 282 loss: 0.875574\n",
            "\tTraining batch 283 loss: 1.138648\n",
            "\tTraining batch 284 loss: 1.136109\n",
            "\tTraining batch 285 loss: 1.103742\n",
            "\tTraining batch 286 loss: 1.393517\n",
            "\tTraining batch 287 loss: 0.898557\n",
            "\tTraining batch 288 loss: 1.492134\n",
            "\tTraining batch 289 loss: 1.104631\n",
            "\tTraining batch 290 loss: 0.922751\n",
            "\tTraining batch 291 loss: 0.946404\n",
            "\tTraining batch 292 loss: 0.995418\n",
            "\tTraining batch 293 loss: 1.013085\n",
            "\tTraining batch 294 loss: 1.228475\n",
            "\tTraining batch 295 loss: 1.214833\n",
            "\tTraining batch 296 loss: 0.919685\n",
            "\tTraining batch 297 loss: 1.259717\n",
            "\tTraining batch 298 loss: 0.818095\n",
            "\tTraining batch 299 loss: 1.332979\n",
            "\tTraining batch 300 loss: 0.941120\n",
            "\tTraining batch 301 loss: 1.074207\n",
            "\tTraining batch 302 loss: 1.130051\n",
            "\tTraining batch 303 loss: 1.177472\n",
            "\tTraining batch 304 loss: 1.355885\n",
            "\tTraining batch 305 loss: 0.903093\n",
            "\tTraining batch 306 loss: 0.933121\n",
            "\tTraining batch 307 loss: 1.198396\n",
            "\tTraining batch 308 loss: 1.184257\n",
            "\tTraining batch 309 loss: 0.959222\n",
            "\tTraining batch 310 loss: 0.672800\n",
            "\tTraining batch 311 loss: 1.142136\n",
            "\tTraining batch 312 loss: 1.143883\n",
            "\tTraining batch 313 loss: 1.113522\n",
            "\tTraining batch 314 loss: 1.090093\n",
            "\tTraining batch 315 loss: 1.097051\n",
            "\tTraining batch 316 loss: 1.212141\n",
            "\tTraining batch 317 loss: 1.127244\n",
            "\tTraining batch 318 loss: 1.059985\n",
            "\tTraining batch 319 loss: 1.012638\n",
            "\tTraining batch 320 loss: 1.178426\n",
            "\tTraining batch 321 loss: 1.138567\n",
            "\tTraining batch 322 loss: 1.226663\n",
            "\tTraining batch 323 loss: 1.133509\n",
            "\tTraining batch 324 loss: 1.039497\n",
            "\tTraining batch 325 loss: 0.858343\n",
            "\tTraining batch 326 loss: 1.165732\n",
            "\tTraining batch 327 loss: 1.132466\n",
            "\tTraining batch 328 loss: 1.334012\n",
            "\tTraining batch 329 loss: 1.343816\n",
            "\tTraining batch 330 loss: 1.289120\n",
            "\tTraining batch 331 loss: 0.872830\n",
            "\tTraining batch 332 loss: 1.196801\n",
            "\tTraining batch 333 loss: 0.838603\n",
            "\tTraining batch 334 loss: 1.020423\n",
            "\tTraining batch 335 loss: 0.962424\n",
            "\tTraining batch 336 loss: 1.059129\n",
            "\tTraining batch 337 loss: 0.837776\n",
            "\tTraining batch 338 loss: 1.083473\n",
            "\tTraining batch 339 loss: 0.749568\n",
            "\tTraining batch 340 loss: 1.158185\n",
            "\tTraining batch 341 loss: 0.820175\n",
            "\tTraining batch 342 loss: 1.275191\n",
            "\tTraining batch 343 loss: 0.587309\n",
            "\tTraining batch 344 loss: 0.862839\n",
            "\tTraining batch 345 loss: 1.085485\n",
            "\tTraining batch 346 loss: 1.397187\n",
            "\tTraining batch 347 loss: 1.172148\n",
            "\tTraining batch 348 loss: 1.140394\n",
            "\tTraining batch 349 loss: 1.037147\n",
            "\tTraining batch 350 loss: 1.134159\n",
            "\tTraining batch 351 loss: 0.997394\n",
            "\tTraining batch 352 loss: 1.311041\n",
            "\tTraining batch 353 loss: 0.821694\n",
            "\tTraining batch 354 loss: 0.906583\n",
            "\tTraining batch 355 loss: 1.007528\n",
            "\tTraining batch 356 loss: 1.364783\n",
            "\tTraining batch 357 loss: 1.165017\n",
            "\tTraining batch 358 loss: 0.847140\n",
            "\tTraining batch 359 loss: 1.138667\n",
            "\tTraining batch 360 loss: 1.065060\n",
            "\tTraining batch 361 loss: 0.950503\n",
            "\tTraining batch 362 loss: 1.011616\n",
            "\tTraining batch 363 loss: 0.914872\n",
            "\tTraining batch 364 loss: 1.129068\n",
            "\tTraining batch 365 loss: 1.344445\n",
            "\tTraining batch 366 loss: 1.051128\n",
            "\tTraining batch 367 loss: 0.819653\n",
            "\tTraining batch 368 loss: 0.932204\n",
            "\tTraining batch 369 loss: 1.024172\n",
            "\tTraining batch 370 loss: 1.071921\n",
            "\tTraining batch 371 loss: 1.094271\n",
            "\tTraining batch 372 loss: 1.345210\n",
            "\tTraining batch 373 loss: 1.301047\n",
            "\tTraining batch 374 loss: 0.799078\n",
            "\tTraining batch 375 loss: 0.815996\n",
            "\tTraining batch 376 loss: 1.142267\n",
            "\tTraining batch 377 loss: 1.168054\n",
            "\tTraining batch 378 loss: 1.095752\n",
            "\tTraining batch 379 loss: 1.170281\n",
            "\tTraining batch 380 loss: 0.982224\n",
            "\tTraining batch 381 loss: 0.989797\n",
            "\tTraining batch 382 loss: 0.996841\n",
            "\tTraining batch 383 loss: 1.019714\n",
            "\tTraining batch 384 loss: 0.808145\n",
            "\tTraining batch 385 loss: 1.095088\n",
            "\tTraining batch 386 loss: 0.786532\n",
            "\tTraining batch 387 loss: 1.076930\n",
            "\tTraining batch 388 loss: 1.318362\n",
            "\tTraining batch 389 loss: 1.281720\n",
            "\tTraining batch 390 loss: 1.268648\n",
            "\tTraining batch 391 loss: 1.209852\n",
            "\tTraining batch 392 loss: 0.890609\n",
            "\tTraining batch 393 loss: 1.246446\n",
            "\tTraining batch 394 loss: 0.830600\n",
            "\tTraining batch 395 loss: 0.801473\n",
            "\tTraining batch 396 loss: 1.189422\n",
            "\tTraining batch 397 loss: 1.273626\n",
            "\tTraining batch 398 loss: 1.238774\n",
            "\tTraining batch 399 loss: 1.064872\n",
            "\tTraining batch 400 loss: 1.340201\n",
            "\tTraining batch 401 loss: 1.139364\n",
            "\tTraining batch 402 loss: 0.945122\n",
            "\tTraining batch 403 loss: 1.157815\n",
            "\tTraining batch 404 loss: 1.018589\n",
            "\tTraining batch 405 loss: 1.022211\n",
            "\tTraining batch 406 loss: 0.863585\n",
            "\tTraining batch 407 loss: 0.874341\n",
            "\tTraining batch 408 loss: 1.091842\n",
            "\tTraining batch 409 loss: 1.011217\n",
            "\tTraining batch 410 loss: 1.107262\n",
            "\tTraining batch 411 loss: 0.771825\n",
            "\tTraining batch 412 loss: 1.129272\n",
            "\tTraining batch 413 loss: 1.118000\n",
            "\tTraining batch 414 loss: 1.039252\n",
            "\tTraining batch 415 loss: 1.000920\n",
            "\tTraining batch 416 loss: 0.887237\n",
            "\tTraining batch 417 loss: 1.212979\n",
            "\tTraining batch 418 loss: 1.301738\n",
            "\tTraining batch 419 loss: 1.275949\n",
            "\tTraining batch 420 loss: 1.141790\n",
            "\tTraining batch 421 loss: 1.984706\n",
            "\tTraining batch 422 loss: 0.899537\n",
            "\tTraining batch 423 loss: 1.434238\n",
            "Training set: Average loss: 1.188217\n",
            "Validation set: Average loss: 1.514789, Accuracy: 2539/5276 (48%)\n",
            "\n",
            "Epoch 2\n",
            "\tTraining batch 1 loss: 1.729851\n",
            "\tTraining batch 2 loss: 2.263652\n",
            "\tTraining batch 3 loss: 1.706354\n",
            "\tTraining batch 4 loss: 1.750915\n",
            "\tTraining batch 5 loss: 1.669362\n",
            "\tTraining batch 6 loss: 1.769230\n",
            "\tTraining batch 7 loss: 1.590122\n",
            "\tTraining batch 8 loss: 1.617879\n",
            "\tTraining batch 9 loss: 1.614100\n",
            "\tTraining batch 10 loss: 1.526661\n",
            "\tTraining batch 11 loss: 1.350529\n",
            "\tTraining batch 12 loss: 1.840941\n",
            "\tTraining batch 13 loss: 1.597029\n",
            "\tTraining batch 14 loss: 1.654152\n",
            "\tTraining batch 15 loss: 1.583278\n",
            "\tTraining batch 16 loss: 1.455284\n",
            "\tTraining batch 17 loss: 1.131558\n",
            "\tTraining batch 18 loss: 1.586208\n",
            "\tTraining batch 19 loss: 1.632411\n",
            "\tTraining batch 20 loss: 1.635144\n",
            "\tTraining batch 21 loss: 1.674156\n",
            "\tTraining batch 22 loss: 1.317490\n",
            "\tTraining batch 23 loss: 1.326165\n",
            "\tTraining batch 24 loss: 1.461207\n",
            "\tTraining batch 25 loss: 1.643826\n",
            "\tTraining batch 26 loss: 1.662801\n",
            "\tTraining batch 27 loss: 1.378908\n",
            "\tTraining batch 28 loss: 1.635632\n",
            "\tTraining batch 29 loss: 1.430847\n",
            "\tTraining batch 30 loss: 1.314530\n",
            "\tTraining batch 31 loss: 1.622434\n",
            "\tTraining batch 32 loss: 1.346005\n",
            "\tTraining batch 33 loss: 1.769159\n",
            "\tTraining batch 34 loss: 1.232996\n",
            "\tTraining batch 35 loss: 1.249603\n",
            "\tTraining batch 36 loss: 1.670393\n",
            "\tTraining batch 37 loss: 1.525090\n",
            "\tTraining batch 38 loss: 1.122780\n",
            "\tTraining batch 39 loss: 1.887336\n",
            "\tTraining batch 40 loss: 1.368319\n",
            "\tTraining batch 41 loss: 1.188750\n",
            "\tTraining batch 42 loss: 1.417164\n",
            "\tTraining batch 43 loss: 1.277952\n",
            "\tTraining batch 44 loss: 1.357795\n",
            "\tTraining batch 45 loss: 1.277855\n",
            "\tTraining batch 46 loss: 1.230272\n",
            "\tTraining batch 47 loss: 1.730443\n",
            "\tTraining batch 48 loss: 1.221637\n",
            "\tTraining batch 49 loss: 1.478317\n",
            "\tTraining batch 50 loss: 1.074041\n",
            "\tTraining batch 51 loss: 1.166011\n",
            "\tTraining batch 52 loss: 1.611219\n",
            "\tTraining batch 53 loss: 1.171898\n",
            "\tTraining batch 54 loss: 1.216222\n",
            "\tTraining batch 55 loss: 1.189892\n",
            "\tTraining batch 56 loss: 1.259253\n",
            "\tTraining batch 57 loss: 1.545667\n",
            "\tTraining batch 58 loss: 1.138575\n",
            "\tTraining batch 59 loss: 1.583751\n",
            "\tTraining batch 60 loss: 1.507814\n",
            "\tTraining batch 61 loss: 1.307572\n",
            "\tTraining batch 62 loss: 1.430372\n",
            "\tTraining batch 63 loss: 1.422071\n",
            "\tTraining batch 64 loss: 1.248786\n",
            "\tTraining batch 65 loss: 1.409150\n",
            "\tTraining batch 66 loss: 1.184966\n",
            "\tTraining batch 67 loss: 0.835447\n",
            "\tTraining batch 68 loss: 1.041062\n",
            "\tTraining batch 69 loss: 1.429833\n",
            "\tTraining batch 70 loss: 1.263717\n",
            "\tTraining batch 71 loss: 1.453129\n",
            "\tTraining batch 72 loss: 1.182514\n",
            "\tTraining batch 73 loss: 1.222850\n",
            "\tTraining batch 74 loss: 1.234618\n",
            "\tTraining batch 75 loss: 1.434417\n",
            "\tTraining batch 76 loss: 1.348879\n",
            "\tTraining batch 77 loss: 1.421095\n",
            "\tTraining batch 78 loss: 1.541708\n",
            "\tTraining batch 79 loss: 1.273939\n",
            "\tTraining batch 80 loss: 1.113974\n",
            "\tTraining batch 81 loss: 1.477903\n",
            "\tTraining batch 82 loss: 1.144526\n",
            "\tTraining batch 83 loss: 1.299736\n",
            "\tTraining batch 84 loss: 1.441277\n",
            "\tTraining batch 85 loss: 1.196699\n",
            "\tTraining batch 86 loss: 1.085803\n",
            "\tTraining batch 87 loss: 1.399484\n",
            "\tTraining batch 88 loss: 1.556762\n",
            "\tTraining batch 89 loss: 1.334463\n",
            "\tTraining batch 90 loss: 1.306468\n",
            "\tTraining batch 91 loss: 1.020456\n",
            "\tTraining batch 92 loss: 0.892455\n",
            "\tTraining batch 93 loss: 1.297420\n",
            "\tTraining batch 94 loss: 1.209630\n",
            "\tTraining batch 95 loss: 1.088052\n",
            "\tTraining batch 96 loss: 1.236837\n",
            "\tTraining batch 97 loss: 1.469103\n",
            "\tTraining batch 98 loss: 1.579157\n",
            "\tTraining batch 99 loss: 1.404135\n",
            "\tTraining batch 100 loss: 1.422895\n",
            "\tTraining batch 101 loss: 1.348411\n",
            "\tTraining batch 102 loss: 1.270661\n",
            "\tTraining batch 103 loss: 1.500993\n",
            "\tTraining batch 104 loss: 1.522652\n",
            "\tTraining batch 105 loss: 1.510653\n",
            "\tTraining batch 106 loss: 1.429727\n",
            "\tTraining batch 107 loss: 1.468848\n",
            "\tTraining batch 108 loss: 1.500616\n",
            "\tTraining batch 109 loss: 1.387551\n",
            "\tTraining batch 110 loss: 1.529982\n",
            "\tTraining batch 111 loss: 1.421546\n",
            "\tTraining batch 112 loss: 1.210589\n",
            "\tTraining batch 113 loss: 1.476475\n",
            "\tTraining batch 114 loss: 1.223659\n",
            "\tTraining batch 115 loss: 1.363423\n",
            "\tTraining batch 116 loss: 1.300082\n",
            "\tTraining batch 117 loss: 1.424285\n",
            "\tTraining batch 118 loss: 1.249491\n",
            "\tTraining batch 119 loss: 1.298199\n",
            "\tTraining batch 120 loss: 1.461706\n",
            "\tTraining batch 121 loss: 1.224541\n",
            "\tTraining batch 122 loss: 1.378647\n",
            "\tTraining batch 123 loss: 1.111240\n",
            "\tTraining batch 124 loss: 1.172111\n",
            "\tTraining batch 125 loss: 1.436962\n",
            "\tTraining batch 126 loss: 1.364839\n",
            "\tTraining batch 127 loss: 1.473414\n",
            "\tTraining batch 128 loss: 1.563661\n",
            "\tTraining batch 129 loss: 1.232453\n",
            "\tTraining batch 130 loss: 1.306469\n",
            "\tTraining batch 131 loss: 1.256355\n",
            "\tTraining batch 132 loss: 1.135560\n",
            "\tTraining batch 133 loss: 1.034954\n",
            "\tTraining batch 134 loss: 1.316567\n",
            "\tTraining batch 135 loss: 1.356933\n",
            "\tTraining batch 136 loss: 1.496433\n",
            "\tTraining batch 137 loss: 1.288091\n",
            "\tTraining batch 138 loss: 1.284008\n",
            "\tTraining batch 139 loss: 1.377720\n",
            "\tTraining batch 140 loss: 1.320895\n",
            "\tTraining batch 141 loss: 1.306361\n",
            "\tTraining batch 142 loss: 1.407716\n",
            "\tTraining batch 143 loss: 1.470215\n",
            "\tTraining batch 144 loss: 1.354460\n",
            "\tTraining batch 145 loss: 1.463618\n",
            "\tTraining batch 146 loss: 1.496900\n",
            "\tTraining batch 147 loss: 1.533159\n",
            "\tTraining batch 148 loss: 1.364048\n",
            "\tTraining batch 149 loss: 1.413103\n",
            "\tTraining batch 150 loss: 1.502232\n",
            "\tTraining batch 151 loss: 1.246020\n",
            "\tTraining batch 152 loss: 1.420997\n",
            "\tTraining batch 153 loss: 1.431946\n",
            "\tTraining batch 154 loss: 1.426564\n",
            "\tTraining batch 155 loss: 1.350580\n",
            "\tTraining batch 156 loss: 1.434263\n",
            "\tTraining batch 157 loss: 1.248327\n",
            "\tTraining batch 158 loss: 1.337406\n",
            "\tTraining batch 159 loss: 1.215036\n",
            "\tTraining batch 160 loss: 1.505547\n",
            "\tTraining batch 161 loss: 1.137560\n",
            "\tTraining batch 162 loss: 1.392652\n",
            "\tTraining batch 163 loss: 1.471193\n",
            "\tTraining batch 164 loss: 1.323528\n",
            "\tTraining batch 165 loss: 0.995673\n",
            "\tTraining batch 166 loss: 1.202647\n",
            "\tTraining batch 167 loss: 1.083511\n",
            "\tTraining batch 168 loss: 1.468140\n",
            "\tTraining batch 169 loss: 1.411144\n",
            "\tTraining batch 170 loss: 1.195304\n",
            "\tTraining batch 171 loss: 1.185869\n",
            "\tTraining batch 172 loss: 1.410741\n",
            "\tTraining batch 173 loss: 1.292728\n",
            "\tTraining batch 174 loss: 1.153262\n",
            "\tTraining batch 175 loss: 1.283056\n",
            "\tTraining batch 176 loss: 1.188940\n",
            "\tTraining batch 177 loss: 1.541405\n",
            "\tTraining batch 178 loss: 1.181028\n",
            "\tTraining batch 179 loss: 1.254094\n",
            "\tTraining batch 180 loss: 1.578974\n",
            "\tTraining batch 181 loss: 1.340414\n",
            "\tTraining batch 182 loss: 1.348309\n",
            "\tTraining batch 183 loss: 1.474810\n",
            "\tTraining batch 184 loss: 1.650143\n",
            "\tTraining batch 185 loss: 1.025971\n",
            "\tTraining batch 186 loss: 1.477313\n",
            "\tTraining batch 187 loss: 1.183283\n",
            "\tTraining batch 188 loss: 1.071419\n",
            "\tTraining batch 189 loss: 1.544588\n",
            "\tTraining batch 190 loss: 1.302869\n",
            "\tTraining batch 191 loss: 1.255776\n",
            "\tTraining batch 192 loss: 1.230510\n",
            "\tTraining batch 193 loss: 1.065035\n",
            "\tTraining batch 194 loss: 0.874488\n",
            "\tTraining batch 195 loss: 1.334322\n",
            "\tTraining batch 196 loss: 1.749618\n",
            "\tTraining batch 197 loss: 1.144359\n",
            "\tTraining batch 198 loss: 1.010757\n",
            "\tTraining batch 199 loss: 1.391561\n",
            "\tTraining batch 200 loss: 1.048515\n",
            "\tTraining batch 201 loss: 1.636664\n",
            "\tTraining batch 202 loss: 1.826976\n",
            "\tTraining batch 203 loss: 1.631870\n",
            "\tTraining batch 204 loss: 1.089623\n",
            "\tTraining batch 205 loss: 1.540893\n",
            "\tTraining batch 206 loss: 1.110003\n",
            "\tTraining batch 207 loss: 1.375693\n",
            "\tTraining batch 208 loss: 1.244131\n",
            "\tTraining batch 209 loss: 1.274185\n",
            "\tTraining batch 210 loss: 1.174608\n",
            "\tTraining batch 211 loss: 1.304272\n",
            "\tTraining batch 212 loss: 1.310127\n",
            "\tTraining batch 213 loss: 1.344255\n",
            "\tTraining batch 214 loss: 1.110332\n",
            "\tTraining batch 215 loss: 1.370051\n",
            "\tTraining batch 216 loss: 0.999646\n",
            "\tTraining batch 217 loss: 1.093868\n",
            "\tTraining batch 218 loss: 1.283805\n",
            "\tTraining batch 219 loss: 1.343053\n",
            "\tTraining batch 220 loss: 1.025457\n",
            "\tTraining batch 221 loss: 1.199538\n",
            "\tTraining batch 222 loss: 1.079132\n",
            "\tTraining batch 223 loss: 1.224001\n",
            "\tTraining batch 224 loss: 1.082803\n",
            "\tTraining batch 225 loss: 1.235957\n",
            "\tTraining batch 226 loss: 0.879014\n",
            "\tTraining batch 227 loss: 1.636989\n",
            "\tTraining batch 228 loss: 0.866923\n",
            "\tTraining batch 229 loss: 1.057544\n",
            "\tTraining batch 230 loss: 0.921813\n",
            "\tTraining batch 231 loss: 1.118069\n",
            "\tTraining batch 232 loss: 1.327613\n",
            "\tTraining batch 233 loss: 1.207134\n",
            "\tTraining batch 234 loss: 0.863124\n",
            "\tTraining batch 235 loss: 1.422017\n",
            "\tTraining batch 236 loss: 0.899010\n",
            "\tTraining batch 237 loss: 0.921163\n",
            "\tTraining batch 238 loss: 1.235999\n",
            "\tTraining batch 239 loss: 1.157315\n",
            "\tTraining batch 240 loss: 1.083299\n",
            "\tTraining batch 241 loss: 1.256331\n",
            "\tTraining batch 242 loss: 1.011796\n",
            "\tTraining batch 243 loss: 1.313012\n",
            "\tTraining batch 244 loss: 1.087582\n",
            "\tTraining batch 245 loss: 1.445708\n",
            "\tTraining batch 246 loss: 1.242351\n",
            "\tTraining batch 247 loss: 1.219805\n",
            "\tTraining batch 248 loss: 1.258120\n",
            "\tTraining batch 249 loss: 1.219443\n",
            "\tTraining batch 250 loss: 1.121726\n",
            "\tTraining batch 251 loss: 0.926156\n",
            "\tTraining batch 252 loss: 1.102561\n",
            "\tTraining batch 253 loss: 0.952322\n",
            "\tTraining batch 254 loss: 1.408327\n",
            "\tTraining batch 255 loss: 1.113361\n",
            "\tTraining batch 256 loss: 1.375203\n",
            "\tTraining batch 257 loss: 1.141419\n",
            "\tTraining batch 258 loss: 1.278576\n",
            "\tTraining batch 259 loss: 1.034042\n",
            "\tTraining batch 260 loss: 1.411609\n",
            "\tTraining batch 261 loss: 1.374839\n",
            "\tTraining batch 262 loss: 1.149726\n",
            "\tTraining batch 263 loss: 1.062885\n",
            "\tTraining batch 264 loss: 1.558802\n",
            "\tTraining batch 265 loss: 1.444178\n",
            "\tTraining batch 266 loss: 1.214118\n",
            "\tTraining batch 267 loss: 1.436410\n",
            "\tTraining batch 268 loss: 1.304198\n",
            "\tTraining batch 269 loss: 1.448565\n",
            "\tTraining batch 270 loss: 0.961491\n",
            "\tTraining batch 271 loss: 1.418480\n",
            "\tTraining batch 272 loss: 1.471526\n",
            "\tTraining batch 273 loss: 1.320229\n",
            "\tTraining batch 274 loss: 1.237387\n",
            "\tTraining batch 275 loss: 1.068503\n",
            "\tTraining batch 276 loss: 1.190190\n",
            "\tTraining batch 277 loss: 1.105022\n",
            "\tTraining batch 278 loss: 1.485191\n",
            "\tTraining batch 279 loss: 1.006510\n",
            "\tTraining batch 280 loss: 1.205720\n",
            "\tTraining batch 281 loss: 1.221329\n",
            "\tTraining batch 282 loss: 0.937913\n",
            "\tTraining batch 283 loss: 1.450096\n",
            "\tTraining batch 284 loss: 1.284667\n",
            "\tTraining batch 285 loss: 1.068830\n",
            "\tTraining batch 286 loss: 1.511371\n",
            "\tTraining batch 287 loss: 1.142997\n",
            "\tTraining batch 288 loss: 1.487882\n",
            "\tTraining batch 289 loss: 1.290279\n",
            "\tTraining batch 290 loss: 1.146212\n",
            "\tTraining batch 291 loss: 1.280024\n",
            "\tTraining batch 292 loss: 1.065003\n",
            "\tTraining batch 293 loss: 1.044051\n",
            "\tTraining batch 294 loss: 1.310783\n",
            "\tTraining batch 295 loss: 1.329125\n",
            "\tTraining batch 296 loss: 0.937330\n",
            "\tTraining batch 297 loss: 1.369821\n",
            "\tTraining batch 298 loss: 0.932462\n",
            "\tTraining batch 299 loss: 1.418925\n",
            "\tTraining batch 300 loss: 1.245759\n",
            "\tTraining batch 301 loss: 1.149066\n",
            "\tTraining batch 302 loss: 1.057627\n",
            "\tTraining batch 303 loss: 1.215473\n",
            "\tTraining batch 304 loss: 1.301503\n",
            "\tTraining batch 305 loss: 1.169878\n",
            "\tTraining batch 306 loss: 1.058157\n",
            "\tTraining batch 307 loss: 1.038011\n",
            "\tTraining batch 308 loss: 1.360551\n",
            "\tTraining batch 309 loss: 0.991305\n",
            "\tTraining batch 310 loss: 0.744490\n",
            "\tTraining batch 311 loss: 0.993563\n",
            "\tTraining batch 312 loss: 1.000250\n",
            "\tTraining batch 313 loss: 1.087490\n",
            "\tTraining batch 314 loss: 1.075176\n",
            "\tTraining batch 315 loss: 1.313937\n",
            "\tTraining batch 316 loss: 1.149792\n",
            "\tTraining batch 317 loss: 1.016666\n",
            "\tTraining batch 318 loss: 0.991578\n",
            "\tTraining batch 319 loss: 1.229084\n",
            "\tTraining batch 320 loss: 1.621256\n",
            "\tTraining batch 321 loss: 1.065581\n",
            "\tTraining batch 322 loss: 1.086443\n",
            "\tTraining batch 323 loss: 0.959989\n",
            "\tTraining batch 324 loss: 1.142612\n",
            "\tTraining batch 325 loss: 1.004693\n",
            "\tTraining batch 326 loss: 1.148282\n",
            "\tTraining batch 327 loss: 1.052276\n",
            "\tTraining batch 328 loss: 1.286954\n",
            "\tTraining batch 329 loss: 1.517274\n",
            "\tTraining batch 330 loss: 1.283406\n",
            "\tTraining batch 331 loss: 1.018677\n",
            "\tTraining batch 332 loss: 1.123144\n",
            "\tTraining batch 333 loss: 0.911277\n",
            "\tTraining batch 334 loss: 1.309562\n",
            "\tTraining batch 335 loss: 0.775992\n",
            "\tTraining batch 336 loss: 1.106938\n",
            "\tTraining batch 337 loss: 0.782890\n",
            "\tTraining batch 338 loss: 1.015978\n",
            "\tTraining batch 339 loss: 0.865436\n",
            "\tTraining batch 340 loss: 1.393217\n",
            "\tTraining batch 341 loss: 0.955763\n",
            "\tTraining batch 342 loss: 1.313065\n",
            "\tTraining batch 343 loss: 0.971465\n",
            "\tTraining batch 344 loss: 0.740096\n",
            "\tTraining batch 345 loss: 0.976968\n",
            "\tTraining batch 346 loss: 1.287827\n",
            "\tTraining batch 347 loss: 1.038134\n",
            "\tTraining batch 348 loss: 1.335533\n",
            "\tTraining batch 349 loss: 0.966513\n",
            "\tTraining batch 350 loss: 1.132677\n",
            "\tTraining batch 351 loss: 0.993559\n",
            "\tTraining batch 352 loss: 1.237895\n",
            "\tTraining batch 353 loss: 1.001111\n",
            "\tTraining batch 354 loss: 0.947684\n",
            "\tTraining batch 355 loss: 1.193274\n",
            "\tTraining batch 356 loss: 1.362381\n",
            "\tTraining batch 357 loss: 1.385823\n",
            "\tTraining batch 358 loss: 0.943342\n",
            "\tTraining batch 359 loss: 1.243479\n",
            "\tTraining batch 360 loss: 1.202369\n",
            "\tTraining batch 361 loss: 1.006285\n",
            "\tTraining batch 362 loss: 1.082394\n",
            "\tTraining batch 363 loss: 0.949650\n",
            "\tTraining batch 364 loss: 1.075847\n",
            "\tTraining batch 365 loss: 1.426613\n",
            "\tTraining batch 366 loss: 1.207968\n",
            "\tTraining batch 367 loss: 0.925893\n",
            "\tTraining batch 368 loss: 1.083044\n",
            "\tTraining batch 369 loss: 1.123258\n",
            "\tTraining batch 370 loss: 0.975524\n",
            "\tTraining batch 371 loss: 1.144370\n",
            "\tTraining batch 372 loss: 1.216496\n",
            "\tTraining batch 373 loss: 1.226428\n",
            "\tTraining batch 374 loss: 0.901935\n",
            "\tTraining batch 375 loss: 0.829774\n",
            "\tTraining batch 376 loss: 1.158616\n",
            "\tTraining batch 377 loss: 0.956333\n",
            "\tTraining batch 378 loss: 1.213734\n",
            "\tTraining batch 379 loss: 1.251089\n",
            "\tTraining batch 380 loss: 1.004619\n",
            "\tTraining batch 381 loss: 0.925410\n",
            "\tTraining batch 382 loss: 0.963277\n",
            "\tTraining batch 383 loss: 1.121905\n",
            "\tTraining batch 384 loss: 0.965758\n",
            "\tTraining batch 385 loss: 0.949206\n",
            "\tTraining batch 386 loss: 0.857519\n",
            "\tTraining batch 387 loss: 0.837132\n",
            "\tTraining batch 388 loss: 1.207604\n",
            "\tTraining batch 389 loss: 1.094992\n",
            "\tTraining batch 390 loss: 1.408184\n",
            "\tTraining batch 391 loss: 1.007336\n",
            "\tTraining batch 392 loss: 1.055742\n",
            "\tTraining batch 393 loss: 1.255570\n",
            "\tTraining batch 394 loss: 0.986556\n",
            "\tTraining batch 395 loss: 0.700083\n",
            "\tTraining batch 396 loss: 1.034796\n",
            "\tTraining batch 397 loss: 1.333665\n",
            "\tTraining batch 398 loss: 1.537311\n",
            "\tTraining batch 399 loss: 1.148983\n",
            "\tTraining batch 400 loss: 1.370869\n",
            "\tTraining batch 401 loss: 1.157294\n",
            "\tTraining batch 402 loss: 0.909600\n",
            "\tTraining batch 403 loss: 1.221111\n",
            "\tTraining batch 404 loss: 1.032940\n",
            "\tTraining batch 405 loss: 0.967190\n",
            "\tTraining batch 406 loss: 0.954956\n",
            "\tTraining batch 407 loss: 0.991662\n",
            "\tTraining batch 408 loss: 1.194310\n",
            "\tTraining batch 409 loss: 1.176430\n",
            "\tTraining batch 410 loss: 1.063398\n",
            "\tTraining batch 411 loss: 0.741482\n",
            "\tTraining batch 412 loss: 0.998576\n",
            "\tTraining batch 413 loss: 0.972912\n",
            "\tTraining batch 414 loss: 1.017657\n",
            "\tTraining batch 415 loss: 1.213252\n",
            "\tTraining batch 416 loss: 1.038341\n",
            "\tTraining batch 417 loss: 1.286715\n",
            "\tTraining batch 418 loss: 1.132635\n",
            "\tTraining batch 419 loss: 1.279610\n",
            "\tTraining batch 420 loss: 0.987903\n",
            "\tTraining batch 421 loss: 1.263675\n",
            "\tTraining batch 422 loss: 0.809337\n",
            "\tTraining batch 423 loss: 0.697500\n",
            "Training set: Average loss: 1.254205\n",
            "Validation set: Average loss: 0.947471, Accuracy: 3508/5276 (66%)\n",
            "\n",
            "Epoch 3\n",
            "\tTraining batch 1 loss: 1.133333\n",
            "\tTraining batch 2 loss: 1.264223\n",
            "\tTraining batch 3 loss: 1.090165\n",
            "\tTraining batch 4 loss: 1.183115\n",
            "\tTraining batch 5 loss: 0.968293\n",
            "\tTraining batch 6 loss: 1.441135\n",
            "\tTraining batch 7 loss: 1.261770\n",
            "\tTraining batch 8 loss: 1.387306\n",
            "\tTraining batch 9 loss: 1.323354\n",
            "\tTraining batch 10 loss: 1.124949\n",
            "\tTraining batch 11 loss: 0.864418\n",
            "\tTraining batch 12 loss: 1.509546\n",
            "\tTraining batch 13 loss: 1.277384\n",
            "\tTraining batch 14 loss: 0.995966\n",
            "\tTraining batch 15 loss: 1.405147\n",
            "\tTraining batch 16 loss: 0.856834\n",
            "\tTraining batch 17 loss: 0.888180\n",
            "\tTraining batch 18 loss: 1.155950\n",
            "\tTraining batch 19 loss: 1.041342\n",
            "\tTraining batch 20 loss: 1.037716\n",
            "\tTraining batch 21 loss: 1.053716\n",
            "\tTraining batch 22 loss: 0.790191\n",
            "\tTraining batch 23 loss: 0.847166\n",
            "\tTraining batch 24 loss: 0.983898\n",
            "\tTraining batch 25 loss: 1.095217\n",
            "\tTraining batch 26 loss: 1.062841\n",
            "\tTraining batch 27 loss: 0.896027\n",
            "\tTraining batch 28 loss: 0.969272\n",
            "\tTraining batch 29 loss: 1.019482\n",
            "\tTraining batch 30 loss: 0.814226\n",
            "\tTraining batch 31 loss: 0.994192\n",
            "\tTraining batch 32 loss: 0.992371\n",
            "\tTraining batch 33 loss: 1.377103\n",
            "\tTraining batch 34 loss: 1.046003\n",
            "\tTraining batch 35 loss: 0.938077\n",
            "\tTraining batch 36 loss: 1.302822\n",
            "\tTraining batch 37 loss: 1.278827\n",
            "\tTraining batch 38 loss: 0.965057\n",
            "\tTraining batch 39 loss: 1.329655\n",
            "\tTraining batch 40 loss: 0.855163\n",
            "\tTraining batch 41 loss: 0.896410\n",
            "\tTraining batch 42 loss: 1.270906\n",
            "\tTraining batch 43 loss: 0.907748\n",
            "\tTraining batch 44 loss: 1.167819\n",
            "\tTraining batch 45 loss: 0.903680\n",
            "\tTraining batch 46 loss: 0.899864\n",
            "\tTraining batch 47 loss: 1.334886\n",
            "\tTraining batch 48 loss: 1.301826\n",
            "\tTraining batch 49 loss: 1.356000\n",
            "\tTraining batch 50 loss: 0.874613\n",
            "\tTraining batch 51 loss: 0.942155\n",
            "\tTraining batch 52 loss: 1.066564\n",
            "\tTraining batch 53 loss: 0.664177\n",
            "\tTraining batch 54 loss: 0.943924\n",
            "\tTraining batch 55 loss: 1.160670\n",
            "\tTraining batch 56 loss: 1.013127\n",
            "\tTraining batch 57 loss: 1.221663\n",
            "\tTraining batch 58 loss: 0.817106\n",
            "\tTraining batch 59 loss: 1.034164\n",
            "\tTraining batch 60 loss: 1.213501\n",
            "\tTraining batch 61 loss: 1.193359\n",
            "\tTraining batch 62 loss: 1.226290\n",
            "\tTraining batch 63 loss: 0.995405\n",
            "\tTraining batch 64 loss: 0.911193\n",
            "\tTraining batch 65 loss: 1.184799\n",
            "\tTraining batch 66 loss: 1.005158\n",
            "\tTraining batch 67 loss: 0.724785\n",
            "\tTraining batch 68 loss: 1.084174\n",
            "\tTraining batch 69 loss: 1.241471\n",
            "\tTraining batch 70 loss: 1.255818\n",
            "\tTraining batch 71 loss: 0.913979\n",
            "\tTraining batch 72 loss: 0.925750\n",
            "\tTraining batch 73 loss: 1.054809\n",
            "\tTraining batch 74 loss: 1.221565\n",
            "\tTraining batch 75 loss: 1.008543\n",
            "\tTraining batch 76 loss: 1.118037\n",
            "\tTraining batch 77 loss: 1.204374\n",
            "\tTraining batch 78 loss: 1.358909\n",
            "\tTraining batch 79 loss: 1.062488\n",
            "\tTraining batch 80 loss: 0.848892\n",
            "\tTraining batch 81 loss: 1.056821\n",
            "\tTraining batch 82 loss: 1.144597\n",
            "\tTraining batch 83 loss: 1.078735\n",
            "\tTraining batch 84 loss: 1.153898\n",
            "\tTraining batch 85 loss: 0.886692\n",
            "\tTraining batch 86 loss: 0.800734\n",
            "\tTraining batch 87 loss: 1.098768\n",
            "\tTraining batch 88 loss: 1.216042\n",
            "\tTraining batch 89 loss: 1.201975\n",
            "\tTraining batch 90 loss: 1.176927\n",
            "\tTraining batch 91 loss: 0.952273\n",
            "\tTraining batch 92 loss: 0.940363\n",
            "\tTraining batch 93 loss: 1.193949\n",
            "\tTraining batch 94 loss: 1.117010\n",
            "\tTraining batch 95 loss: 1.088840\n",
            "\tTraining batch 96 loss: 1.127975\n",
            "\tTraining batch 97 loss: 1.460003\n",
            "\tTraining batch 98 loss: 1.046739\n",
            "\tTraining batch 99 loss: 1.200327\n",
            "\tTraining batch 100 loss: 1.086223\n",
            "\tTraining batch 101 loss: 1.103347\n",
            "\tTraining batch 102 loss: 0.877457\n",
            "\tTraining batch 103 loss: 1.341046\n",
            "\tTraining batch 104 loss: 1.190642\n",
            "\tTraining batch 105 loss: 1.128917\n",
            "\tTraining batch 106 loss: 1.322590\n",
            "\tTraining batch 107 loss: 1.173505\n",
            "\tTraining batch 108 loss: 1.112962\n",
            "\tTraining batch 109 loss: 0.963883\n",
            "\tTraining batch 110 loss: 1.161256\n",
            "\tTraining batch 111 loss: 1.093072\n",
            "\tTraining batch 112 loss: 0.951467\n",
            "\tTraining batch 113 loss: 1.264256\n",
            "\tTraining batch 114 loss: 1.113672\n",
            "\tTraining batch 115 loss: 1.015272\n",
            "\tTraining batch 116 loss: 1.093165\n",
            "\tTraining batch 117 loss: 1.290227\n",
            "\tTraining batch 118 loss: 1.043972\n",
            "\tTraining batch 119 loss: 1.112022\n",
            "\tTraining batch 120 loss: 1.178473\n",
            "\tTraining batch 121 loss: 1.056229\n",
            "\tTraining batch 122 loss: 1.119803\n",
            "\tTraining batch 123 loss: 0.860668\n",
            "\tTraining batch 124 loss: 0.884649\n",
            "\tTraining batch 125 loss: 1.022585\n",
            "\tTraining batch 126 loss: 0.856466\n",
            "\tTraining batch 127 loss: 1.207358\n",
            "\tTraining batch 128 loss: 1.176060\n",
            "\tTraining batch 129 loss: 0.850182\n",
            "\tTraining batch 130 loss: 1.024462\n",
            "\tTraining batch 131 loss: 1.496401\n",
            "\tTraining batch 132 loss: 0.865027\n",
            "\tTraining batch 133 loss: 0.916897\n",
            "\tTraining batch 134 loss: 1.305706\n",
            "\tTraining batch 135 loss: 1.365957\n",
            "\tTraining batch 136 loss: 1.282030\n",
            "\tTraining batch 137 loss: 1.285512\n",
            "\tTraining batch 138 loss: 0.946061\n",
            "\tTraining batch 139 loss: 1.121330\n",
            "\tTraining batch 140 loss: 1.338692\n",
            "\tTraining batch 141 loss: 1.335583\n",
            "\tTraining batch 142 loss: 1.507536\n",
            "\tTraining batch 143 loss: 1.396121\n",
            "\tTraining batch 144 loss: 1.288626\n",
            "\tTraining batch 145 loss: 1.345803\n",
            "\tTraining batch 146 loss: 1.321381\n",
            "\tTraining batch 147 loss: 1.527183\n",
            "\tTraining batch 148 loss: 1.206084\n",
            "\tTraining batch 149 loss: 1.161769\n",
            "\tTraining batch 150 loss: 1.072617\n",
            "\tTraining batch 151 loss: 0.884816\n",
            "\tTraining batch 152 loss: 1.119052\n",
            "\tTraining batch 153 loss: 1.471537\n",
            "\tTraining batch 154 loss: 1.247731\n",
            "\tTraining batch 155 loss: 1.108724\n",
            "\tTraining batch 156 loss: 1.146533\n",
            "\tTraining batch 157 loss: 0.707467\n",
            "\tTraining batch 158 loss: 0.966298\n",
            "\tTraining batch 159 loss: 1.133323\n",
            "\tTraining batch 160 loss: 0.885336\n",
            "\tTraining batch 161 loss: 1.110630\n",
            "\tTraining batch 162 loss: 1.165818\n",
            "\tTraining batch 163 loss: 1.020626\n",
            "\tTraining batch 164 loss: 1.027763\n",
            "\tTraining batch 165 loss: 1.067857\n",
            "\tTraining batch 166 loss: 1.157209\n",
            "\tTraining batch 167 loss: 1.076169\n",
            "\tTraining batch 168 loss: 1.096384\n",
            "\tTraining batch 169 loss: 1.300939\n",
            "\tTraining batch 170 loss: 1.078642\n",
            "\tTraining batch 171 loss: 1.063911\n",
            "\tTraining batch 172 loss: 1.114593\n",
            "\tTraining batch 173 loss: 1.232533\n",
            "\tTraining batch 174 loss: 1.123711\n",
            "\tTraining batch 175 loss: 1.384001\n",
            "\tTraining batch 176 loss: 1.108658\n",
            "\tTraining batch 177 loss: 1.448207\n",
            "\tTraining batch 178 loss: 0.868013\n",
            "\tTraining batch 179 loss: 1.072149\n",
            "\tTraining batch 180 loss: 1.415731\n",
            "\tTraining batch 181 loss: 1.284520\n",
            "\tTraining batch 182 loss: 1.268774\n",
            "\tTraining batch 183 loss: 1.192922\n",
            "\tTraining batch 184 loss: 1.339462\n",
            "\tTraining batch 185 loss: 0.716798\n",
            "\tTraining batch 186 loss: 1.172371\n",
            "\tTraining batch 187 loss: 1.087309\n",
            "\tTraining batch 188 loss: 0.829207\n",
            "\tTraining batch 189 loss: 1.047694\n",
            "\tTraining batch 190 loss: 1.193965\n",
            "\tTraining batch 191 loss: 1.035130\n",
            "\tTraining batch 192 loss: 1.094292\n",
            "\tTraining batch 193 loss: 1.179365\n",
            "\tTraining batch 194 loss: 0.749148\n",
            "\tTraining batch 195 loss: 1.156735\n",
            "\tTraining batch 196 loss: 1.339859\n",
            "\tTraining batch 197 loss: 1.145079\n",
            "\tTraining batch 198 loss: 0.920817\n",
            "\tTraining batch 199 loss: 1.040957\n",
            "\tTraining batch 200 loss: 0.867845\n",
            "\tTraining batch 201 loss: 1.536371\n",
            "\tTraining batch 202 loss: 1.236712\n",
            "\tTraining batch 203 loss: 1.428415\n",
            "\tTraining batch 204 loss: 1.143778\n",
            "\tTraining batch 205 loss: 1.077391\n",
            "\tTraining batch 206 loss: 1.136914\n",
            "\tTraining batch 207 loss: 1.187325\n",
            "\tTraining batch 208 loss: 1.165842\n",
            "\tTraining batch 209 loss: 1.035890\n",
            "\tTraining batch 210 loss: 1.021844\n",
            "\tTraining batch 211 loss: 1.271067\n",
            "\tTraining batch 212 loss: 1.151492\n",
            "\tTraining batch 213 loss: 1.097626\n",
            "\tTraining batch 214 loss: 1.029200\n",
            "\tTraining batch 215 loss: 1.329251\n",
            "\tTraining batch 216 loss: 0.826598\n",
            "\tTraining batch 217 loss: 1.052487\n",
            "\tTraining batch 218 loss: 1.081498\n",
            "\tTraining batch 219 loss: 1.222072\n",
            "\tTraining batch 220 loss: 0.918463\n",
            "\tTraining batch 221 loss: 1.066526\n",
            "\tTraining batch 222 loss: 0.896364\n",
            "\tTraining batch 223 loss: 1.015628\n",
            "\tTraining batch 224 loss: 1.168316\n",
            "\tTraining batch 225 loss: 1.376259\n",
            "\tTraining batch 226 loss: 0.792794\n",
            "\tTraining batch 227 loss: 1.456873\n",
            "\tTraining batch 228 loss: 0.583297\n",
            "\tTraining batch 229 loss: 0.864043\n",
            "\tTraining batch 230 loss: 0.662507\n",
            "\tTraining batch 231 loss: 1.133037\n",
            "\tTraining batch 232 loss: 1.209683\n",
            "\tTraining batch 233 loss: 1.184278\n",
            "\tTraining batch 234 loss: 0.868907\n",
            "\tTraining batch 235 loss: 1.150307\n",
            "\tTraining batch 236 loss: 0.813395\n",
            "\tTraining batch 237 loss: 0.739466\n",
            "\tTraining batch 238 loss: 1.144449\n",
            "\tTraining batch 239 loss: 1.178649\n",
            "\tTraining batch 240 loss: 0.935248\n",
            "\tTraining batch 241 loss: 0.982025\n",
            "\tTraining batch 242 loss: 0.911068\n",
            "\tTraining batch 243 loss: 1.033949\n",
            "\tTraining batch 244 loss: 1.011897\n",
            "\tTraining batch 245 loss: 0.993699\n",
            "\tTraining batch 246 loss: 0.893363\n",
            "\tTraining batch 247 loss: 1.224613\n",
            "\tTraining batch 248 loss: 0.967443\n",
            "\tTraining batch 249 loss: 1.090399\n",
            "\tTraining batch 250 loss: 1.024000\n",
            "\tTraining batch 251 loss: 0.868192\n",
            "\tTraining batch 252 loss: 1.120395\n",
            "\tTraining batch 253 loss: 0.903170\n",
            "\tTraining batch 254 loss: 1.330274\n",
            "\tTraining batch 255 loss: 0.902214\n",
            "\tTraining batch 256 loss: 1.215040\n",
            "\tTraining batch 257 loss: 1.010567\n",
            "\tTraining batch 258 loss: 1.044801\n",
            "\tTraining batch 259 loss: 0.966408\n",
            "\tTraining batch 260 loss: 1.218820\n",
            "\tTraining batch 261 loss: 1.104816\n",
            "\tTraining batch 262 loss: 1.108100\n",
            "\tTraining batch 263 loss: 1.087942\n",
            "\tTraining batch 264 loss: 1.247434\n",
            "\tTraining batch 265 loss: 1.232367\n",
            "\tTraining batch 266 loss: 0.895364\n",
            "\tTraining batch 267 loss: 1.134819\n",
            "\tTraining batch 268 loss: 1.396666\n",
            "\tTraining batch 269 loss: 1.384882\n",
            "\tTraining batch 270 loss: 0.775365\n",
            "\tTraining batch 271 loss: 0.975594\n",
            "\tTraining batch 272 loss: 1.272127\n",
            "\tTraining batch 273 loss: 1.018906\n",
            "\tTraining batch 274 loss: 1.443493\n",
            "\tTraining batch 275 loss: 1.023923\n",
            "\tTraining batch 276 loss: 1.073902\n",
            "\tTraining batch 277 loss: 0.984315\n",
            "\tTraining batch 278 loss: 1.085090\n",
            "\tTraining batch 279 loss: 0.820194\n",
            "\tTraining batch 280 loss: 1.144558\n",
            "\tTraining batch 281 loss: 1.275920\n",
            "\tTraining batch 282 loss: 1.044370\n",
            "\tTraining batch 283 loss: 1.203605\n",
            "\tTraining batch 284 loss: 1.087944\n",
            "\tTraining batch 285 loss: 0.960884\n",
            "\tTraining batch 286 loss: 1.433288\n",
            "\tTraining batch 287 loss: 1.133212\n",
            "\tTraining batch 288 loss: 1.156633\n",
            "\tTraining batch 289 loss: 0.938538\n",
            "\tTraining batch 290 loss: 1.006152\n",
            "\tTraining batch 291 loss: 1.341847\n",
            "\tTraining batch 292 loss: 0.963287\n",
            "\tTraining batch 293 loss: 0.941000\n",
            "\tTraining batch 294 loss: 1.151199\n",
            "\tTraining batch 295 loss: 1.376632\n",
            "\tTraining batch 296 loss: 0.938938\n",
            "\tTraining batch 297 loss: 1.202029\n",
            "\tTraining batch 298 loss: 0.878733\n",
            "\tTraining batch 299 loss: 1.290626\n",
            "\tTraining batch 300 loss: 1.057608\n",
            "\tTraining batch 301 loss: 1.103842\n",
            "\tTraining batch 302 loss: 1.040980\n",
            "\tTraining batch 303 loss: 1.216234\n",
            "\tTraining batch 304 loss: 1.250912\n",
            "\tTraining batch 305 loss: 1.250865\n",
            "\tTraining batch 306 loss: 0.897057\n",
            "\tTraining batch 307 loss: 0.932505\n",
            "\tTraining batch 308 loss: 1.131515\n",
            "\tTraining batch 309 loss: 1.208290\n",
            "\tTraining batch 310 loss: 0.669938\n",
            "\tTraining batch 311 loss: 0.893526\n",
            "\tTraining batch 312 loss: 1.014524\n",
            "\tTraining batch 313 loss: 1.110734\n",
            "\tTraining batch 314 loss: 0.973220\n",
            "\tTraining batch 315 loss: 1.341175\n",
            "\tTraining batch 316 loss: 1.274332\n",
            "\tTraining batch 317 loss: 0.980434\n",
            "\tTraining batch 318 loss: 1.109915\n",
            "\tTraining batch 319 loss: 1.108039\n",
            "\tTraining batch 320 loss: 1.195297\n",
            "\tTraining batch 321 loss: 1.115146\n",
            "\tTraining batch 322 loss: 1.119805\n",
            "\tTraining batch 323 loss: 0.874279\n",
            "\tTraining batch 324 loss: 1.005934\n",
            "\tTraining batch 325 loss: 0.855329\n",
            "\tTraining batch 326 loss: 1.168473\n",
            "\tTraining batch 327 loss: 1.233197\n",
            "\tTraining batch 328 loss: 1.330494\n",
            "\tTraining batch 329 loss: 1.232669\n",
            "\tTraining batch 330 loss: 1.507234\n",
            "\tTraining batch 331 loss: 0.941629\n",
            "\tTraining batch 332 loss: 0.937187\n",
            "\tTraining batch 333 loss: 0.714936\n",
            "\tTraining batch 334 loss: 1.219720\n",
            "\tTraining batch 335 loss: 0.791568\n",
            "\tTraining batch 336 loss: 1.084025\n",
            "\tTraining batch 337 loss: 0.719623\n",
            "\tTraining batch 338 loss: 1.046914\n",
            "\tTraining batch 339 loss: 0.849844\n",
            "\tTraining batch 340 loss: 1.063483\n",
            "\tTraining batch 341 loss: 0.792932\n",
            "\tTraining batch 342 loss: 1.185208\n",
            "\tTraining batch 343 loss: 0.828271\n",
            "\tTraining batch 344 loss: 0.914195\n",
            "\tTraining batch 345 loss: 0.858228\n",
            "\tTraining batch 346 loss: 1.253183\n",
            "\tTraining batch 347 loss: 0.863400\n",
            "\tTraining batch 348 loss: 1.253839\n",
            "\tTraining batch 349 loss: 0.789317\n",
            "\tTraining batch 350 loss: 0.959541\n",
            "\tTraining batch 351 loss: 0.986064\n",
            "\tTraining batch 352 loss: 1.433627\n",
            "\tTraining batch 353 loss: 0.824180\n",
            "\tTraining batch 354 loss: 0.745985\n",
            "\tTraining batch 355 loss: 1.150390\n",
            "\tTraining batch 356 loss: 1.470360\n",
            "\tTraining batch 357 loss: 1.287298\n",
            "\tTraining batch 358 loss: 0.760049\n",
            "\tTraining batch 359 loss: 0.894893\n",
            "\tTraining batch 360 loss: 1.154459\n",
            "\tTraining batch 361 loss: 1.053310\n",
            "\tTraining batch 362 loss: 1.084127\n",
            "\tTraining batch 363 loss: 0.975682\n",
            "\tTraining batch 364 loss: 0.940777\n",
            "\tTraining batch 365 loss: 1.241082\n",
            "\tTraining batch 366 loss: 1.048743\n",
            "\tTraining batch 367 loss: 0.813223\n",
            "\tTraining batch 368 loss: 0.866529\n",
            "\tTraining batch 369 loss: 1.039481\n",
            "\tTraining batch 370 loss: 0.955764\n",
            "\tTraining batch 371 loss: 1.077135\n",
            "\tTraining batch 372 loss: 1.223313\n",
            "\tTraining batch 373 loss: 1.186090\n",
            "\tTraining batch 374 loss: 0.747394\n",
            "\tTraining batch 375 loss: 0.810033\n",
            "\tTraining batch 376 loss: 1.004516\n",
            "\tTraining batch 377 loss: 0.970735\n",
            "\tTraining batch 378 loss: 1.255106\n",
            "\tTraining batch 379 loss: 1.216998\n",
            "\tTraining batch 380 loss: 1.137163\n",
            "\tTraining batch 381 loss: 1.039409\n",
            "\tTraining batch 382 loss: 0.952915\n",
            "\tTraining batch 383 loss: 0.980819\n",
            "\tTraining batch 384 loss: 0.799446\n",
            "\tTraining batch 385 loss: 0.985864\n",
            "\tTraining batch 386 loss: 0.864006\n",
            "\tTraining batch 387 loss: 0.831616\n",
            "\tTraining batch 388 loss: 1.064306\n",
            "\tTraining batch 389 loss: 1.065245\n",
            "\tTraining batch 390 loss: 1.106285\n",
            "\tTraining batch 391 loss: 0.930430\n",
            "\tTraining batch 392 loss: 0.805709\n",
            "\tTraining batch 393 loss: 1.242902\n",
            "\tTraining batch 394 loss: 0.832670\n",
            "\tTraining batch 395 loss: 0.875340\n",
            "\tTraining batch 396 loss: 0.949004\n",
            "\tTraining batch 397 loss: 1.467254\n",
            "\tTraining batch 398 loss: 1.200751\n",
            "\tTraining batch 399 loss: 0.957317\n",
            "\tTraining batch 400 loss: 1.396344\n",
            "\tTraining batch 401 loss: 1.067915\n",
            "\tTraining batch 402 loss: 0.799994\n",
            "\tTraining batch 403 loss: 1.149038\n",
            "\tTraining batch 404 loss: 0.823360\n",
            "\tTraining batch 405 loss: 1.004682\n",
            "\tTraining batch 406 loss: 0.923521\n",
            "\tTraining batch 407 loss: 0.820220\n",
            "\tTraining batch 408 loss: 1.024148\n",
            "\tTraining batch 409 loss: 1.107235\n",
            "\tTraining batch 410 loss: 1.143322\n",
            "\tTraining batch 411 loss: 0.810749\n",
            "\tTraining batch 412 loss: 1.029183\n",
            "\tTraining batch 413 loss: 1.056897\n",
            "\tTraining batch 414 loss: 1.114516\n",
            "\tTraining batch 415 loss: 0.961096\n",
            "\tTraining batch 416 loss: 0.987486\n",
            "\tTraining batch 417 loss: 1.261331\n",
            "\tTraining batch 418 loss: 1.056377\n",
            "\tTraining batch 419 loss: 1.135617\n",
            "\tTraining batch 420 loss: 1.015927\n",
            "\tTraining batch 421 loss: 1.212627\n",
            "\tTraining batch 422 loss: 0.757946\n",
            "\tTraining batch 423 loss: 0.674738\n",
            "Training set: Average loss: 1.079601\n",
            "Validation set: Average loss: 0.839028, Accuracy: 3687/5276 (70%)\n",
            "\n",
            "Epoch 4\n",
            "\tTraining batch 1 loss: 0.814526\n",
            "\tTraining batch 2 loss: 1.247630\n",
            "\tTraining batch 3 loss: 0.926469\n",
            "\tTraining batch 4 loss: 0.912162\n",
            "\tTraining batch 5 loss: 1.042153\n",
            "\tTraining batch 6 loss: 1.217668\n",
            "\tTraining batch 7 loss: 1.063244\n",
            "\tTraining batch 8 loss: 1.168154\n",
            "\tTraining batch 9 loss: 1.084649\n",
            "\tTraining batch 10 loss: 1.018444\n",
            "\tTraining batch 11 loss: 0.919544\n",
            "\tTraining batch 12 loss: 1.403612\n",
            "\tTraining batch 13 loss: 1.262423\n",
            "\tTraining batch 14 loss: 1.094208\n",
            "\tTraining batch 15 loss: 1.311040\n",
            "\tTraining batch 16 loss: 0.915793\n",
            "\tTraining batch 17 loss: 0.689986\n",
            "\tTraining batch 18 loss: 1.152509\n",
            "\tTraining batch 19 loss: 1.249219\n",
            "\tTraining batch 20 loss: 1.187343\n",
            "\tTraining batch 21 loss: 1.217883\n",
            "\tTraining batch 22 loss: 0.984555\n",
            "\tTraining batch 23 loss: 0.789585\n",
            "\tTraining batch 24 loss: 0.908780\n",
            "\tTraining batch 25 loss: 1.228039\n",
            "\tTraining batch 26 loss: 1.142574\n",
            "\tTraining batch 27 loss: 0.764339\n",
            "\tTraining batch 28 loss: 1.226073\n",
            "\tTraining batch 29 loss: 1.170204\n",
            "\tTraining batch 30 loss: 1.081177\n",
            "\tTraining batch 31 loss: 1.104756\n",
            "\tTraining batch 32 loss: 1.095045\n",
            "\tTraining batch 33 loss: 1.131063\n",
            "\tTraining batch 34 loss: 0.964848\n",
            "\tTraining batch 35 loss: 0.861226\n",
            "\tTraining batch 36 loss: 1.306696\n",
            "\tTraining batch 37 loss: 1.184028\n",
            "\tTraining batch 38 loss: 0.624576\n",
            "\tTraining batch 39 loss: 1.369041\n",
            "\tTraining batch 40 loss: 0.865198\n",
            "\tTraining batch 41 loss: 0.734675\n",
            "\tTraining batch 42 loss: 1.112251\n",
            "\tTraining batch 43 loss: 0.980738\n",
            "\tTraining batch 44 loss: 1.068101\n",
            "\tTraining batch 45 loss: 1.076815\n",
            "\tTraining batch 46 loss: 1.064282\n",
            "\tTraining batch 47 loss: 1.344658\n",
            "\tTraining batch 48 loss: 1.233166\n",
            "\tTraining batch 49 loss: 1.215541\n",
            "\tTraining batch 50 loss: 1.040490\n",
            "\tTraining batch 51 loss: 0.913465\n",
            "\tTraining batch 52 loss: 1.153323\n",
            "\tTraining batch 53 loss: 0.701747\n",
            "\tTraining batch 54 loss: 1.153606\n",
            "\tTraining batch 55 loss: 1.004179\n",
            "\tTraining batch 56 loss: 0.932444\n",
            "\tTraining batch 57 loss: 1.172851\n",
            "\tTraining batch 58 loss: 0.782708\n",
            "\tTraining batch 59 loss: 1.144214\n",
            "\tTraining batch 60 loss: 1.255025\n",
            "\tTraining batch 61 loss: 0.986985\n",
            "\tTraining batch 62 loss: 1.155682\n",
            "\tTraining batch 63 loss: 1.006829\n",
            "\tTraining batch 64 loss: 0.833721\n",
            "\tTraining batch 65 loss: 1.236947\n",
            "\tTraining batch 66 loss: 0.915159\n",
            "\tTraining batch 67 loss: 0.686569\n",
            "\tTraining batch 68 loss: 0.829563\n",
            "\tTraining batch 69 loss: 1.383984\n",
            "\tTraining batch 70 loss: 1.115464\n",
            "\tTraining batch 71 loss: 0.937409\n",
            "\tTraining batch 72 loss: 0.689834\n",
            "\tTraining batch 73 loss: 1.164897\n",
            "\tTraining batch 74 loss: 1.038492\n",
            "\tTraining batch 75 loss: 0.984443\n",
            "\tTraining batch 76 loss: 0.878774\n",
            "\tTraining batch 77 loss: 1.291211\n",
            "\tTraining batch 78 loss: 1.318542\n",
            "\tTraining batch 79 loss: 1.187257\n",
            "\tTraining batch 80 loss: 0.698889\n",
            "\tTraining batch 81 loss: 1.085846\n",
            "\tTraining batch 82 loss: 0.834942\n",
            "\tTraining batch 83 loss: 1.002197\n",
            "\tTraining batch 84 loss: 1.132334\n",
            "\tTraining batch 85 loss: 0.901734\n",
            "\tTraining batch 86 loss: 0.816505\n",
            "\tTraining batch 87 loss: 1.020816\n",
            "\tTraining batch 88 loss: 1.499061\n",
            "\tTraining batch 89 loss: 1.068456\n",
            "\tTraining batch 90 loss: 0.974163\n",
            "\tTraining batch 91 loss: 0.843465\n",
            "\tTraining batch 92 loss: 0.773705\n",
            "\tTraining batch 93 loss: 1.082844\n",
            "\tTraining batch 94 loss: 0.959956\n",
            "\tTraining batch 95 loss: 0.879435\n",
            "\tTraining batch 96 loss: 1.009066\n",
            "\tTraining batch 97 loss: 1.112159\n",
            "\tTraining batch 98 loss: 0.906845\n",
            "\tTraining batch 99 loss: 1.227663\n",
            "\tTraining batch 100 loss: 1.105213\n",
            "\tTraining batch 101 loss: 1.018947\n",
            "\tTraining batch 102 loss: 1.016276\n",
            "\tTraining batch 103 loss: 1.286695\n",
            "\tTraining batch 104 loss: 1.154946\n",
            "\tTraining batch 105 loss: 1.218396\n",
            "\tTraining batch 106 loss: 0.864792\n",
            "\tTraining batch 107 loss: 1.030044\n",
            "\tTraining batch 108 loss: 1.080660\n",
            "\tTraining batch 109 loss: 0.890001\n",
            "\tTraining batch 110 loss: 1.267221\n",
            "\tTraining batch 111 loss: 1.024852\n",
            "\tTraining batch 112 loss: 0.848099\n",
            "\tTraining batch 113 loss: 1.452672\n",
            "\tTraining batch 114 loss: 1.131865\n",
            "\tTraining batch 115 loss: 1.168753\n",
            "\tTraining batch 116 loss: 1.006268\n",
            "\tTraining batch 117 loss: 1.299880\n",
            "\tTraining batch 118 loss: 1.106403\n",
            "\tTraining batch 119 loss: 1.330745\n",
            "\tTraining batch 120 loss: 1.312585\n",
            "\tTraining batch 121 loss: 0.897312\n",
            "\tTraining batch 122 loss: 1.323450\n",
            "\tTraining batch 123 loss: 0.968615\n",
            "\tTraining batch 124 loss: 1.057539\n",
            "\tTraining batch 125 loss: 1.254334\n",
            "\tTraining batch 126 loss: 1.139133\n",
            "\tTraining batch 127 loss: 1.282526\n",
            "\tTraining batch 128 loss: 1.416758\n",
            "\tTraining batch 129 loss: 0.939772\n",
            "\tTraining batch 130 loss: 0.968169\n",
            "\tTraining batch 131 loss: 1.160119\n",
            "\tTraining batch 132 loss: 0.819227\n",
            "\tTraining batch 133 loss: 1.218756\n",
            "\tTraining batch 134 loss: 1.106174\n",
            "\tTraining batch 135 loss: 1.194862\n",
            "\tTraining batch 136 loss: 1.210365\n",
            "\tTraining batch 137 loss: 1.071241\n",
            "\tTraining batch 138 loss: 0.855757\n",
            "\tTraining batch 139 loss: 1.001258\n",
            "\tTraining batch 140 loss: 1.238171\n",
            "\tTraining batch 141 loss: 1.048078\n",
            "\tTraining batch 142 loss: 1.121574\n",
            "\tTraining batch 143 loss: 1.076179\n",
            "\tTraining batch 144 loss: 0.991295\n",
            "\tTraining batch 145 loss: 0.969283\n",
            "\tTraining batch 146 loss: 1.276792\n",
            "\tTraining batch 147 loss: 1.382294\n",
            "\tTraining batch 148 loss: 1.005385\n",
            "\tTraining batch 149 loss: 1.068338\n",
            "\tTraining batch 150 loss: 0.959801\n",
            "\tTraining batch 151 loss: 0.808890\n",
            "\tTraining batch 152 loss: 0.936400\n",
            "\tTraining batch 153 loss: 1.207342\n",
            "\tTraining batch 154 loss: 0.964730\n",
            "\tTraining batch 155 loss: 0.886981\n",
            "\tTraining batch 156 loss: 0.928882\n",
            "\tTraining batch 157 loss: 0.699912\n",
            "\tTraining batch 158 loss: 0.943955\n",
            "\tTraining batch 159 loss: 0.859172\n",
            "\tTraining batch 160 loss: 0.991480\n",
            "\tTraining batch 161 loss: 0.903861\n",
            "\tTraining batch 162 loss: 1.133797\n",
            "\tTraining batch 163 loss: 0.932488\n",
            "\tTraining batch 164 loss: 1.148040\n",
            "\tTraining batch 165 loss: 0.904129\n",
            "\tTraining batch 166 loss: 0.896756\n",
            "\tTraining batch 167 loss: 0.793840\n",
            "\tTraining batch 168 loss: 1.234803\n",
            "\tTraining batch 169 loss: 1.142480\n",
            "\tTraining batch 170 loss: 1.072910\n",
            "\tTraining batch 171 loss: 0.995661\n",
            "\tTraining batch 172 loss: 0.883171\n",
            "\tTraining batch 173 loss: 1.074617\n",
            "\tTraining batch 174 loss: 0.970827\n",
            "\tTraining batch 175 loss: 1.210069\n",
            "\tTraining batch 176 loss: 0.977335\n",
            "\tTraining batch 177 loss: 1.163842\n",
            "\tTraining batch 178 loss: 0.837113\n",
            "\tTraining batch 179 loss: 1.083096\n",
            "\tTraining batch 180 loss: 1.228644\n",
            "\tTraining batch 181 loss: 0.872274\n",
            "\tTraining batch 182 loss: 1.160523\n",
            "\tTraining batch 183 loss: 1.009943\n",
            "\tTraining batch 184 loss: 1.317500\n",
            "\tTraining batch 185 loss: 0.625086\n",
            "\tTraining batch 186 loss: 1.084615\n",
            "\tTraining batch 187 loss: 1.013865\n",
            "\tTraining batch 188 loss: 0.677357\n",
            "\tTraining batch 189 loss: 0.866634\n",
            "\tTraining batch 190 loss: 1.007439\n",
            "\tTraining batch 191 loss: 1.017000\n",
            "\tTraining batch 192 loss: 1.094040\n",
            "\tTraining batch 193 loss: 1.151383\n",
            "\tTraining batch 194 loss: 0.535424\n",
            "\tTraining batch 195 loss: 1.150259\n",
            "\tTraining batch 196 loss: 1.469140\n",
            "\tTraining batch 197 loss: 1.088830\n",
            "\tTraining batch 198 loss: 0.917073\n",
            "\tTraining batch 199 loss: 1.140485\n",
            "\tTraining batch 200 loss: 0.816578\n",
            "\tTraining batch 201 loss: 1.615952\n",
            "\tTraining batch 202 loss: 1.485352\n",
            "\tTraining batch 203 loss: 1.395486\n",
            "\tTraining batch 204 loss: 0.946515\n",
            "\tTraining batch 205 loss: 1.274107\n",
            "\tTraining batch 206 loss: 0.966864\n",
            "\tTraining batch 207 loss: 0.988027\n",
            "\tTraining batch 208 loss: 0.969777\n",
            "\tTraining batch 209 loss: 1.030491\n",
            "\tTraining batch 210 loss: 1.019681\n",
            "\tTraining batch 211 loss: 1.029917\n",
            "\tTraining batch 212 loss: 1.011282\n",
            "\tTraining batch 213 loss: 1.216389\n",
            "\tTraining batch 214 loss: 1.065563\n",
            "\tTraining batch 215 loss: 1.195791\n",
            "\tTraining batch 216 loss: 0.884307\n",
            "\tTraining batch 217 loss: 1.010950\n",
            "\tTraining batch 218 loss: 0.987024\n",
            "\tTraining batch 219 loss: 1.005739\n",
            "\tTraining batch 220 loss: 0.857124\n",
            "\tTraining batch 221 loss: 0.966026\n",
            "\tTraining batch 222 loss: 0.837400\n",
            "\tTraining batch 223 loss: 1.013747\n",
            "\tTraining batch 224 loss: 0.963034\n",
            "\tTraining batch 225 loss: 1.265070\n",
            "\tTraining batch 226 loss: 0.916800\n",
            "\tTraining batch 227 loss: 1.254051\n",
            "\tTraining batch 228 loss: 0.726905\n",
            "\tTraining batch 229 loss: 0.930323\n",
            "\tTraining batch 230 loss: 0.808065\n",
            "\tTraining batch 231 loss: 1.212059\n",
            "\tTraining batch 232 loss: 1.274249\n",
            "\tTraining batch 233 loss: 1.082265\n",
            "\tTraining batch 234 loss: 0.835660\n",
            "\tTraining batch 235 loss: 1.221219\n",
            "\tTraining batch 236 loss: 0.860365\n",
            "\tTraining batch 237 loss: 0.760008\n",
            "\tTraining batch 238 loss: 1.023950\n",
            "\tTraining batch 239 loss: 1.017407\n",
            "\tTraining batch 240 loss: 0.815174\n",
            "\tTraining batch 241 loss: 1.104214\n",
            "\tTraining batch 242 loss: 0.833999\n",
            "\tTraining batch 243 loss: 1.026691\n",
            "\tTraining batch 244 loss: 0.960923\n",
            "\tTraining batch 245 loss: 0.918406\n",
            "\tTraining batch 246 loss: 0.927294\n",
            "\tTraining batch 247 loss: 1.123868\n",
            "\tTraining batch 248 loss: 0.949679\n",
            "\tTraining batch 249 loss: 0.924519\n",
            "\tTraining batch 250 loss: 0.783138\n",
            "\tTraining batch 251 loss: 0.715787\n",
            "\tTraining batch 252 loss: 1.069320\n",
            "\tTraining batch 253 loss: 0.751773\n",
            "\tTraining batch 254 loss: 1.346896\n",
            "\tTraining batch 255 loss: 0.652774\n",
            "\tTraining batch 256 loss: 1.065806\n",
            "\tTraining batch 257 loss: 0.861375\n",
            "\tTraining batch 258 loss: 0.881917\n",
            "\tTraining batch 259 loss: 0.991283\n",
            "\tTraining batch 260 loss: 1.035979\n",
            "\tTraining batch 261 loss: 0.950165\n",
            "\tTraining batch 262 loss: 1.085939\n",
            "\tTraining batch 263 loss: 0.786264\n",
            "\tTraining batch 264 loss: 1.316454\n",
            "\tTraining batch 265 loss: 1.224744\n",
            "\tTraining batch 266 loss: 0.898505\n",
            "\tTraining batch 267 loss: 1.008359\n",
            "\tTraining batch 268 loss: 1.059151\n",
            "\tTraining batch 269 loss: 0.979289\n",
            "\tTraining batch 270 loss: 0.757282\n",
            "\tTraining batch 271 loss: 1.016119\n",
            "\tTraining batch 272 loss: 1.068699\n",
            "\tTraining batch 273 loss: 0.961150\n",
            "\tTraining batch 274 loss: 1.162640\n",
            "\tTraining batch 275 loss: 0.782112\n",
            "\tTraining batch 276 loss: 0.921610\n",
            "\tTraining batch 277 loss: 0.834010\n",
            "\tTraining batch 278 loss: 1.109082\n",
            "\tTraining batch 279 loss: 0.814652\n",
            "\tTraining batch 280 loss: 0.949527\n",
            "\tTraining batch 281 loss: 1.122231\n",
            "\tTraining batch 282 loss: 0.945677\n",
            "\tTraining batch 283 loss: 1.045786\n",
            "\tTraining batch 284 loss: 0.974564\n",
            "\tTraining batch 285 loss: 0.897527\n",
            "\tTraining batch 286 loss: 1.162230\n",
            "\tTraining batch 287 loss: 0.716710\n",
            "\tTraining batch 288 loss: 0.923005\n",
            "\tTraining batch 289 loss: 0.897766\n",
            "\tTraining batch 290 loss: 0.851101\n",
            "\tTraining batch 291 loss: 1.234640\n",
            "\tTraining batch 292 loss: 0.765465\n",
            "\tTraining batch 293 loss: 0.885344\n",
            "\tTraining batch 294 loss: 1.084643\n",
            "\tTraining batch 295 loss: 1.189906\n",
            "\tTraining batch 296 loss: 0.911584\n",
            "\tTraining batch 297 loss: 1.378880\n",
            "\tTraining batch 298 loss: 0.752402\n",
            "\tTraining batch 299 loss: 1.194338\n",
            "\tTraining batch 300 loss: 1.046555\n",
            "\tTraining batch 301 loss: 1.180217\n",
            "\tTraining batch 302 loss: 0.955076\n",
            "\tTraining batch 303 loss: 1.067785\n",
            "\tTraining batch 304 loss: 1.176270\n",
            "\tTraining batch 305 loss: 1.073966\n",
            "\tTraining batch 306 loss: 0.988207\n",
            "\tTraining batch 307 loss: 0.851644\n",
            "\tTraining batch 308 loss: 1.241255\n",
            "\tTraining batch 309 loss: 0.852088\n",
            "\tTraining batch 310 loss: 0.666519\n",
            "\tTraining batch 311 loss: 0.915423\n",
            "\tTraining batch 312 loss: 0.822293\n",
            "\tTraining batch 313 loss: 1.038170\n",
            "\tTraining batch 314 loss: 0.898594\n",
            "\tTraining batch 315 loss: 0.995815\n",
            "\tTraining batch 316 loss: 1.128365\n",
            "\tTraining batch 317 loss: 0.847089\n",
            "\tTraining batch 318 loss: 0.919420\n",
            "\tTraining batch 319 loss: 1.025361\n",
            "\tTraining batch 320 loss: 1.139310\n",
            "\tTraining batch 321 loss: 0.986489\n",
            "\tTraining batch 322 loss: 1.030889\n",
            "\tTraining batch 323 loss: 0.954600\n",
            "\tTraining batch 324 loss: 1.007424\n",
            "\tTraining batch 325 loss: 0.871127\n",
            "\tTraining batch 326 loss: 1.035315\n",
            "\tTraining batch 327 loss: 0.950571\n",
            "\tTraining batch 328 loss: 1.285040\n",
            "\tTraining batch 329 loss: 1.161386\n",
            "\tTraining batch 330 loss: 1.203198\n",
            "\tTraining batch 331 loss: 0.783487\n",
            "\tTraining batch 332 loss: 0.769243\n",
            "\tTraining batch 333 loss: 0.798229\n",
            "\tTraining batch 334 loss: 1.064350\n",
            "\tTraining batch 335 loss: 0.677374\n",
            "\tTraining batch 336 loss: 1.099163\n",
            "\tTraining batch 337 loss: 0.846795\n",
            "\tTraining batch 338 loss: 0.971089\n",
            "\tTraining batch 339 loss: 0.753596\n",
            "\tTraining batch 340 loss: 1.197217\n",
            "\tTraining batch 341 loss: 0.916674\n",
            "\tTraining batch 342 loss: 1.326072\n",
            "\tTraining batch 343 loss: 0.922240\n",
            "\tTraining batch 344 loss: 1.147522\n",
            "\tTraining batch 345 loss: 1.203931\n",
            "\tTraining batch 346 loss: 1.219920\n",
            "\tTraining batch 347 loss: 1.185995\n",
            "\tTraining batch 348 loss: 1.265015\n",
            "\tTraining batch 349 loss: 1.053504\n",
            "\tTraining batch 350 loss: 1.148283\n",
            "\tTraining batch 351 loss: 1.132075\n",
            "\tTraining batch 352 loss: 1.393217\n",
            "\tTraining batch 353 loss: 0.759288\n",
            "\tTraining batch 354 loss: 0.858810\n",
            "\tTraining batch 355 loss: 1.144799\n",
            "\tTraining batch 356 loss: 1.351080\n",
            "\tTraining batch 357 loss: 1.251425\n",
            "\tTraining batch 358 loss: 0.821752\n",
            "\tTraining batch 359 loss: 0.946271\n",
            "\tTraining batch 360 loss: 1.218093\n",
            "\tTraining batch 361 loss: 1.057154\n",
            "\tTraining batch 362 loss: 0.983505\n",
            "\tTraining batch 363 loss: 0.874587\n",
            "\tTraining batch 364 loss: 0.968527\n",
            "\tTraining batch 365 loss: 1.265185\n",
            "\tTraining batch 366 loss: 0.968488\n",
            "\tTraining batch 367 loss: 0.668001\n",
            "\tTraining batch 368 loss: 0.755109\n",
            "\tTraining batch 369 loss: 0.888785\n",
            "\tTraining batch 370 loss: 0.880421\n",
            "\tTraining batch 371 loss: 1.043628\n",
            "\tTraining batch 372 loss: 1.231964\n",
            "\tTraining batch 373 loss: 0.973069\n",
            "\tTraining batch 374 loss: 0.676238\n",
            "\tTraining batch 375 loss: 0.673249\n",
            "\tTraining batch 376 loss: 1.071762\n",
            "\tTraining batch 377 loss: 0.935391\n",
            "\tTraining batch 378 loss: 0.933407\n",
            "\tTraining batch 379 loss: 1.074191\n",
            "\tTraining batch 380 loss: 1.031654\n",
            "\tTraining batch 381 loss: 1.024437\n",
            "\tTraining batch 382 loss: 0.899938\n",
            "\tTraining batch 383 loss: 0.956516\n",
            "\tTraining batch 384 loss: 0.656925\n",
            "\tTraining batch 385 loss: 0.719460\n",
            "\tTraining batch 386 loss: 0.670995\n",
            "\tTraining batch 387 loss: 0.869499\n",
            "\tTraining batch 388 loss: 1.304898\n",
            "\tTraining batch 389 loss: 1.026762\n",
            "\tTraining batch 390 loss: 1.170084\n",
            "\tTraining batch 391 loss: 0.829805\n",
            "\tTraining batch 392 loss: 0.763634\n",
            "\tTraining batch 393 loss: 1.274647\n",
            "\tTraining batch 394 loss: 0.885920\n",
            "\tTraining batch 395 loss: 0.722681\n",
            "\tTraining batch 396 loss: 0.962977\n",
            "\tTraining batch 397 loss: 1.130631\n",
            "\tTraining batch 398 loss: 1.411975\n",
            "\tTraining batch 399 loss: 1.111209\n",
            "\tTraining batch 400 loss: 1.414514\n",
            "\tTraining batch 401 loss: 1.254087\n",
            "\tTraining batch 402 loss: 0.867237\n",
            "\tTraining batch 403 loss: 1.092744\n",
            "\tTraining batch 404 loss: 0.872218\n",
            "\tTraining batch 405 loss: 0.987080\n",
            "\tTraining batch 406 loss: 0.956273\n",
            "\tTraining batch 407 loss: 0.829032\n",
            "\tTraining batch 408 loss: 1.144472\n",
            "\tTraining batch 409 loss: 1.076433\n",
            "\tTraining batch 410 loss: 0.910796\n",
            "\tTraining batch 411 loss: 0.748748\n",
            "\tTraining batch 412 loss: 1.107551\n",
            "\tTraining batch 413 loss: 1.040672\n",
            "\tTraining batch 414 loss: 1.161723\n",
            "\tTraining batch 415 loss: 1.244372\n",
            "\tTraining batch 416 loss: 1.027194\n",
            "\tTraining batch 417 loss: 1.358784\n",
            "\tTraining batch 418 loss: 1.412226\n",
            "\tTraining batch 419 loss: 1.292193\n",
            "\tTraining batch 420 loss: 1.041081\n",
            "\tTraining batch 421 loss: 1.360757\n",
            "\tTraining batch 422 loss: 0.882960\n",
            "\tTraining batch 423 loss: 0.044802\n",
            "Training set: Average loss: 1.026070\n",
            "Validation set: Average loss: 1.036006, Accuracy: 3305/5276 (63%)\n",
            "\n",
            "Epoch 5\n",
            "\tTraining batch 1 loss: 1.297795\n",
            "\tTraining batch 2 loss: 1.157273\n",
            "\tTraining batch 3 loss: 1.180452\n",
            "\tTraining batch 4 loss: 1.202313\n",
            "\tTraining batch 5 loss: 1.175931\n",
            "\tTraining batch 6 loss: 1.364082\n",
            "\tTraining batch 7 loss: 1.122563\n",
            "\tTraining batch 8 loss: 1.377692\n",
            "\tTraining batch 9 loss: 1.219225\n",
            "\tTraining batch 10 loss: 1.065284\n",
            "\tTraining batch 11 loss: 0.779345\n",
            "\tTraining batch 12 loss: 1.414868\n",
            "\tTraining batch 13 loss: 1.323159\n",
            "\tTraining batch 14 loss: 1.003812\n",
            "\tTraining batch 15 loss: 1.353961\n",
            "\tTraining batch 16 loss: 0.939473\n",
            "\tTraining batch 17 loss: 0.896885\n",
            "\tTraining batch 18 loss: 1.146874\n",
            "\tTraining batch 19 loss: 1.107487\n",
            "\tTraining batch 20 loss: 1.098250\n",
            "\tTraining batch 21 loss: 0.922162\n",
            "\tTraining batch 22 loss: 0.815159\n",
            "\tTraining batch 23 loss: 0.737545\n",
            "\tTraining batch 24 loss: 0.913973\n",
            "\tTraining batch 25 loss: 0.863992\n",
            "\tTraining batch 26 loss: 0.949239\n",
            "\tTraining batch 27 loss: 0.863658\n",
            "\tTraining batch 28 loss: 0.980700\n",
            "\tTraining batch 29 loss: 0.988927\n",
            "\tTraining batch 30 loss: 0.916283\n",
            "\tTraining batch 31 loss: 1.136788\n",
            "\tTraining batch 32 loss: 1.048826\n",
            "\tTraining batch 33 loss: 1.157476\n",
            "\tTraining batch 34 loss: 0.971533\n",
            "\tTraining batch 35 loss: 0.971735\n",
            "\tTraining batch 36 loss: 1.205109\n",
            "\tTraining batch 37 loss: 1.049772\n",
            "\tTraining batch 38 loss: 0.934507\n",
            "\tTraining batch 39 loss: 1.282555\n",
            "\tTraining batch 40 loss: 0.786873\n",
            "\tTraining batch 41 loss: 0.860616\n",
            "\tTraining batch 42 loss: 1.180518\n",
            "\tTraining batch 43 loss: 0.728260\n",
            "\tTraining batch 44 loss: 1.057843\n",
            "\tTraining batch 45 loss: 0.840996\n",
            "\tTraining batch 46 loss: 0.784457\n",
            "\tTraining batch 47 loss: 1.579165\n",
            "\tTraining batch 48 loss: 1.024226\n",
            "\tTraining batch 49 loss: 1.223830\n",
            "\tTraining batch 50 loss: 0.974045\n",
            "\tTraining batch 51 loss: 0.962061\n",
            "\tTraining batch 52 loss: 0.968469\n",
            "\tTraining batch 53 loss: 0.578628\n",
            "\tTraining batch 54 loss: 1.032811\n",
            "\tTraining batch 55 loss: 0.926014\n",
            "\tTraining batch 56 loss: 0.824118\n",
            "\tTraining batch 57 loss: 1.140332\n",
            "\tTraining batch 58 loss: 0.684249\n",
            "\tTraining batch 59 loss: 1.175179\n",
            "\tTraining batch 60 loss: 1.111052\n",
            "\tTraining batch 61 loss: 1.066916\n",
            "\tTraining batch 62 loss: 1.071050\n",
            "\tTraining batch 63 loss: 1.139413\n",
            "\tTraining batch 64 loss: 0.794923\n",
            "\tTraining batch 65 loss: 1.133676\n",
            "\tTraining batch 66 loss: 1.009454\n",
            "\tTraining batch 67 loss: 0.667436\n",
            "\tTraining batch 68 loss: 0.814759\n",
            "\tTraining batch 69 loss: 1.123586\n",
            "\tTraining batch 70 loss: 1.119965\n",
            "\tTraining batch 71 loss: 0.968455\n",
            "\tTraining batch 72 loss: 0.799902\n",
            "\tTraining batch 73 loss: 0.945883\n",
            "\tTraining batch 74 loss: 1.150794\n",
            "\tTraining batch 75 loss: 1.098182\n",
            "\tTraining batch 76 loss: 1.050621\n",
            "\tTraining batch 77 loss: 1.152690\n",
            "\tTraining batch 78 loss: 1.224959\n",
            "\tTraining batch 79 loss: 0.968979\n",
            "\tTraining batch 80 loss: 0.745384\n",
            "\tTraining batch 81 loss: 1.045377\n",
            "\tTraining batch 82 loss: 0.842649\n",
            "\tTraining batch 83 loss: 1.089140\n",
            "\tTraining batch 84 loss: 1.149547\n",
            "\tTraining batch 85 loss: 0.831187\n",
            "\tTraining batch 86 loss: 0.846330\n",
            "\tTraining batch 87 loss: 0.935622\n",
            "\tTraining batch 88 loss: 1.226022\n",
            "\tTraining batch 89 loss: 1.126363\n",
            "\tTraining batch 90 loss: 1.024711\n",
            "\tTraining batch 91 loss: 0.852466\n",
            "\tTraining batch 92 loss: 0.889170\n",
            "\tTraining batch 93 loss: 1.050897\n",
            "\tTraining batch 94 loss: 1.017290\n",
            "\tTraining batch 95 loss: 0.863877\n",
            "\tTraining batch 96 loss: 0.969324\n",
            "\tTraining batch 97 loss: 1.196673\n",
            "\tTraining batch 98 loss: 0.984548\n",
            "\tTraining batch 99 loss: 1.103528\n",
            "\tTraining batch 100 loss: 1.120524\n",
            "\tTraining batch 101 loss: 1.037863\n",
            "\tTraining batch 102 loss: 0.969692\n",
            "\tTraining batch 103 loss: 1.229061\n",
            "\tTraining batch 104 loss: 1.122650\n",
            "\tTraining batch 105 loss: 1.073619\n",
            "\tTraining batch 106 loss: 1.068821\n",
            "\tTraining batch 107 loss: 1.256708\n",
            "\tTraining batch 108 loss: 1.137643\n",
            "\tTraining batch 109 loss: 0.836284\n",
            "\tTraining batch 110 loss: 1.087304\n",
            "\tTraining batch 111 loss: 1.053390\n",
            "\tTraining batch 112 loss: 0.800733\n",
            "\tTraining batch 113 loss: 1.342108\n",
            "\tTraining batch 114 loss: 0.857075\n",
            "\tTraining batch 115 loss: 0.899214\n",
            "\tTraining batch 116 loss: 0.839328\n",
            "\tTraining batch 117 loss: 1.168548\n",
            "\tTraining batch 118 loss: 0.742158\n",
            "\tTraining batch 119 loss: 1.130389\n",
            "\tTraining batch 120 loss: 1.068675\n",
            "\tTraining batch 121 loss: 0.770725\n",
            "\tTraining batch 122 loss: 1.007118\n",
            "\tTraining batch 123 loss: 0.831993\n",
            "\tTraining batch 124 loss: 0.892096\n",
            "\tTraining batch 125 loss: 1.026294\n",
            "\tTraining batch 126 loss: 1.055943\n",
            "\tTraining batch 127 loss: 1.142431\n",
            "\tTraining batch 128 loss: 1.159070\n",
            "\tTraining batch 129 loss: 0.977952\n",
            "\tTraining batch 130 loss: 0.994853\n",
            "\tTraining batch 131 loss: 0.996356\n",
            "\tTraining batch 132 loss: 0.944394\n",
            "\tTraining batch 133 loss: 0.879552\n",
            "\tTraining batch 134 loss: 1.086962\n",
            "\tTraining batch 135 loss: 1.317549\n",
            "\tTraining batch 136 loss: 1.153786\n",
            "\tTraining batch 137 loss: 1.064781\n",
            "\tTraining batch 138 loss: 0.990749\n",
            "\tTraining batch 139 loss: 0.920439\n",
            "\tTraining batch 140 loss: 0.975307\n",
            "\tTraining batch 141 loss: 1.060500\n",
            "\tTraining batch 142 loss: 1.102664\n",
            "\tTraining batch 143 loss: 1.064246\n",
            "\tTraining batch 144 loss: 0.951182\n",
            "\tTraining batch 145 loss: 1.061144\n",
            "\tTraining batch 146 loss: 1.374996\n",
            "\tTraining batch 147 loss: 1.273384\n",
            "\tTraining batch 148 loss: 0.970401\n",
            "\tTraining batch 149 loss: 0.752694\n",
            "\tTraining batch 150 loss: 0.881870\n",
            "\tTraining batch 151 loss: 0.757431\n",
            "\tTraining batch 152 loss: 1.043267\n",
            "\tTraining batch 153 loss: 1.042528\n",
            "\tTraining batch 154 loss: 1.244043\n",
            "\tTraining batch 155 loss: 0.974867\n",
            "\tTraining batch 156 loss: 0.930217\n",
            "\tTraining batch 157 loss: 0.700657\n",
            "\tTraining batch 158 loss: 1.169360\n",
            "\tTraining batch 159 loss: 1.037160\n",
            "\tTraining batch 160 loss: 0.901775\n",
            "\tTraining batch 161 loss: 0.915988\n",
            "\tTraining batch 162 loss: 1.100782\n",
            "\tTraining batch 163 loss: 0.970468\n",
            "\tTraining batch 164 loss: 0.908260\n",
            "\tTraining batch 165 loss: 0.936776\n",
            "\tTraining batch 166 loss: 1.050112\n",
            "\tTraining batch 167 loss: 0.832372\n",
            "\tTraining batch 168 loss: 1.128977\n",
            "\tTraining batch 169 loss: 1.166347\n",
            "\tTraining batch 170 loss: 1.008292\n",
            "\tTraining batch 171 loss: 0.892104\n",
            "\tTraining batch 172 loss: 0.945051\n",
            "\tTraining batch 173 loss: 0.928200\n",
            "\tTraining batch 174 loss: 1.050739\n",
            "\tTraining batch 175 loss: 1.095024\n",
            "\tTraining batch 176 loss: 1.075391\n",
            "\tTraining batch 177 loss: 1.177446\n",
            "\tTraining batch 178 loss: 0.806654\n",
            "\tTraining batch 179 loss: 0.955931\n",
            "\tTraining batch 180 loss: 1.262923\n",
            "\tTraining batch 181 loss: 0.989575\n",
            "\tTraining batch 182 loss: 1.175322\n",
            "\tTraining batch 183 loss: 1.208127\n",
            "\tTraining batch 184 loss: 1.381206\n",
            "\tTraining batch 185 loss: 0.705267\n",
            "\tTraining batch 186 loss: 0.918388\n",
            "\tTraining batch 187 loss: 0.941662\n",
            "\tTraining batch 188 loss: 0.955466\n",
            "\tTraining batch 189 loss: 0.854462\n",
            "\tTraining batch 190 loss: 1.106218\n",
            "\tTraining batch 191 loss: 0.985887\n",
            "\tTraining batch 192 loss: 1.163284\n",
            "\tTraining batch 193 loss: 1.096825\n",
            "\tTraining batch 194 loss: 0.622524\n",
            "\tTraining batch 195 loss: 1.027790\n",
            "\tTraining batch 196 loss: 1.314310\n",
            "\tTraining batch 197 loss: 1.097612\n",
            "\tTraining batch 198 loss: 0.765190\n",
            "\tTraining batch 199 loss: 1.066043\n",
            "\tTraining batch 200 loss: 0.845578\n",
            "\tTraining batch 201 loss: 1.370590\n",
            "\tTraining batch 202 loss: 1.381711\n",
            "\tTraining batch 203 loss: 1.551470\n",
            "\tTraining batch 204 loss: 0.992358\n",
            "\tTraining batch 205 loss: 0.969422\n",
            "\tTraining batch 206 loss: 0.852620\n",
            "\tTraining batch 207 loss: 1.007016\n",
            "\tTraining batch 208 loss: 1.238612\n",
            "\tTraining batch 209 loss: 1.036321\n",
            "\tTraining batch 210 loss: 1.036491\n",
            "\tTraining batch 211 loss: 1.104292\n",
            "\tTraining batch 212 loss: 1.259171\n",
            "\tTraining batch 213 loss: 1.059184\n",
            "\tTraining batch 214 loss: 0.955365\n",
            "\tTraining batch 215 loss: 1.185445\n",
            "\tTraining batch 216 loss: 0.886950\n",
            "\tTraining batch 217 loss: 0.907591\n",
            "\tTraining batch 218 loss: 1.015646\n",
            "\tTraining batch 219 loss: 0.999752\n",
            "\tTraining batch 220 loss: 0.808901\n",
            "\tTraining batch 221 loss: 1.046454\n",
            "\tTraining batch 222 loss: 0.780226\n",
            "\tTraining batch 223 loss: 1.203820\n",
            "\tTraining batch 224 loss: 0.901521\n",
            "\tTraining batch 225 loss: 1.207058\n",
            "\tTraining batch 226 loss: 0.799022\n",
            "\tTraining batch 227 loss: 1.230471\n",
            "\tTraining batch 228 loss: 0.705529\n",
            "\tTraining batch 229 loss: 0.948937\n",
            "\tTraining batch 230 loss: 0.517584\n",
            "\tTraining batch 231 loss: 0.981317\n",
            "\tTraining batch 232 loss: 1.333467\n",
            "\tTraining batch 233 loss: 0.990474\n",
            "\tTraining batch 234 loss: 0.835784\n",
            "\tTraining batch 235 loss: 1.063110\n",
            "\tTraining batch 236 loss: 0.811147\n",
            "\tTraining batch 237 loss: 0.660285\n",
            "\tTraining batch 238 loss: 1.169847\n",
            "\tTraining batch 239 loss: 0.954557\n",
            "\tTraining batch 240 loss: 0.943720\n",
            "\tTraining batch 241 loss: 0.892464\n",
            "\tTraining batch 242 loss: 0.883383\n",
            "\tTraining batch 243 loss: 1.107339\n",
            "\tTraining batch 244 loss: 1.000197\n",
            "\tTraining batch 245 loss: 0.884800\n",
            "\tTraining batch 246 loss: 0.875305\n",
            "\tTraining batch 247 loss: 1.060983\n",
            "\tTraining batch 248 loss: 0.932911\n",
            "\tTraining batch 249 loss: 0.876073\n",
            "\tTraining batch 250 loss: 0.787081\n",
            "\tTraining batch 251 loss: 0.691256\n",
            "\tTraining batch 252 loss: 1.080868\n",
            "\tTraining batch 253 loss: 0.877096\n",
            "\tTraining batch 254 loss: 1.248333\n",
            "\tTraining batch 255 loss: 0.814958\n",
            "\tTraining batch 256 loss: 0.903234\n",
            "\tTraining batch 257 loss: 0.798049\n",
            "\tTraining batch 258 loss: 1.096389\n",
            "\tTraining batch 259 loss: 0.869135\n",
            "\tTraining batch 260 loss: 1.146774\n",
            "\tTraining batch 261 loss: 0.904217\n",
            "\tTraining batch 262 loss: 1.006048\n",
            "\tTraining batch 263 loss: 1.038741\n",
            "\tTraining batch 264 loss: 1.226094\n",
            "\tTraining batch 265 loss: 1.188479\n",
            "\tTraining batch 266 loss: 0.991102\n",
            "\tTraining batch 267 loss: 1.348751\n",
            "\tTraining batch 268 loss: 1.016972\n",
            "\tTraining batch 269 loss: 1.221143\n",
            "\tTraining batch 270 loss: 0.738362\n",
            "\tTraining batch 271 loss: 0.964602\n",
            "\tTraining batch 272 loss: 1.381000\n",
            "\tTraining batch 273 loss: 0.776119\n",
            "\tTraining batch 274 loss: 1.290103\n",
            "\tTraining batch 275 loss: 0.708593\n",
            "\tTraining batch 276 loss: 0.921746\n",
            "\tTraining batch 277 loss: 0.814699\n",
            "\tTraining batch 278 loss: 0.858351\n",
            "\tTraining batch 279 loss: 0.755028\n",
            "\tTraining batch 280 loss: 0.958266\n",
            "\tTraining batch 281 loss: 1.132735\n",
            "\tTraining batch 282 loss: 0.752372\n",
            "\tTraining batch 283 loss: 1.274695\n",
            "\tTraining batch 284 loss: 1.031150\n",
            "\tTraining batch 285 loss: 1.069494\n",
            "\tTraining batch 286 loss: 1.322776\n",
            "\tTraining batch 287 loss: 0.725441\n",
            "\tTraining batch 288 loss: 1.141972\n",
            "\tTraining batch 289 loss: 1.098710\n",
            "\tTraining batch 290 loss: 1.021721\n",
            "\tTraining batch 291 loss: 1.118176\n",
            "\tTraining batch 292 loss: 0.762806\n",
            "\tTraining batch 293 loss: 0.895176\n",
            "\tTraining batch 294 loss: 1.209581\n",
            "\tTraining batch 295 loss: 1.545542\n",
            "\tTraining batch 296 loss: 0.702257\n",
            "\tTraining batch 297 loss: 1.230835\n",
            "\tTraining batch 298 loss: 0.845143\n",
            "\tTraining batch 299 loss: 1.304644\n",
            "\tTraining batch 300 loss: 1.077865\n",
            "\tTraining batch 301 loss: 0.969848\n",
            "\tTraining batch 302 loss: 0.966143\n",
            "\tTraining batch 303 loss: 1.024012\n",
            "\tTraining batch 304 loss: 1.171444\n",
            "\tTraining batch 305 loss: 1.020190\n",
            "\tTraining batch 306 loss: 0.835819\n",
            "\tTraining batch 307 loss: 0.930715\n",
            "\tTraining batch 308 loss: 1.036829\n",
            "\tTraining batch 309 loss: 0.868986\n",
            "\tTraining batch 310 loss: 0.665955\n",
            "\tTraining batch 311 loss: 0.937208\n",
            "\tTraining batch 312 loss: 1.026761\n",
            "\tTraining batch 313 loss: 0.973748\n",
            "\tTraining batch 314 loss: 1.048491\n",
            "\tTraining batch 315 loss: 1.075353\n",
            "\tTraining batch 316 loss: 1.151706\n",
            "\tTraining batch 317 loss: 0.868787\n",
            "\tTraining batch 318 loss: 1.048797\n",
            "\tTraining batch 319 loss: 0.956885\n",
            "\tTraining batch 320 loss: 0.972316\n",
            "\tTraining batch 321 loss: 0.954365\n",
            "\tTraining batch 322 loss: 1.085948\n",
            "\tTraining batch 323 loss: 1.020515\n",
            "\tTraining batch 324 loss: 0.975883\n",
            "\tTraining batch 325 loss: 0.703768\n",
            "\tTraining batch 326 loss: 1.012148\n",
            "\tTraining batch 327 loss: 1.154010\n",
            "\tTraining batch 328 loss: 1.289337\n",
            "\tTraining batch 329 loss: 1.220650\n",
            "\tTraining batch 330 loss: 1.205421\n",
            "\tTraining batch 331 loss: 0.984101\n",
            "\tTraining batch 332 loss: 0.965679\n",
            "\tTraining batch 333 loss: 0.878340\n",
            "\tTraining batch 334 loss: 1.180267\n",
            "\tTraining batch 335 loss: 0.627266\n",
            "\tTraining batch 336 loss: 0.996257\n",
            "\tTraining batch 337 loss: 0.750414\n",
            "\tTraining batch 338 loss: 1.069780\n",
            "\tTraining batch 339 loss: 0.684016\n",
            "\tTraining batch 340 loss: 0.764979\n",
            "\tTraining batch 341 loss: 0.851436\n",
            "\tTraining batch 342 loss: 1.026710\n",
            "\tTraining batch 343 loss: 0.663511\n",
            "\tTraining batch 344 loss: 1.059279\n",
            "\tTraining batch 345 loss: 1.088339\n",
            "\tTraining batch 346 loss: 1.363220\n",
            "\tTraining batch 347 loss: 0.917315\n",
            "\tTraining batch 348 loss: 1.107760\n",
            "\tTraining batch 349 loss: 0.818596\n",
            "\tTraining batch 350 loss: 1.071215\n",
            "\tTraining batch 351 loss: 1.009380\n",
            "\tTraining batch 352 loss: 1.125584\n",
            "\tTraining batch 353 loss: 0.828173\n",
            "\tTraining batch 354 loss: 0.881211\n",
            "\tTraining batch 355 loss: 1.090756\n",
            "\tTraining batch 356 loss: 1.123392\n",
            "\tTraining batch 357 loss: 1.255615\n",
            "\tTraining batch 358 loss: 0.830698\n",
            "\tTraining batch 359 loss: 1.024966\n",
            "\tTraining batch 360 loss: 1.132908\n",
            "\tTraining batch 361 loss: 0.894568\n",
            "\tTraining batch 362 loss: 1.061448\n",
            "\tTraining batch 363 loss: 0.898744\n",
            "\tTraining batch 364 loss: 0.963027\n",
            "\tTraining batch 365 loss: 1.241974\n",
            "\tTraining batch 366 loss: 1.018733\n",
            "\tTraining batch 367 loss: 0.785315\n",
            "\tTraining batch 368 loss: 1.050907\n",
            "\tTraining batch 369 loss: 0.989945\n",
            "\tTraining batch 370 loss: 0.978906\n",
            "\tTraining batch 371 loss: 1.077118\n",
            "\tTraining batch 372 loss: 1.147207\n",
            "\tTraining batch 373 loss: 1.123913\n",
            "\tTraining batch 374 loss: 0.778956\n",
            "\tTraining batch 375 loss: 0.694351\n",
            "\tTraining batch 376 loss: 0.959228\n",
            "\tTraining batch 377 loss: 0.916998\n",
            "\tTraining batch 378 loss: 1.107340\n",
            "\tTraining batch 379 loss: 1.061766\n",
            "\tTraining batch 380 loss: 0.949099\n",
            "\tTraining batch 381 loss: 1.071382\n",
            "\tTraining batch 382 loss: 0.816088\n",
            "\tTraining batch 383 loss: 1.041693\n",
            "\tTraining batch 384 loss: 0.695766\n",
            "\tTraining batch 385 loss: 0.809088\n",
            "\tTraining batch 386 loss: 0.820847\n",
            "\tTraining batch 387 loss: 0.808477\n",
            "\tTraining batch 388 loss: 1.394390\n",
            "\tTraining batch 389 loss: 1.053985\n",
            "\tTraining batch 390 loss: 1.369181\n",
            "\tTraining batch 391 loss: 0.891007\n",
            "\tTraining batch 392 loss: 0.900802\n",
            "\tTraining batch 393 loss: 1.298589\n",
            "\tTraining batch 394 loss: 1.008983\n",
            "\tTraining batch 395 loss: 0.674731\n",
            "\tTraining batch 396 loss: 0.960586\n",
            "\tTraining batch 397 loss: 1.297035\n",
            "\tTraining batch 398 loss: 1.253845\n",
            "\tTraining batch 399 loss: 1.089910\n",
            "\tTraining batch 400 loss: 1.362361\n",
            "\tTraining batch 401 loss: 1.055503\n",
            "\tTraining batch 402 loss: 0.946319\n",
            "\tTraining batch 403 loss: 1.313664\n",
            "\tTraining batch 404 loss: 0.835277\n",
            "\tTraining batch 405 loss: 1.081925\n",
            "\tTraining batch 406 loss: 0.960312\n",
            "\tTraining batch 407 loss: 0.969131\n",
            "\tTraining batch 408 loss: 0.926389\n",
            "\tTraining batch 409 loss: 0.898689\n",
            "\tTraining batch 410 loss: 1.092265\n",
            "\tTraining batch 411 loss: 0.832323\n",
            "\tTraining batch 412 loss: 1.197431\n",
            "\tTraining batch 413 loss: 1.099789\n",
            "\tTraining batch 414 loss: 1.176649\n",
            "\tTraining batch 415 loss: 0.920932\n",
            "\tTraining batch 416 loss: 1.070398\n",
            "\tTraining batch 417 loss: 1.232892\n",
            "\tTraining batch 418 loss: 1.330122\n",
            "\tTraining batch 419 loss: 1.182550\n",
            "\tTraining batch 420 loss: 0.952679\n",
            "\tTraining batch 421 loss: 1.126604\n",
            "\tTraining batch 422 loss: 0.825195\n",
            "\tTraining batch 423 loss: 0.273793\n",
            "Training set: Average loss: 1.012031\n",
            "Validation set: Average loss: 0.864213, Accuracy: 3661/5276 (69%)\n",
            "\n",
            "Epoch 6\n",
            "\tTraining batch 1 loss: 0.890516\n",
            "\tTraining batch 2 loss: 1.242774\n",
            "\tTraining batch 3 loss: 1.050781\n",
            "\tTraining batch 4 loss: 1.064231\n",
            "\tTraining batch 5 loss: 1.025742\n",
            "\tTraining batch 6 loss: 1.287725\n",
            "\tTraining batch 7 loss: 1.081908\n",
            "\tTraining batch 8 loss: 1.451770\n",
            "\tTraining batch 9 loss: 1.218265\n",
            "\tTraining batch 10 loss: 1.238799\n",
            "\tTraining batch 11 loss: 0.823869\n",
            "\tTraining batch 12 loss: 1.405247\n",
            "\tTraining batch 13 loss: 1.241666\n",
            "\tTraining batch 14 loss: 1.061673\n",
            "\tTraining batch 15 loss: 1.353513\n",
            "\tTraining batch 16 loss: 0.959232\n",
            "\tTraining batch 17 loss: 0.798409\n",
            "\tTraining batch 18 loss: 1.158654\n",
            "\tTraining batch 19 loss: 0.913119\n",
            "\tTraining batch 20 loss: 1.132909\n",
            "\tTraining batch 21 loss: 1.134757\n",
            "\tTraining batch 22 loss: 0.783137\n",
            "\tTraining batch 23 loss: 0.794997\n",
            "\tTraining batch 24 loss: 0.822109\n",
            "\tTraining batch 25 loss: 1.070531\n",
            "\tTraining batch 26 loss: 1.056255\n",
            "\tTraining batch 27 loss: 0.875802\n",
            "\tTraining batch 28 loss: 1.111683\n",
            "\tTraining batch 29 loss: 0.955889\n",
            "\tTraining batch 30 loss: 0.983492\n",
            "\tTraining batch 31 loss: 1.149162\n",
            "\tTraining batch 32 loss: 0.949724\n",
            "\tTraining batch 33 loss: 1.099164\n",
            "\tTraining batch 34 loss: 0.918196\n",
            "\tTraining batch 35 loss: 0.802847\n",
            "\tTraining batch 36 loss: 1.215769\n",
            "\tTraining batch 37 loss: 1.215677\n",
            "\tTraining batch 38 loss: 0.858701\n",
            "\tTraining batch 39 loss: 1.153136\n",
            "\tTraining batch 40 loss: 0.856697\n",
            "\tTraining batch 41 loss: 0.718212\n",
            "\tTraining batch 42 loss: 1.232049\n",
            "\tTraining batch 43 loss: 0.788072\n",
            "\tTraining batch 44 loss: 1.035293\n",
            "\tTraining batch 45 loss: 0.876252\n",
            "\tTraining batch 46 loss: 0.968711\n",
            "\tTraining batch 47 loss: 1.178455\n",
            "\tTraining batch 48 loss: 0.917062\n",
            "\tTraining batch 49 loss: 1.115467\n",
            "\tTraining batch 50 loss: 0.712754\n",
            "\tTraining batch 51 loss: 0.881825\n",
            "\tTraining batch 52 loss: 1.117606\n",
            "\tTraining batch 53 loss: 0.621811\n",
            "\tTraining batch 54 loss: 1.077620\n",
            "\tTraining batch 55 loss: 0.779380\n",
            "\tTraining batch 56 loss: 0.850375\n",
            "\tTraining batch 57 loss: 1.194524\n",
            "\tTraining batch 58 loss: 0.728692\n",
            "\tTraining batch 59 loss: 0.984495\n",
            "\tTraining batch 60 loss: 1.176160\n",
            "\tTraining batch 61 loss: 0.946651\n",
            "\tTraining batch 62 loss: 0.925148\n",
            "\tTraining batch 63 loss: 1.141134\n",
            "\tTraining batch 64 loss: 0.957799\n",
            "\tTraining batch 65 loss: 1.065971\n",
            "\tTraining batch 66 loss: 0.976119\n",
            "\tTraining batch 67 loss: 0.520709\n",
            "\tTraining batch 68 loss: 0.852420\n",
            "\tTraining batch 69 loss: 0.917699\n",
            "\tTraining batch 70 loss: 0.787800\n",
            "\tTraining batch 71 loss: 0.896634\n",
            "\tTraining batch 72 loss: 0.868774\n",
            "\tTraining batch 73 loss: 0.967342\n",
            "\tTraining batch 74 loss: 1.126139\n",
            "\tTraining batch 75 loss: 1.018043\n",
            "\tTraining batch 76 loss: 0.908115\n",
            "\tTraining batch 77 loss: 1.160540\n",
            "\tTraining batch 78 loss: 1.305459\n",
            "\tTraining batch 79 loss: 0.986877\n",
            "\tTraining batch 80 loss: 0.701777\n",
            "\tTraining batch 81 loss: 1.084389\n",
            "\tTraining batch 82 loss: 0.857876\n",
            "\tTraining batch 83 loss: 1.187503\n",
            "\tTraining batch 84 loss: 1.054062\n",
            "\tTraining batch 85 loss: 0.886858\n",
            "\tTraining batch 86 loss: 0.756355\n",
            "\tTraining batch 87 loss: 0.956417\n",
            "\tTraining batch 88 loss: 1.282313\n",
            "\tTraining batch 89 loss: 1.067571\n",
            "\tTraining batch 90 loss: 1.009532\n",
            "\tTraining batch 91 loss: 0.818024\n",
            "\tTraining batch 92 loss: 0.804641\n",
            "\tTraining batch 93 loss: 1.154459\n",
            "\tTraining batch 94 loss: 0.860405\n",
            "\tTraining batch 95 loss: 0.792410\n",
            "\tTraining batch 96 loss: 0.861570\n",
            "\tTraining batch 97 loss: 1.115416\n",
            "\tTraining batch 98 loss: 0.950415\n",
            "\tTraining batch 99 loss: 1.078936\n",
            "\tTraining batch 100 loss: 0.960514\n",
            "\tTraining batch 101 loss: 0.958425\n",
            "\tTraining batch 102 loss: 0.853308\n",
            "\tTraining batch 103 loss: 1.075540\n",
            "\tTraining batch 104 loss: 1.102609\n",
            "\tTraining batch 105 loss: 1.053714\n",
            "\tTraining batch 106 loss: 0.904225\n",
            "\tTraining batch 107 loss: 1.326629\n",
            "\tTraining batch 108 loss: 1.227810\n",
            "\tTraining batch 109 loss: 0.845286\n",
            "\tTraining batch 110 loss: 1.722042\n",
            "\tTraining batch 111 loss: 1.007389\n",
            "\tTraining batch 112 loss: 0.746959\n",
            "\tTraining batch 113 loss: 1.185243\n",
            "\tTraining batch 114 loss: 1.080023\n",
            "\tTraining batch 115 loss: 1.038561\n",
            "\tTraining batch 116 loss: 1.002615\n",
            "\tTraining batch 117 loss: 1.262181\n",
            "\tTraining batch 118 loss: 0.847376\n",
            "\tTraining batch 119 loss: 1.083522\n",
            "\tTraining batch 120 loss: 1.163236\n",
            "\tTraining batch 121 loss: 0.884638\n",
            "\tTraining batch 122 loss: 1.075040\n",
            "\tTraining batch 123 loss: 0.807271\n",
            "\tTraining batch 124 loss: 0.947963\n",
            "\tTraining batch 125 loss: 0.952528\n",
            "\tTraining batch 126 loss: 0.797878\n",
            "\tTraining batch 127 loss: 1.152276\n",
            "\tTraining batch 128 loss: 1.199875\n",
            "\tTraining batch 129 loss: 0.906049\n",
            "\tTraining batch 130 loss: 1.252480\n",
            "\tTraining batch 131 loss: 1.063705\n",
            "\tTraining batch 132 loss: 0.823904\n",
            "\tTraining batch 133 loss: 0.877547\n",
            "\tTraining batch 134 loss: 1.152274\n",
            "\tTraining batch 135 loss: 1.088404\n",
            "\tTraining batch 136 loss: 1.068772\n",
            "\tTraining batch 137 loss: 1.122419\n",
            "\tTraining batch 138 loss: 0.672268\n",
            "\tTraining batch 139 loss: 0.925602\n",
            "\tTraining batch 140 loss: 1.123648\n",
            "\tTraining batch 141 loss: 0.975816\n",
            "\tTraining batch 142 loss: 0.931442\n",
            "\tTraining batch 143 loss: 1.157547\n",
            "\tTraining batch 144 loss: 1.122176\n",
            "\tTraining batch 145 loss: 1.005962\n",
            "\tTraining batch 146 loss: 1.194546\n",
            "\tTraining batch 147 loss: 1.353841\n",
            "\tTraining batch 148 loss: 1.005430\n",
            "\tTraining batch 149 loss: 0.955852\n",
            "\tTraining batch 150 loss: 0.795680\n",
            "\tTraining batch 151 loss: 0.863499\n",
            "\tTraining batch 152 loss: 1.104852\n",
            "\tTraining batch 153 loss: 1.044647\n",
            "\tTraining batch 154 loss: 1.258034\n",
            "\tTraining batch 155 loss: 0.762329\n",
            "\tTraining batch 156 loss: 0.909633\n",
            "\tTraining batch 157 loss: 0.681853\n",
            "\tTraining batch 158 loss: 1.023404\n",
            "\tTraining batch 159 loss: 0.806813\n",
            "\tTraining batch 160 loss: 0.871679\n",
            "\tTraining batch 161 loss: 0.875650\n",
            "\tTraining batch 162 loss: 1.024738\n",
            "\tTraining batch 163 loss: 1.003179\n",
            "\tTraining batch 164 loss: 1.086726\n",
            "\tTraining batch 165 loss: 0.923394\n",
            "\tTraining batch 166 loss: 0.988580\n",
            "\tTraining batch 167 loss: 0.886262\n",
            "\tTraining batch 168 loss: 1.043022\n",
            "\tTraining batch 169 loss: 1.276840\n",
            "\tTraining batch 170 loss: 0.968307\n",
            "\tTraining batch 171 loss: 0.897944\n",
            "\tTraining batch 172 loss: 0.975817\n",
            "\tTraining batch 173 loss: 1.099624\n",
            "\tTraining batch 174 loss: 0.937920\n",
            "\tTraining batch 175 loss: 1.164050\n",
            "\tTraining batch 176 loss: 1.043860\n",
            "\tTraining batch 177 loss: 1.039967\n",
            "\tTraining batch 178 loss: 0.866932\n",
            "\tTraining batch 179 loss: 0.993126\n",
            "\tTraining batch 180 loss: 1.323807\n",
            "\tTraining batch 181 loss: 0.988540\n",
            "\tTraining batch 182 loss: 1.047278\n",
            "\tTraining batch 183 loss: 1.199884\n",
            "\tTraining batch 184 loss: 1.223592\n",
            "\tTraining batch 185 loss: 0.695916\n",
            "\tTraining batch 186 loss: 0.913351\n",
            "\tTraining batch 187 loss: 0.926467\n",
            "\tTraining batch 188 loss: 0.664184\n",
            "\tTraining batch 189 loss: 0.835996\n",
            "\tTraining batch 190 loss: 1.047062\n",
            "\tTraining batch 191 loss: 1.102874\n",
            "\tTraining batch 192 loss: 1.022380\n",
            "\tTraining batch 193 loss: 0.929979\n",
            "\tTraining batch 194 loss: 0.551481\n",
            "\tTraining batch 195 loss: 1.042987\n",
            "\tTraining batch 196 loss: 1.248528\n",
            "\tTraining batch 197 loss: 1.146912\n",
            "\tTraining batch 198 loss: 0.929059\n",
            "\tTraining batch 199 loss: 0.926995\n",
            "\tTraining batch 200 loss: 0.847403\n",
            "\tTraining batch 201 loss: 1.623421\n",
            "\tTraining batch 202 loss: 1.318895\n",
            "\tTraining batch 203 loss: 1.578709\n",
            "\tTraining batch 204 loss: 0.931668\n",
            "\tTraining batch 205 loss: 1.045572\n",
            "\tTraining batch 206 loss: 0.946374\n",
            "\tTraining batch 207 loss: 1.157514\n",
            "\tTraining batch 208 loss: 1.168263\n",
            "\tTraining batch 209 loss: 0.720837\n",
            "\tTraining batch 210 loss: 1.022288\n",
            "\tTraining batch 211 loss: 1.063685\n",
            "\tTraining batch 212 loss: 0.920206\n",
            "\tTraining batch 213 loss: 1.072684\n",
            "\tTraining batch 214 loss: 0.996235\n",
            "\tTraining batch 215 loss: 1.235703\n",
            "\tTraining batch 216 loss: 0.799171\n",
            "\tTraining batch 217 loss: 0.754405\n",
            "\tTraining batch 218 loss: 0.939640\n",
            "\tTraining batch 219 loss: 0.936730\n",
            "\tTraining batch 220 loss: 0.740389\n",
            "\tTraining batch 221 loss: 0.951925\n",
            "\tTraining batch 222 loss: 0.764520\n",
            "\tTraining batch 223 loss: 1.104786\n",
            "\tTraining batch 224 loss: 0.725770\n",
            "\tTraining batch 225 loss: 1.010114\n",
            "\tTraining batch 226 loss: 0.992552\n",
            "\tTraining batch 227 loss: 1.228274\n",
            "\tTraining batch 228 loss: 0.565533\n",
            "\tTraining batch 229 loss: 0.854366\n",
            "\tTraining batch 230 loss: 0.483768\n",
            "\tTraining batch 231 loss: 1.101428\n",
            "\tTraining batch 232 loss: 1.254254\n",
            "\tTraining batch 233 loss: 1.039324\n",
            "\tTraining batch 234 loss: 0.650786\n",
            "\tTraining batch 235 loss: 1.244262\n",
            "\tTraining batch 236 loss: 0.839697\n",
            "\tTraining batch 237 loss: 0.747026\n",
            "\tTraining batch 238 loss: 0.994041\n",
            "\tTraining batch 239 loss: 1.128444\n",
            "\tTraining batch 240 loss: 0.810979\n",
            "\tTraining batch 241 loss: 0.923480\n",
            "\tTraining batch 242 loss: 0.891008\n",
            "\tTraining batch 243 loss: 1.019577\n",
            "\tTraining batch 244 loss: 0.789289\n",
            "\tTraining batch 245 loss: 0.971542\n",
            "\tTraining batch 246 loss: 0.930262\n",
            "\tTraining batch 247 loss: 1.204435\n",
            "\tTraining batch 248 loss: 0.965457\n",
            "\tTraining batch 249 loss: 0.980852\n",
            "\tTraining batch 250 loss: 0.832065\n",
            "\tTraining batch 251 loss: 0.636311\n",
            "\tTraining batch 252 loss: 0.971309\n",
            "\tTraining batch 253 loss: 0.740275\n",
            "\tTraining batch 254 loss: 1.186268\n",
            "\tTraining batch 255 loss: 0.901914\n",
            "\tTraining batch 256 loss: 1.116480\n",
            "\tTraining batch 257 loss: 0.679770\n",
            "\tTraining batch 258 loss: 1.082549\n",
            "\tTraining batch 259 loss: 0.858911\n",
            "\tTraining batch 260 loss: 1.059607\n",
            "\tTraining batch 261 loss: 0.968491\n",
            "\tTraining batch 262 loss: 0.928737\n",
            "\tTraining batch 263 loss: 0.787119\n",
            "\tTraining batch 264 loss: 1.165903\n",
            "\tTraining batch 265 loss: 1.153951\n",
            "\tTraining batch 266 loss: 0.754112\n",
            "\tTraining batch 267 loss: 0.834770\n",
            "\tTraining batch 268 loss: 0.971134\n",
            "\tTraining batch 269 loss: 1.045015\n",
            "\tTraining batch 270 loss: 0.594724\n",
            "\tTraining batch 271 loss: 0.963690\n",
            "\tTraining batch 272 loss: 1.218694\n",
            "\tTraining batch 273 loss: 0.888956\n",
            "\tTraining batch 274 loss: 1.277804\n",
            "\tTraining batch 275 loss: 0.839275\n",
            "\tTraining batch 276 loss: 0.992677\n",
            "\tTraining batch 277 loss: 0.826257\n",
            "\tTraining batch 278 loss: 0.869556\n",
            "\tTraining batch 279 loss: 0.802143\n",
            "\tTraining batch 280 loss: 0.981680\n",
            "\tTraining batch 281 loss: 1.077379\n",
            "\tTraining batch 282 loss: 0.664636\n",
            "\tTraining batch 283 loss: 1.059255\n",
            "\tTraining batch 284 loss: 0.852960\n",
            "\tTraining batch 285 loss: 0.926052\n",
            "\tTraining batch 286 loss: 1.254269\n",
            "\tTraining batch 287 loss: 0.799795\n",
            "\tTraining batch 288 loss: 0.995015\n",
            "\tTraining batch 289 loss: 0.941763\n",
            "\tTraining batch 290 loss: 1.009637\n",
            "\tTraining batch 291 loss: 1.097168\n",
            "\tTraining batch 292 loss: 0.937924\n",
            "\tTraining batch 293 loss: 0.899607\n",
            "\tTraining batch 294 loss: 0.956113\n",
            "\tTraining batch 295 loss: 1.224743\n",
            "\tTraining batch 296 loss: 0.765992\n",
            "\tTraining batch 297 loss: 1.419723\n",
            "\tTraining batch 298 loss: 0.786553\n",
            "\tTraining batch 299 loss: 1.323116\n",
            "\tTraining batch 300 loss: 0.901936\n",
            "\tTraining batch 301 loss: 0.965344\n",
            "\tTraining batch 302 loss: 0.927572\n",
            "\tTraining batch 303 loss: 1.060576\n",
            "\tTraining batch 304 loss: 1.284934\n",
            "\tTraining batch 305 loss: 0.877092\n",
            "\tTraining batch 306 loss: 0.784479\n",
            "\tTraining batch 307 loss: 0.806049\n",
            "\tTraining batch 308 loss: 0.990028\n",
            "\tTraining batch 309 loss: 0.890882\n",
            "\tTraining batch 310 loss: 0.683955\n",
            "\tTraining batch 311 loss: 0.855406\n",
            "\tTraining batch 312 loss: 0.865525\n",
            "\tTraining batch 313 loss: 0.990426\n",
            "\tTraining batch 314 loss: 0.880051\n",
            "\tTraining batch 315 loss: 1.121227\n",
            "\tTraining batch 316 loss: 1.337853\n",
            "\tTraining batch 317 loss: 0.934349\n",
            "\tTraining batch 318 loss: 0.789991\n",
            "\tTraining batch 319 loss: 0.964269\n",
            "\tTraining batch 320 loss: 1.073088\n",
            "\tTraining batch 321 loss: 1.033126\n",
            "\tTraining batch 322 loss: 1.004611\n",
            "\tTraining batch 323 loss: 1.007271\n",
            "\tTraining batch 324 loss: 0.880591\n",
            "\tTraining batch 325 loss: 0.898564\n",
            "\tTraining batch 326 loss: 1.092824\n",
            "\tTraining batch 327 loss: 1.117444\n",
            "\tTraining batch 328 loss: 1.314045\n",
            "\tTraining batch 329 loss: 1.143081\n",
            "\tTraining batch 330 loss: 1.256832\n",
            "\tTraining batch 331 loss: 0.873266\n",
            "\tTraining batch 332 loss: 0.919681\n",
            "\tTraining batch 333 loss: 0.913612\n",
            "\tTraining batch 334 loss: 1.229552\n",
            "\tTraining batch 335 loss: 0.699259\n",
            "\tTraining batch 336 loss: 1.013191\n",
            "\tTraining batch 337 loss: 0.736062\n",
            "\tTraining batch 338 loss: 0.988810\n",
            "\tTraining batch 339 loss: 0.744074\n",
            "\tTraining batch 340 loss: 1.100922\n",
            "\tTraining batch 341 loss: 0.883019\n",
            "\tTraining batch 342 loss: 1.205790\n",
            "\tTraining batch 343 loss: 0.572643\n",
            "\tTraining batch 344 loss: 0.899266\n",
            "\tTraining batch 345 loss: 1.067091\n",
            "\tTraining batch 346 loss: 1.338808\n",
            "\tTraining batch 347 loss: 0.940698\n",
            "\tTraining batch 348 loss: 1.168759\n",
            "\tTraining batch 349 loss: 0.879773\n",
            "\tTraining batch 350 loss: 1.093004\n",
            "\tTraining batch 351 loss: 0.943832\n",
            "\tTraining batch 352 loss: 1.051732\n",
            "\tTraining batch 353 loss: 0.815110\n",
            "\tTraining batch 354 loss: 0.755562\n",
            "\tTraining batch 355 loss: 1.017056\n",
            "\tTraining batch 356 loss: 1.244246\n",
            "\tTraining batch 357 loss: 1.163807\n",
            "\tTraining batch 358 loss: 0.792196\n",
            "\tTraining batch 359 loss: 1.030355\n",
            "\tTraining batch 360 loss: 0.808372\n",
            "\tTraining batch 361 loss: 1.020796\n",
            "\tTraining batch 362 loss: 0.995670\n",
            "\tTraining batch 363 loss: 0.887067\n",
            "\tTraining batch 364 loss: 0.683792\n",
            "\tTraining batch 365 loss: 1.283352\n",
            "\tTraining batch 366 loss: 1.138266\n",
            "\tTraining batch 367 loss: 0.697840\n",
            "\tTraining batch 368 loss: 0.833400\n",
            "\tTraining batch 369 loss: 0.988007\n",
            "\tTraining batch 370 loss: 0.899664\n",
            "\tTraining batch 371 loss: 1.019369\n",
            "\tTraining batch 372 loss: 1.039143\n",
            "\tTraining batch 373 loss: 1.106203\n",
            "\tTraining batch 374 loss: 0.714587\n",
            "\tTraining batch 375 loss: 0.659636\n",
            "\tTraining batch 376 loss: 0.964652\n",
            "\tTraining batch 377 loss: 0.967675\n",
            "\tTraining batch 378 loss: 1.369834\n",
            "\tTraining batch 379 loss: 1.046841\n",
            "\tTraining batch 380 loss: 1.035199\n",
            "\tTraining batch 381 loss: 1.042715\n",
            "\tTraining batch 382 loss: 0.893272\n",
            "\tTraining batch 383 loss: 0.982870\n",
            "\tTraining batch 384 loss: 0.953519\n",
            "\tTraining batch 385 loss: 0.920050\n",
            "\tTraining batch 386 loss: 0.841900\n",
            "\tTraining batch 387 loss: 0.842772\n",
            "\tTraining batch 388 loss: 1.157957\n",
            "\tTraining batch 389 loss: 1.063879\n",
            "\tTraining batch 390 loss: 1.130493\n",
            "\tTraining batch 391 loss: 0.882767\n",
            "\tTraining batch 392 loss: 0.726139\n",
            "\tTraining batch 393 loss: 1.160956\n",
            "\tTraining batch 394 loss: 0.812988\n",
            "\tTraining batch 395 loss: 0.745442\n",
            "\tTraining batch 396 loss: 0.783763\n",
            "\tTraining batch 397 loss: 1.165460\n",
            "\tTraining batch 398 loss: 1.112139\n",
            "\tTraining batch 399 loss: 0.996503\n",
            "\tTraining batch 400 loss: 1.276965\n",
            "\tTraining batch 401 loss: 1.001602\n",
            "\tTraining batch 402 loss: 0.894212\n",
            "\tTraining batch 403 loss: 1.280914\n",
            "\tTraining batch 404 loss: 0.928776\n",
            "\tTraining batch 405 loss: 0.979025\n",
            "\tTraining batch 406 loss: 1.008586\n",
            "\tTraining batch 407 loss: 0.910554\n",
            "\tTraining batch 408 loss: 1.122787\n",
            "\tTraining batch 409 loss: 1.205300\n",
            "\tTraining batch 410 loss: 1.036298\n",
            "\tTraining batch 411 loss: 0.754441\n",
            "\tTraining batch 412 loss: 1.209830\n",
            "\tTraining batch 413 loss: 1.140356\n",
            "\tTraining batch 414 loss: 1.122676\n",
            "\tTraining batch 415 loss: 1.038996\n",
            "\tTraining batch 416 loss: 1.071068\n",
            "\tTraining batch 417 loss: 1.135753\n",
            "\tTraining batch 418 loss: 1.055780\n",
            "\tTraining batch 419 loss: 1.213348\n",
            "\tTraining batch 420 loss: 1.017551\n",
            "\tTraining batch 421 loss: 1.144775\n",
            "\tTraining batch 422 loss: 0.858046\n",
            "\tTraining batch 423 loss: 0.205936\n",
            "Training set: Average loss: 0.989400\n",
            "Validation set: Average loss: 0.873009, Accuracy: 3671/5276 (70%)\n",
            "\n",
            "Epoch 7\n",
            "\tTraining batch 1 loss: 1.111924\n",
            "\tTraining batch 2 loss: 1.320336\n",
            "\tTraining batch 3 loss: 1.032814\n",
            "\tTraining batch 4 loss: 1.021796\n",
            "\tTraining batch 5 loss: 0.884953\n",
            "\tTraining batch 6 loss: 1.034491\n",
            "\tTraining batch 7 loss: 1.168611\n",
            "\tTraining batch 8 loss: 1.448211\n",
            "\tTraining batch 9 loss: 1.191791\n",
            "\tTraining batch 10 loss: 1.009695\n",
            "\tTraining batch 11 loss: 0.825588\n",
            "\tTraining batch 12 loss: 1.353601\n",
            "\tTraining batch 13 loss: 1.271097\n",
            "\tTraining batch 14 loss: 0.990506\n",
            "\tTraining batch 15 loss: 1.438282\n",
            "\tTraining batch 16 loss: 0.964458\n",
            "\tTraining batch 17 loss: 0.746313\n",
            "\tTraining batch 18 loss: 1.127982\n",
            "\tTraining batch 19 loss: 1.025497\n",
            "\tTraining batch 20 loss: 1.203302\n",
            "\tTraining batch 21 loss: 1.270813\n",
            "\tTraining batch 22 loss: 0.842449\n",
            "\tTraining batch 23 loss: 0.724480\n",
            "\tTraining batch 24 loss: 0.926103\n",
            "\tTraining batch 25 loss: 1.166363\n",
            "\tTraining batch 26 loss: 1.001547\n",
            "\tTraining batch 27 loss: 0.840342\n",
            "\tTraining batch 28 loss: 1.085264\n",
            "\tTraining batch 29 loss: 1.045613\n",
            "\tTraining batch 30 loss: 1.178317\n",
            "\tTraining batch 31 loss: 0.922360\n",
            "\tTraining batch 32 loss: 1.061354\n",
            "\tTraining batch 33 loss: 1.191743\n",
            "\tTraining batch 34 loss: 0.691791\n",
            "\tTraining batch 35 loss: 0.854097\n",
            "\tTraining batch 36 loss: 1.273264\n",
            "\tTraining batch 37 loss: 1.173915\n",
            "\tTraining batch 38 loss: 0.749421\n",
            "\tTraining batch 39 loss: 1.228915\n",
            "\tTraining batch 40 loss: 0.935832\n",
            "\tTraining batch 41 loss: 1.037626\n",
            "\tTraining batch 42 loss: 1.171020\n",
            "\tTraining batch 43 loss: 0.816178\n",
            "\tTraining batch 44 loss: 1.101482\n",
            "\tTraining batch 45 loss: 0.833401\n",
            "\tTraining batch 46 loss: 0.886194\n",
            "\tTraining batch 47 loss: 1.264106\n",
            "\tTraining batch 48 loss: 0.898876\n",
            "\tTraining batch 49 loss: 1.065374\n",
            "\tTraining batch 50 loss: 0.841583\n",
            "\tTraining batch 51 loss: 0.910065\n",
            "\tTraining batch 52 loss: 0.941626\n",
            "\tTraining batch 53 loss: 0.599113\n",
            "\tTraining batch 54 loss: 0.841771\n",
            "\tTraining batch 55 loss: 0.928016\n",
            "\tTraining batch 56 loss: 0.841033\n",
            "\tTraining batch 57 loss: 1.210106\n",
            "\tTraining batch 58 loss: 0.802542\n",
            "\tTraining batch 59 loss: 1.034077\n",
            "\tTraining batch 60 loss: 1.143042\n",
            "\tTraining batch 61 loss: 0.928174\n",
            "\tTraining batch 62 loss: 0.956978\n",
            "\tTraining batch 63 loss: 1.116404\n",
            "\tTraining batch 64 loss: 0.898180\n",
            "\tTraining batch 65 loss: 1.425246\n",
            "\tTraining batch 66 loss: 0.958360\n",
            "\tTraining batch 67 loss: 0.657063\n",
            "\tTraining batch 68 loss: 1.006646\n",
            "\tTraining batch 69 loss: 1.212308\n",
            "\tTraining batch 70 loss: 1.218879\n",
            "\tTraining batch 71 loss: 1.170096\n",
            "\tTraining batch 72 loss: 1.120283\n",
            "\tTraining batch 73 loss: 1.167429\n",
            "\tTraining batch 74 loss: 1.326931\n",
            "\tTraining batch 75 loss: 1.173871\n",
            "\tTraining batch 76 loss: 1.249303\n",
            "\tTraining batch 77 loss: 1.407016\n",
            "\tTraining batch 78 loss: 1.484997\n",
            "\tTraining batch 79 loss: 1.309612\n",
            "\tTraining batch 80 loss: 1.078860\n",
            "\tTraining batch 81 loss: 1.243293\n",
            "\tTraining batch 82 loss: 1.201387\n",
            "\tTraining batch 83 loss: 1.112654\n",
            "\tTraining batch 84 loss: 1.214306\n",
            "\tTraining batch 85 loss: 1.018322\n",
            "\tTraining batch 86 loss: 1.097502\n",
            "\tTraining batch 87 loss: 1.234340\n",
            "\tTraining batch 88 loss: 1.468159\n",
            "\tTraining batch 89 loss: 1.387583\n",
            "\tTraining batch 90 loss: 1.329445\n",
            "\tTraining batch 91 loss: 1.157985\n",
            "\tTraining batch 92 loss: 0.868232\n",
            "\tTraining batch 93 loss: 1.382115\n",
            "\tTraining batch 94 loss: 1.050252\n",
            "\tTraining batch 95 loss: 1.144338\n",
            "\tTraining batch 96 loss: 1.223045\n",
            "\tTraining batch 97 loss: 1.503347\n",
            "\tTraining batch 98 loss: 1.271109\n",
            "\tTraining batch 99 loss: 1.200191\n",
            "\tTraining batch 100 loss: 1.139756\n",
            "\tTraining batch 101 loss: 1.227604\n",
            "\tTraining batch 102 loss: 0.937789\n",
            "\tTraining batch 103 loss: 1.264645\n",
            "\tTraining batch 104 loss: 1.192911\n",
            "\tTraining batch 105 loss: 1.153224\n",
            "\tTraining batch 106 loss: 1.194274\n",
            "\tTraining batch 107 loss: 1.150923\n",
            "\tTraining batch 108 loss: 1.229996\n",
            "\tTraining batch 109 loss: 0.905993\n",
            "\tTraining batch 110 loss: 1.293207\n",
            "\tTraining batch 111 loss: 0.993459\n",
            "\tTraining batch 112 loss: 0.949495\n",
            "\tTraining batch 113 loss: 1.087237\n",
            "\tTraining batch 114 loss: 1.068553\n",
            "\tTraining batch 115 loss: 1.364463\n",
            "\tTraining batch 116 loss: 0.988393\n",
            "\tTraining batch 117 loss: 1.255833\n",
            "\tTraining batch 118 loss: 1.025148\n",
            "\tTraining batch 119 loss: 1.334244\n",
            "\tTraining batch 120 loss: 1.223190\n",
            "\tTraining batch 121 loss: 0.882185\n",
            "\tTraining batch 122 loss: 1.116872\n",
            "\tTraining batch 123 loss: 0.884647\n",
            "\tTraining batch 124 loss: 0.915241\n",
            "\tTraining batch 125 loss: 1.054525\n",
            "\tTraining batch 126 loss: 1.104195\n",
            "\tTraining batch 127 loss: 1.124756\n",
            "\tTraining batch 128 loss: 1.104234\n",
            "\tTraining batch 129 loss: 0.988506\n",
            "\tTraining batch 130 loss: 1.111853\n",
            "\tTraining batch 131 loss: 0.943115\n",
            "\tTraining batch 132 loss: 0.869914\n",
            "\tTraining batch 133 loss: 0.873204\n",
            "\tTraining batch 134 loss: 1.246530\n",
            "\tTraining batch 135 loss: 1.263905\n",
            "\tTraining batch 136 loss: 0.988843\n",
            "\tTraining batch 137 loss: 1.345656\n",
            "\tTraining batch 138 loss: 0.919873\n",
            "\tTraining batch 139 loss: 1.080799\n",
            "\tTraining batch 140 loss: 1.081092\n",
            "\tTraining batch 141 loss: 0.963451\n",
            "\tTraining batch 142 loss: 1.087700\n",
            "\tTraining batch 143 loss: 1.153127\n",
            "\tTraining batch 144 loss: 1.173742\n",
            "\tTraining batch 145 loss: 1.109873\n",
            "\tTraining batch 146 loss: 1.255222\n",
            "\tTraining batch 147 loss: 1.392233\n",
            "\tTraining batch 148 loss: 0.890939\n",
            "\tTraining batch 149 loss: 0.908678\n",
            "\tTraining batch 150 loss: 0.933974\n",
            "\tTraining batch 151 loss: 0.898096\n",
            "\tTraining batch 152 loss: 0.937281\n",
            "\tTraining batch 153 loss: 1.174891\n",
            "\tTraining batch 154 loss: 0.992461\n",
            "\tTraining batch 155 loss: 0.826302\n",
            "\tTraining batch 156 loss: 0.890951\n",
            "\tTraining batch 157 loss: 0.656056\n",
            "\tTraining batch 158 loss: 1.045420\n",
            "\tTraining batch 159 loss: 0.799053\n",
            "\tTraining batch 160 loss: 0.908105\n",
            "\tTraining batch 161 loss: 0.881385\n",
            "\tTraining batch 162 loss: 1.030214\n",
            "\tTraining batch 163 loss: 1.046036\n",
            "\tTraining batch 164 loss: 1.353436\n",
            "\tTraining batch 165 loss: 0.964905\n",
            "\tTraining batch 166 loss: 1.153360\n",
            "\tTraining batch 167 loss: 1.096416\n",
            "\tTraining batch 168 loss: 1.108234\n",
            "\tTraining batch 169 loss: 1.215347\n",
            "\tTraining batch 170 loss: 0.817636\n",
            "\tTraining batch 171 loss: 1.093462\n",
            "\tTraining batch 172 loss: 1.091147\n",
            "\tTraining batch 173 loss: 1.078339\n",
            "\tTraining batch 174 loss: 1.080490\n",
            "\tTraining batch 175 loss: 1.261656\n",
            "\tTraining batch 176 loss: 1.025040\n",
            "\tTraining batch 177 loss: 1.194256\n",
            "\tTraining batch 178 loss: 0.829322\n",
            "\tTraining batch 179 loss: 0.965126\n",
            "\tTraining batch 180 loss: 1.385582\n",
            "\tTraining batch 181 loss: 0.878154\n",
            "\tTraining batch 182 loss: 1.166543\n",
            "\tTraining batch 183 loss: 1.066537\n",
            "\tTraining batch 184 loss: 1.414894\n",
            "\tTraining batch 185 loss: 0.692902\n",
            "\tTraining batch 186 loss: 1.080994\n",
            "\tTraining batch 187 loss: 1.054742\n",
            "\tTraining batch 188 loss: 0.810152\n",
            "\tTraining batch 189 loss: 1.030889\n",
            "\tTraining batch 190 loss: 1.027538\n",
            "\tTraining batch 191 loss: 1.123789\n",
            "\tTraining batch 192 loss: 0.943590\n",
            "\tTraining batch 193 loss: 0.968219\n",
            "\tTraining batch 194 loss: 0.577752\n",
            "\tTraining batch 195 loss: 1.092878\n",
            "\tTraining batch 196 loss: 1.290583\n",
            "\tTraining batch 197 loss: 0.993367\n",
            "\tTraining batch 198 loss: 0.941443\n",
            "\tTraining batch 199 loss: 1.082751\n",
            "\tTraining batch 200 loss: 0.771556\n",
            "\tTraining batch 201 loss: 1.369243\n",
            "\tTraining batch 202 loss: 1.348226\n",
            "\tTraining batch 203 loss: 1.520146\n",
            "\tTraining batch 204 loss: 1.028087\n",
            "\tTraining batch 205 loss: 1.430336\n",
            "\tTraining batch 206 loss: 1.090432\n",
            "\tTraining batch 207 loss: 1.027040\n",
            "\tTraining batch 208 loss: 1.107670\n",
            "\tTraining batch 209 loss: 1.016079\n",
            "\tTraining batch 210 loss: 1.010085\n",
            "\tTraining batch 211 loss: 1.103539\n",
            "\tTraining batch 212 loss: 1.081680\n",
            "\tTraining batch 213 loss: 0.956228\n",
            "\tTraining batch 214 loss: 0.906832\n",
            "\tTraining batch 215 loss: 1.300688\n",
            "\tTraining batch 216 loss: 0.738150\n",
            "\tTraining batch 217 loss: 0.923347\n",
            "\tTraining batch 218 loss: 0.985595\n",
            "\tTraining batch 219 loss: 1.048463\n",
            "\tTraining batch 220 loss: 0.801994\n",
            "\tTraining batch 221 loss: 0.909046\n",
            "\tTraining batch 222 loss: 0.776017\n",
            "\tTraining batch 223 loss: 1.055666\n",
            "\tTraining batch 224 loss: 0.887602\n",
            "\tTraining batch 225 loss: 1.248567\n",
            "\tTraining batch 226 loss: 0.787335\n",
            "\tTraining batch 227 loss: 1.345730\n",
            "\tTraining batch 228 loss: 0.813011\n",
            "\tTraining batch 229 loss: 0.862119\n",
            "\tTraining batch 230 loss: 0.712666\n",
            "\tTraining batch 231 loss: 1.042940\n",
            "\tTraining batch 232 loss: 1.239791\n",
            "\tTraining batch 233 loss: 1.085688\n",
            "\tTraining batch 234 loss: 0.896704\n",
            "\tTraining batch 235 loss: 1.193856\n",
            "\tTraining batch 236 loss: 0.890500\n",
            "\tTraining batch 237 loss: 0.810724\n",
            "\tTraining batch 238 loss: 0.985918\n",
            "\tTraining batch 239 loss: 0.876441\n",
            "\tTraining batch 240 loss: 0.848196\n",
            "\tTraining batch 241 loss: 0.966831\n",
            "\tTraining batch 242 loss: 0.855989\n",
            "\tTraining batch 243 loss: 0.920474\n",
            "\tTraining batch 244 loss: 0.873948\n",
            "\tTraining batch 245 loss: 0.878243\n",
            "\tTraining batch 246 loss: 0.886348\n",
            "\tTraining batch 247 loss: 0.983996\n",
            "\tTraining batch 248 loss: 0.930091\n",
            "\tTraining batch 249 loss: 1.047350\n",
            "\tTraining batch 250 loss: 0.813624\n",
            "\tTraining batch 251 loss: 0.596391\n",
            "\tTraining batch 252 loss: 0.852488\n",
            "\tTraining batch 253 loss: 0.858627\n",
            "\tTraining batch 254 loss: 1.114492\n",
            "\tTraining batch 255 loss: 0.818166\n",
            "\tTraining batch 256 loss: 1.303982\n",
            "\tTraining batch 257 loss: 0.947915\n",
            "\tTraining batch 258 loss: 0.933806\n",
            "\tTraining batch 259 loss: 0.867974\n",
            "\tTraining batch 260 loss: 0.967270\n",
            "\tTraining batch 261 loss: 1.079156\n",
            "\tTraining batch 262 loss: 0.921488\n",
            "\tTraining batch 263 loss: 1.080209\n",
            "\tTraining batch 264 loss: 1.162540\n",
            "\tTraining batch 265 loss: 1.046952\n",
            "\tTraining batch 266 loss: 0.924830\n",
            "\tTraining batch 267 loss: 1.076619\n",
            "\tTraining batch 268 loss: 1.008877\n",
            "\tTraining batch 269 loss: 1.114138\n",
            "\tTraining batch 270 loss: 0.729114\n",
            "\tTraining batch 271 loss: 0.970252\n",
            "\tTraining batch 272 loss: 1.313057\n",
            "\tTraining batch 273 loss: 0.815600\n",
            "\tTraining batch 274 loss: 1.295514\n",
            "\tTraining batch 275 loss: 0.683074\n",
            "\tTraining batch 276 loss: 0.885634\n",
            "\tTraining batch 277 loss: 0.979455\n",
            "\tTraining batch 278 loss: 1.147125\n",
            "\tTraining batch 279 loss: 0.834001\n",
            "\tTraining batch 280 loss: 0.896596\n",
            "\tTraining batch 281 loss: 1.165681\n",
            "\tTraining batch 282 loss: 0.812219\n",
            "\tTraining batch 283 loss: 1.291748\n",
            "\tTraining batch 284 loss: 1.053273\n",
            "\tTraining batch 285 loss: 1.003336\n",
            "\tTraining batch 286 loss: 1.418404\n",
            "\tTraining batch 287 loss: 0.697397\n",
            "\tTraining batch 288 loss: 1.242951\n",
            "\tTraining batch 289 loss: 0.976380\n",
            "\tTraining batch 290 loss: 0.938483\n",
            "\tTraining batch 291 loss: 1.064782\n",
            "\tTraining batch 292 loss: 1.058133\n",
            "\tTraining batch 293 loss: 0.896997\n",
            "\tTraining batch 294 loss: 1.089719\n",
            "\tTraining batch 295 loss: 1.218018\n",
            "\tTraining batch 296 loss: 0.769078\n",
            "\tTraining batch 297 loss: 1.417776\n",
            "\tTraining batch 298 loss: 1.039075\n",
            "\tTraining batch 299 loss: 1.216398\n",
            "\tTraining batch 300 loss: 0.983049\n",
            "\tTraining batch 301 loss: 0.981467\n",
            "\tTraining batch 302 loss: 0.876777\n",
            "\tTraining batch 303 loss: 1.077288\n",
            "\tTraining batch 304 loss: 1.142508\n",
            "\tTraining batch 305 loss: 0.954039\n",
            "\tTraining batch 306 loss: 0.874814\n",
            "\tTraining batch 307 loss: 0.878392\n",
            "\tTraining batch 308 loss: 1.234963\n",
            "\tTraining batch 309 loss: 0.757502\n",
            "\tTraining batch 310 loss: 0.842302\n",
            "\tTraining batch 311 loss: 1.006213\n",
            "\tTraining batch 312 loss: 0.840289\n",
            "\tTraining batch 313 loss: 1.070048\n",
            "\tTraining batch 314 loss: 0.955191\n",
            "\tTraining batch 315 loss: 1.178161\n",
            "\tTraining batch 316 loss: 1.311841\n",
            "\tTraining batch 317 loss: 0.990907\n",
            "\tTraining batch 318 loss: 1.083485\n",
            "\tTraining batch 319 loss: 1.019328\n",
            "\tTraining batch 320 loss: 0.992349\n",
            "\tTraining batch 321 loss: 0.827027\n",
            "\tTraining batch 322 loss: 1.120648\n",
            "\tTraining batch 323 loss: 0.981957\n",
            "\tTraining batch 324 loss: 1.323744\n",
            "\tTraining batch 325 loss: 0.950872\n",
            "\tTraining batch 326 loss: 1.164039\n",
            "\tTraining batch 327 loss: 1.081954\n",
            "\tTraining batch 328 loss: 1.121226\n",
            "\tTraining batch 329 loss: 1.238481\n",
            "\tTraining batch 330 loss: 1.326217\n",
            "\tTraining batch 331 loss: 0.793212\n",
            "\tTraining batch 332 loss: 0.899486\n",
            "\tTraining batch 333 loss: 0.834413\n",
            "\tTraining batch 334 loss: 1.164818\n",
            "\tTraining batch 335 loss: 0.808959\n",
            "\tTraining batch 336 loss: 1.111616\n",
            "\tTraining batch 337 loss: 0.973909\n",
            "\tTraining batch 338 loss: 0.879331\n",
            "\tTraining batch 339 loss: 0.574511\n",
            "\tTraining batch 340 loss: 0.980882\n",
            "\tTraining batch 341 loss: 0.802299\n",
            "\tTraining batch 342 loss: 0.920205\n",
            "\tTraining batch 343 loss: 0.589331\n",
            "\tTraining batch 344 loss: 0.905809\n",
            "\tTraining batch 345 loss: 1.083292\n",
            "\tTraining batch 346 loss: 1.087569\n",
            "\tTraining batch 347 loss: 0.817060\n",
            "\tTraining batch 348 loss: 1.069768\n",
            "\tTraining batch 349 loss: 0.681574\n",
            "\tTraining batch 350 loss: 0.986166\n",
            "\tTraining batch 351 loss: 0.913685\n",
            "\tTraining batch 352 loss: 1.244752\n",
            "\tTraining batch 353 loss: 0.751161\n",
            "\tTraining batch 354 loss: 1.323257\n",
            "\tTraining batch 355 loss: 0.902892\n",
            "\tTraining batch 356 loss: 1.155487\n",
            "\tTraining batch 357 loss: 1.145642\n",
            "\tTraining batch 358 loss: 0.729505\n",
            "\tTraining batch 359 loss: 0.873291\n",
            "\tTraining batch 360 loss: 1.126046\n",
            "\tTraining batch 361 loss: 1.013737\n",
            "\tTraining batch 362 loss: 1.198156\n",
            "\tTraining batch 363 loss: 0.970251\n",
            "\tTraining batch 364 loss: 0.978208\n",
            "\tTraining batch 365 loss: 1.288682\n",
            "\tTraining batch 366 loss: 1.059826\n",
            "\tTraining batch 367 loss: 0.816776\n",
            "\tTraining batch 368 loss: 0.845981\n",
            "\tTraining batch 369 loss: 0.941928\n",
            "\tTraining batch 370 loss: 1.024686\n",
            "\tTraining batch 371 loss: 1.118483\n",
            "\tTraining batch 372 loss: 1.044465\n",
            "\tTraining batch 373 loss: 1.006112\n",
            "\tTraining batch 374 loss: 0.820122\n",
            "\tTraining batch 375 loss: 0.805946\n",
            "\tTraining batch 376 loss: 0.885010\n",
            "\tTraining batch 377 loss: 0.972342\n",
            "\tTraining batch 378 loss: 1.178985\n",
            "\tTraining batch 379 loss: 0.967942\n",
            "\tTraining batch 380 loss: 0.824431\n",
            "\tTraining batch 381 loss: 0.928582\n",
            "\tTraining batch 382 loss: 0.946939\n",
            "\tTraining batch 383 loss: 0.725504\n",
            "\tTraining batch 384 loss: 0.762975\n",
            "\tTraining batch 385 loss: 0.811792\n",
            "\tTraining batch 386 loss: 0.825886\n",
            "\tTraining batch 387 loss: 0.718512\n",
            "\tTraining batch 388 loss: 1.011259\n",
            "\tTraining batch 389 loss: 1.054352\n",
            "\tTraining batch 390 loss: 1.020651\n",
            "\tTraining batch 391 loss: 0.956540\n",
            "\tTraining batch 392 loss: 0.696621\n",
            "\tTraining batch 393 loss: 1.066128\n",
            "\tTraining batch 394 loss: 0.726348\n",
            "\tTraining batch 395 loss: 0.696400\n",
            "\tTraining batch 396 loss: 1.108534\n",
            "\tTraining batch 397 loss: 1.097057\n",
            "\tTraining batch 398 loss: 1.186386\n",
            "\tTraining batch 399 loss: 1.151606\n",
            "\tTraining batch 400 loss: 1.237972\n",
            "\tTraining batch 401 loss: 1.019278\n",
            "\tTraining batch 402 loss: 0.931036\n",
            "\tTraining batch 403 loss: 1.036748\n",
            "\tTraining batch 404 loss: 0.871570\n",
            "\tTraining batch 405 loss: 0.858982\n",
            "\tTraining batch 406 loss: 1.028411\n",
            "\tTraining batch 407 loss: 0.734949\n",
            "\tTraining batch 408 loss: 1.010503\n",
            "\tTraining batch 409 loss: 0.982255\n",
            "\tTraining batch 410 loss: 1.017365\n",
            "\tTraining batch 411 loss: 0.755829\n",
            "\tTraining batch 412 loss: 0.877514\n",
            "\tTraining batch 413 loss: 1.084488\n",
            "\tTraining batch 414 loss: 0.981849\n",
            "\tTraining batch 415 loss: 0.974212\n",
            "\tTraining batch 416 loss: 1.091166\n",
            "\tTraining batch 417 loss: 1.029880\n",
            "\tTraining batch 418 loss: 1.200136\n",
            "\tTraining batch 419 loss: 1.214117\n",
            "\tTraining batch 420 loss: 0.905863\n",
            "\tTraining batch 421 loss: 1.029935\n",
            "\tTraining batch 422 loss: 0.816555\n",
            "\tTraining batch 423 loss: 1.543598\n",
            "Training set: Average loss: 1.029917\n",
            "Validation set: Average loss: 0.854984, Accuracy: 3703/5276 (70%)\n",
            "\n",
            "Epoch 8\n",
            "\tTraining batch 1 loss: 0.888150\n",
            "\tTraining batch 2 loss: 1.092041\n",
            "\tTraining batch 3 loss: 0.895211\n",
            "\tTraining batch 4 loss: 1.057094\n",
            "\tTraining batch 5 loss: 1.164466\n",
            "\tTraining batch 6 loss: 1.097225\n",
            "\tTraining batch 7 loss: 1.099699\n",
            "\tTraining batch 8 loss: 1.314090\n",
            "\tTraining batch 9 loss: 1.182534\n",
            "\tTraining batch 10 loss: 1.058882\n",
            "\tTraining batch 11 loss: 0.740200\n",
            "\tTraining batch 12 loss: 1.373353\n",
            "\tTraining batch 13 loss: 1.196062\n",
            "\tTraining batch 14 loss: 1.008862\n",
            "\tTraining batch 15 loss: 1.279812\n",
            "\tTraining batch 16 loss: 0.856976\n",
            "\tTraining batch 17 loss: 0.611226\n",
            "\tTraining batch 18 loss: 1.066346\n",
            "\tTraining batch 19 loss: 1.087705\n",
            "\tTraining batch 20 loss: 0.996103\n",
            "\tTraining batch 21 loss: 1.568983\n",
            "\tTraining batch 22 loss: 0.888161\n",
            "\tTraining batch 23 loss: 0.840547\n",
            "\tTraining batch 24 loss: 1.010286\n",
            "\tTraining batch 25 loss: 1.117377\n",
            "\tTraining batch 26 loss: 0.876604\n",
            "\tTraining batch 27 loss: 0.963348\n",
            "\tTraining batch 28 loss: 1.155348\n",
            "\tTraining batch 29 loss: 1.102318\n",
            "\tTraining batch 30 loss: 1.075659\n",
            "\tTraining batch 31 loss: 1.230189\n",
            "\tTraining batch 32 loss: 0.946804\n",
            "\tTraining batch 33 loss: 1.002181\n",
            "\tTraining batch 34 loss: 0.873198\n",
            "\tTraining batch 35 loss: 0.642588\n",
            "\tTraining batch 36 loss: 1.232114\n",
            "\tTraining batch 37 loss: 1.094316\n",
            "\tTraining batch 38 loss: 0.735222\n",
            "\tTraining batch 39 loss: 1.159977\n",
            "\tTraining batch 40 loss: 0.794555\n",
            "\tTraining batch 41 loss: 0.749544\n",
            "\tTraining batch 42 loss: 1.131316\n",
            "\tTraining batch 43 loss: 0.756580\n",
            "\tTraining batch 44 loss: 1.074456\n",
            "\tTraining batch 45 loss: 0.897031\n",
            "\tTraining batch 46 loss: 0.837943\n",
            "\tTraining batch 47 loss: 1.249325\n",
            "\tTraining batch 48 loss: 1.001558\n",
            "\tTraining batch 49 loss: 1.325384\n",
            "\tTraining batch 50 loss: 0.967426\n",
            "\tTraining batch 51 loss: 0.869514\n",
            "\tTraining batch 52 loss: 1.061982\n",
            "\tTraining batch 53 loss: 0.688897\n",
            "\tTraining batch 54 loss: 0.887376\n",
            "\tTraining batch 55 loss: 1.129390\n",
            "\tTraining batch 56 loss: 0.968025\n",
            "\tTraining batch 57 loss: 1.309388\n",
            "\tTraining batch 58 loss: 0.888260\n",
            "\tTraining batch 59 loss: 1.217505\n",
            "\tTraining batch 60 loss: 1.211618\n",
            "\tTraining batch 61 loss: 1.103765\n",
            "\tTraining batch 62 loss: 1.110506\n",
            "\tTraining batch 63 loss: 1.090487\n",
            "\tTraining batch 64 loss: 0.906306\n",
            "\tTraining batch 65 loss: 1.222493\n",
            "\tTraining batch 66 loss: 1.255104\n",
            "\tTraining batch 67 loss: 0.857077\n",
            "\tTraining batch 68 loss: 1.213274\n",
            "\tTraining batch 69 loss: 1.464498\n",
            "\tTraining batch 70 loss: 1.432214\n",
            "\tTraining batch 71 loss: 1.283965\n",
            "\tTraining batch 72 loss: 1.093882\n",
            "\tTraining batch 73 loss: 1.238830\n",
            "\tTraining batch 74 loss: 1.170261\n",
            "\tTraining batch 75 loss: 1.206882\n",
            "\tTraining batch 76 loss: 1.151907\n",
            "\tTraining batch 77 loss: 1.447631\n",
            "\tTraining batch 78 loss: 1.443933\n",
            "\tTraining batch 79 loss: 2.279868\n",
            "\tTraining batch 80 loss: 1.023927\n",
            "\tTraining batch 81 loss: 1.321593\n",
            "\tTraining batch 82 loss: 1.023453\n",
            "\tTraining batch 83 loss: 1.312866\n",
            "\tTraining batch 84 loss: 1.408068\n",
            "\tTraining batch 85 loss: 1.187616\n",
            "\tTraining batch 86 loss: 1.009301\n",
            "\tTraining batch 87 loss: 1.572266\n",
            "\tTraining batch 88 loss: 1.628657\n",
            "\tTraining batch 89 loss: 1.546299\n",
            "\tTraining batch 90 loss: 1.282983\n",
            "\tTraining batch 91 loss: 1.195475\n",
            "\tTraining batch 92 loss: 1.051998\n",
            "\tTraining batch 93 loss: 1.297351\n",
            "\tTraining batch 94 loss: 1.470855\n",
            "\tTraining batch 95 loss: 1.425787\n",
            "\tTraining batch 96 loss: 1.119984\n",
            "\tTraining batch 97 loss: 1.178892\n",
            "\tTraining batch 98 loss: 1.546340\n",
            "\tTraining batch 99 loss: 1.347586\n",
            "\tTraining batch 100 loss: 1.286685\n",
            "\tTraining batch 101 loss: 1.287322\n",
            "\tTraining batch 102 loss: 1.269983\n",
            "\tTraining batch 103 loss: 1.296473\n",
            "\tTraining batch 104 loss: 1.351393\n",
            "\tTraining batch 105 loss: 1.290250\n",
            "\tTraining batch 106 loss: 1.168080\n",
            "\tTraining batch 107 loss: 1.268997\n",
            "\tTraining batch 108 loss: 1.178835\n",
            "\tTraining batch 109 loss: 1.233352\n",
            "\tTraining batch 110 loss: 1.247252\n",
            "\tTraining batch 111 loss: 0.922865\n",
            "\tTraining batch 112 loss: 0.892873\n",
            "\tTraining batch 113 loss: 1.472318\n",
            "\tTraining batch 114 loss: 1.139638\n",
            "\tTraining batch 115 loss: 1.128458\n",
            "\tTraining batch 116 loss: 1.075380\n",
            "\tTraining batch 117 loss: 1.540197\n",
            "\tTraining batch 118 loss: 1.048053\n",
            "\tTraining batch 119 loss: 1.499426\n",
            "\tTraining batch 120 loss: 1.242613\n",
            "\tTraining batch 121 loss: 1.115851\n",
            "\tTraining batch 122 loss: 1.241570\n",
            "\tTraining batch 123 loss: 0.942922\n",
            "\tTraining batch 124 loss: 0.988263\n",
            "\tTraining batch 125 loss: 1.253636\n",
            "\tTraining batch 126 loss: 1.115298\n",
            "\tTraining batch 127 loss: 1.384628\n",
            "\tTraining batch 128 loss: 1.086036\n",
            "\tTraining batch 129 loss: 1.093015\n",
            "\tTraining batch 130 loss: 1.162326\n",
            "\tTraining batch 131 loss: 1.219265\n",
            "\tTraining batch 132 loss: 1.043953\n",
            "\tTraining batch 133 loss: 1.372726\n",
            "\tTraining batch 134 loss: 1.162477\n",
            "\tTraining batch 135 loss: 1.201699\n",
            "\tTraining batch 136 loss: 1.037133\n",
            "\tTraining batch 137 loss: 1.151134\n",
            "\tTraining batch 138 loss: 0.960400\n",
            "\tTraining batch 139 loss: 0.946301\n",
            "\tTraining batch 140 loss: 1.241410\n",
            "\tTraining batch 141 loss: 1.024546\n",
            "\tTraining batch 142 loss: 0.946263\n",
            "\tTraining batch 143 loss: 1.205111\n",
            "\tTraining batch 144 loss: 1.056418\n",
            "\tTraining batch 145 loss: 1.099462\n",
            "\tTraining batch 146 loss: 1.340393\n",
            "\tTraining batch 147 loss: 1.442516\n",
            "\tTraining batch 148 loss: 0.992904\n",
            "\tTraining batch 149 loss: 0.902504\n",
            "\tTraining batch 150 loss: 0.930789\n",
            "\tTraining batch 151 loss: 0.888951\n",
            "\tTraining batch 152 loss: 0.969414\n",
            "\tTraining batch 153 loss: 1.087441\n",
            "\tTraining batch 154 loss: 1.112755\n",
            "\tTraining batch 155 loss: 0.880534\n",
            "\tTraining batch 156 loss: 1.009800\n",
            "\tTraining batch 157 loss: 0.835423\n",
            "\tTraining batch 158 loss: 0.993944\n",
            "\tTraining batch 159 loss: 1.005553\n",
            "\tTraining batch 160 loss: 0.901249\n",
            "\tTraining batch 161 loss: 0.901797\n",
            "\tTraining batch 162 loss: 1.058664\n",
            "\tTraining batch 163 loss: 0.926883\n",
            "\tTraining batch 164 loss: 1.189494\n",
            "\tTraining batch 165 loss: 0.951466\n",
            "\tTraining batch 166 loss: 1.062580\n",
            "\tTraining batch 167 loss: 0.939778\n",
            "\tTraining batch 168 loss: 1.115806\n",
            "\tTraining batch 169 loss: 1.362455\n",
            "\tTraining batch 170 loss: 1.127800\n",
            "\tTraining batch 171 loss: 0.955148\n",
            "\tTraining batch 172 loss: 0.961963\n",
            "\tTraining batch 173 loss: 1.211963\n",
            "\tTraining batch 174 loss: 1.016027\n",
            "\tTraining batch 175 loss: 1.330255\n",
            "\tTraining batch 176 loss: 1.024370\n",
            "\tTraining batch 177 loss: 1.401588\n",
            "\tTraining batch 178 loss: 0.877277\n",
            "\tTraining batch 179 loss: 1.229444\n",
            "\tTraining batch 180 loss: 1.425154\n",
            "\tTraining batch 181 loss: 1.067049\n",
            "\tTraining batch 182 loss: 1.278148\n",
            "\tTraining batch 183 loss: 1.341175\n",
            "\tTraining batch 184 loss: 1.560694\n",
            "\tTraining batch 185 loss: 0.701913\n",
            "\tTraining batch 186 loss: 0.979662\n",
            "\tTraining batch 187 loss: 0.989058\n",
            "\tTraining batch 188 loss: 0.573636\n",
            "\tTraining batch 189 loss: 0.909276\n",
            "\tTraining batch 190 loss: 1.112004\n",
            "\tTraining batch 191 loss: 1.088956\n",
            "\tTraining batch 192 loss: 1.205102\n",
            "\tTraining batch 193 loss: 1.094800\n",
            "\tTraining batch 194 loss: 0.574085\n",
            "\tTraining batch 195 loss: 1.227048\n",
            "\tTraining batch 196 loss: 1.414651\n",
            "\tTraining batch 197 loss: 0.897440\n",
            "\tTraining batch 198 loss: 0.882857\n",
            "\tTraining batch 199 loss: 1.176334\n",
            "\tTraining batch 200 loss: 0.808610\n",
            "\tTraining batch 201 loss: 1.506330\n",
            "\tTraining batch 202 loss: 1.305533\n",
            "\tTraining batch 203 loss: 1.395386\n",
            "\tTraining batch 204 loss: 1.165362\n",
            "\tTraining batch 205 loss: 1.222143\n",
            "\tTraining batch 206 loss: 1.070432\n",
            "\tTraining batch 207 loss: 1.117836\n",
            "\tTraining batch 208 loss: 1.059860\n",
            "\tTraining batch 209 loss: 0.912757\n",
            "\tTraining batch 210 loss: 0.987792\n",
            "\tTraining batch 211 loss: 1.018919\n",
            "\tTraining batch 212 loss: 1.155193\n",
            "\tTraining batch 213 loss: 1.119800\n",
            "\tTraining batch 214 loss: 1.071394\n",
            "\tTraining batch 215 loss: 1.341487\n",
            "\tTraining batch 216 loss: 0.796896\n",
            "\tTraining batch 217 loss: 0.877859\n",
            "\tTraining batch 218 loss: 0.824846\n",
            "\tTraining batch 219 loss: 1.113133\n",
            "\tTraining batch 220 loss: 0.965347\n",
            "\tTraining batch 221 loss: 0.995422\n",
            "\tTraining batch 222 loss: 0.827626\n",
            "\tTraining batch 223 loss: 1.207630\n",
            "\tTraining batch 224 loss: 1.025949\n",
            "\tTraining batch 225 loss: 1.238672\n",
            "\tTraining batch 226 loss: 0.991418\n",
            "\tTraining batch 227 loss: 1.127788\n",
            "\tTraining batch 228 loss: 0.933742\n",
            "\tTraining batch 229 loss: 1.103083\n",
            "\tTraining batch 230 loss: 0.860398\n",
            "\tTraining batch 231 loss: 1.265770\n",
            "\tTraining batch 232 loss: 1.155088\n",
            "\tTraining batch 233 loss: 1.096260\n",
            "\tTraining batch 234 loss: 0.956203\n",
            "\tTraining batch 235 loss: 1.137880\n",
            "\tTraining batch 236 loss: 0.843620\n",
            "\tTraining batch 237 loss: 0.761275\n",
            "\tTraining batch 238 loss: 0.934488\n",
            "\tTraining batch 239 loss: 1.232875\n",
            "\tTraining batch 240 loss: 0.837029\n",
            "\tTraining batch 241 loss: 1.122585\n",
            "\tTraining batch 242 loss: 0.861970\n",
            "\tTraining batch 243 loss: 1.233202\n",
            "\tTraining batch 244 loss: 0.973675\n",
            "\tTraining batch 245 loss: 0.921574\n",
            "\tTraining batch 246 loss: 0.849390\n",
            "\tTraining batch 247 loss: 1.079134\n",
            "\tTraining batch 248 loss: 1.015908\n",
            "\tTraining batch 249 loss: 0.980630\n",
            "\tTraining batch 250 loss: 0.839996\n",
            "\tTraining batch 251 loss: 0.750419\n",
            "\tTraining batch 252 loss: 1.055601\n",
            "\tTraining batch 253 loss: 0.787152\n",
            "\tTraining batch 254 loss: 1.304450\n",
            "\tTraining batch 255 loss: 0.967986\n",
            "\tTraining batch 256 loss: 1.318029\n",
            "\tTraining batch 257 loss: 0.976852\n",
            "\tTraining batch 258 loss: 1.019755\n",
            "\tTraining batch 259 loss: 0.828951\n",
            "\tTraining batch 260 loss: 1.100127\n",
            "\tTraining batch 261 loss: 1.051042\n",
            "\tTraining batch 262 loss: 0.876182\n",
            "\tTraining batch 263 loss: 0.979076\n",
            "\tTraining batch 264 loss: 1.226001\n",
            "\tTraining batch 265 loss: 1.331941\n",
            "\tTraining batch 266 loss: 0.851879\n",
            "\tTraining batch 267 loss: 1.207872\n",
            "\tTraining batch 268 loss: 1.240701\n",
            "\tTraining batch 269 loss: 1.325488\n",
            "\tTraining batch 270 loss: 0.707251\n",
            "\tTraining batch 271 loss: 0.978214\n",
            "\tTraining batch 272 loss: 1.379447\n",
            "\tTraining batch 273 loss: 0.920650\n",
            "\tTraining batch 274 loss: 1.115764\n",
            "\tTraining batch 275 loss: 0.779130\n",
            "\tTraining batch 276 loss: 1.036895\n",
            "\tTraining batch 277 loss: 0.827592\n",
            "\tTraining batch 278 loss: 0.927540\n",
            "\tTraining batch 279 loss: 0.964429\n",
            "\tTraining batch 280 loss: 1.229329\n",
            "\tTraining batch 281 loss: 1.108139\n",
            "\tTraining batch 282 loss: 0.777729\n",
            "\tTraining batch 283 loss: 1.236354\n",
            "\tTraining batch 284 loss: 1.031604\n",
            "\tTraining batch 285 loss: 0.967418\n",
            "\tTraining batch 286 loss: 1.278183\n",
            "\tTraining batch 287 loss: 0.815869\n",
            "\tTraining batch 288 loss: 1.044400\n",
            "\tTraining batch 289 loss: 0.950437\n",
            "\tTraining batch 290 loss: 0.906225\n",
            "\tTraining batch 291 loss: 1.198550\n",
            "\tTraining batch 292 loss: 0.844325\n",
            "\tTraining batch 293 loss: 0.885721\n",
            "\tTraining batch 294 loss: 1.158362\n",
            "\tTraining batch 295 loss: 1.160923\n",
            "\tTraining batch 296 loss: 0.860384\n",
            "\tTraining batch 297 loss: 1.219884\n",
            "\tTraining batch 298 loss: 0.891816\n",
            "\tTraining batch 299 loss: 1.384588\n",
            "\tTraining batch 300 loss: 0.921807\n",
            "\tTraining batch 301 loss: 0.909020\n",
            "\tTraining batch 302 loss: 1.054367\n",
            "\tTraining batch 303 loss: 1.256503\n",
            "\tTraining batch 304 loss: 1.241991\n",
            "\tTraining batch 305 loss: 0.932552\n",
            "\tTraining batch 306 loss: 0.863098\n",
            "\tTraining batch 307 loss: 0.902653\n",
            "\tTraining batch 308 loss: 0.972618\n",
            "\tTraining batch 309 loss: 1.023265\n",
            "\tTraining batch 310 loss: 0.561493\n",
            "\tTraining batch 311 loss: 1.085524\n",
            "\tTraining batch 312 loss: 0.822048\n",
            "\tTraining batch 313 loss: 0.967756\n",
            "\tTraining batch 314 loss: 0.914852\n",
            "\tTraining batch 315 loss: 1.048154\n",
            "\tTraining batch 316 loss: 1.279126\n",
            "\tTraining batch 317 loss: 1.097131\n",
            "\tTraining batch 318 loss: 0.959329\n",
            "\tTraining batch 319 loss: 0.902490\n",
            "\tTraining batch 320 loss: 1.088470\n",
            "\tTraining batch 321 loss: 1.093593\n",
            "\tTraining batch 322 loss: 1.140105\n",
            "\tTraining batch 323 loss: 1.095405\n",
            "\tTraining batch 324 loss: 1.100975\n",
            "\tTraining batch 325 loss: 0.962991\n",
            "\tTraining batch 326 loss: 1.197766\n",
            "\tTraining batch 327 loss: 1.063383\n",
            "\tTraining batch 328 loss: 1.202063\n",
            "\tTraining batch 329 loss: 1.241605\n",
            "\tTraining batch 330 loss: 1.328330\n",
            "\tTraining batch 331 loss: 0.887328\n",
            "\tTraining batch 332 loss: 0.958780\n",
            "\tTraining batch 333 loss: 0.781472\n",
            "\tTraining batch 334 loss: 1.032656\n",
            "\tTraining batch 335 loss: 0.714820\n",
            "\tTraining batch 336 loss: 1.026352\n",
            "\tTraining batch 337 loss: 0.829805\n",
            "\tTraining batch 338 loss: 0.903527\n",
            "\tTraining batch 339 loss: 0.692036\n",
            "\tTraining batch 340 loss: 0.890377\n",
            "\tTraining batch 341 loss: 0.888367\n",
            "\tTraining batch 342 loss: 1.020650\n",
            "\tTraining batch 343 loss: 0.562057\n",
            "\tTraining batch 344 loss: 0.987389\n",
            "\tTraining batch 345 loss: 1.063343\n",
            "\tTraining batch 346 loss: 1.145511\n",
            "\tTraining batch 347 loss: 0.838398\n",
            "\tTraining batch 348 loss: 1.238012\n",
            "\tTraining batch 349 loss: 0.853020\n",
            "\tTraining batch 350 loss: 1.029262\n",
            "\tTraining batch 351 loss: 0.974585\n",
            "\tTraining batch 352 loss: 1.100706\n",
            "\tTraining batch 353 loss: 0.837973\n",
            "\tTraining batch 354 loss: 1.120280\n",
            "\tTraining batch 355 loss: 1.258370\n",
            "\tTraining batch 356 loss: 1.357856\n",
            "\tTraining batch 357 loss: 1.343233\n",
            "\tTraining batch 358 loss: 0.826589\n",
            "\tTraining batch 359 loss: 0.952449\n",
            "\tTraining batch 360 loss: 1.342543\n",
            "\tTraining batch 361 loss: 1.142894\n",
            "\tTraining batch 362 loss: 1.412106\n",
            "\tTraining batch 363 loss: 1.129182\n",
            "\tTraining batch 364 loss: 1.053231\n",
            "\tTraining batch 365 loss: 1.296868\n",
            "\tTraining batch 366 loss: 1.008340\n",
            "\tTraining batch 367 loss: 0.912239\n",
            "\tTraining batch 368 loss: 0.746374\n",
            "\tTraining batch 369 loss: 1.075480\n",
            "\tTraining batch 370 loss: 1.029212\n",
            "\tTraining batch 371 loss: 1.178325\n",
            "\tTraining batch 372 loss: 1.166439\n",
            "\tTraining batch 373 loss: 1.163145\n",
            "\tTraining batch 374 loss: 0.865791\n",
            "\tTraining batch 375 loss: 0.730449\n",
            "\tTraining batch 376 loss: 1.067275\n",
            "\tTraining batch 377 loss: 1.016385\n",
            "\tTraining batch 378 loss: 0.998688\n",
            "\tTraining batch 379 loss: 1.081994\n",
            "\tTraining batch 380 loss: 1.036901\n",
            "\tTraining batch 381 loss: 1.030228\n",
            "\tTraining batch 382 loss: 0.946451\n",
            "\tTraining batch 383 loss: 0.843434\n",
            "\tTraining batch 384 loss: 0.712010\n",
            "\tTraining batch 385 loss: 0.965425\n",
            "\tTraining batch 386 loss: 0.769730\n",
            "\tTraining batch 387 loss: 0.899607\n",
            "\tTraining batch 388 loss: 1.106198\n",
            "\tTraining batch 389 loss: 1.125413\n",
            "\tTraining batch 390 loss: 1.362262\n",
            "\tTraining batch 391 loss: 0.972168\n",
            "\tTraining batch 392 loss: 0.795473\n",
            "\tTraining batch 393 loss: 1.029018\n",
            "\tTraining batch 394 loss: 0.866464\n",
            "\tTraining batch 395 loss: 0.651850\n",
            "\tTraining batch 396 loss: 0.984752\n",
            "\tTraining batch 397 loss: 1.181659\n",
            "\tTraining batch 398 loss: 1.064958\n",
            "\tTraining batch 399 loss: 0.928375\n",
            "\tTraining batch 400 loss: 1.361158\n",
            "\tTraining batch 401 loss: 0.980681\n",
            "\tTraining batch 402 loss: 0.880890\n",
            "\tTraining batch 403 loss: 1.354146\n",
            "\tTraining batch 404 loss: 0.724370\n",
            "\tTraining batch 405 loss: 0.911854\n",
            "\tTraining batch 406 loss: 0.812730\n",
            "\tTraining batch 407 loss: 0.839042\n",
            "\tTraining batch 408 loss: 0.990928\n",
            "\tTraining batch 409 loss: 0.953365\n",
            "\tTraining batch 410 loss: 1.158044\n",
            "\tTraining batch 411 loss: 0.787918\n",
            "\tTraining batch 412 loss: 0.934054\n",
            "\tTraining batch 413 loss: 1.021067\n",
            "\tTraining batch 414 loss: 1.037966\n",
            "\tTraining batch 415 loss: 0.948009\n",
            "\tTraining batch 416 loss: 1.003230\n",
            "\tTraining batch 417 loss: 1.180785\n",
            "\tTraining batch 418 loss: 1.070197\n",
            "\tTraining batch 419 loss: 1.286693\n",
            "\tTraining batch 420 loss: 1.084091\n",
            "\tTraining batch 421 loss: 1.124623\n",
            "\tTraining batch 422 loss: 0.937627\n",
            "\tTraining batch 423 loss: 0.790799\n",
            "Training set: Average loss: 1.068785\n",
            "Validation set: Average loss: 0.811032, Accuracy: 3807/5276 (72%)\n",
            "\n",
            "Epoch 9\n",
            "\tTraining batch 1 loss: 0.835162\n",
            "\tTraining batch 2 loss: 1.153154\n",
            "\tTraining batch 3 loss: 1.030350\n",
            "\tTraining batch 4 loss: 1.136751\n",
            "\tTraining batch 5 loss: 1.076837\n",
            "\tTraining batch 6 loss: 1.268753\n",
            "\tTraining batch 7 loss: 1.143242\n",
            "\tTraining batch 8 loss: 1.545062\n",
            "\tTraining batch 9 loss: 1.190856\n",
            "\tTraining batch 10 loss: 1.061507\n",
            "\tTraining batch 11 loss: 0.797807\n",
            "\tTraining batch 12 loss: 1.525117\n",
            "\tTraining batch 13 loss: 1.141715\n",
            "\tTraining batch 14 loss: 0.905218\n",
            "\tTraining batch 15 loss: 1.274246\n",
            "\tTraining batch 16 loss: 0.961922\n",
            "\tTraining batch 17 loss: 0.771342\n",
            "\tTraining batch 18 loss: 1.237676\n",
            "\tTraining batch 19 loss: 1.084582\n",
            "\tTraining batch 20 loss: 1.039817\n",
            "\tTraining batch 21 loss: 1.289865\n",
            "\tTraining batch 22 loss: 0.862914\n",
            "\tTraining batch 23 loss: 0.867801\n",
            "\tTraining batch 24 loss: 0.922209\n",
            "\tTraining batch 25 loss: 1.329269\n",
            "\tTraining batch 26 loss: 1.149120\n",
            "\tTraining batch 27 loss: 1.372441\n",
            "\tTraining batch 28 loss: 1.171698\n",
            "\tTraining batch 29 loss: 1.048443\n",
            "\tTraining batch 30 loss: 1.087159\n",
            "\tTraining batch 31 loss: 1.203550\n",
            "\tTraining batch 32 loss: 1.048411\n",
            "\tTraining batch 33 loss: 1.469534\n",
            "\tTraining batch 34 loss: 0.711319\n",
            "\tTraining batch 35 loss: 0.875330\n",
            "\tTraining batch 36 loss: 1.312063\n",
            "\tTraining batch 37 loss: 1.145230\n",
            "\tTraining batch 38 loss: 0.828057\n",
            "\tTraining batch 39 loss: 1.253762\n",
            "\tTraining batch 40 loss: 0.905096\n",
            "\tTraining batch 41 loss: 0.855366\n",
            "\tTraining batch 42 loss: 1.019975\n",
            "\tTraining batch 43 loss: 0.884393\n",
            "\tTraining batch 44 loss: 0.999535\n",
            "\tTraining batch 45 loss: 0.886783\n",
            "\tTraining batch 46 loss: 0.989034\n",
            "\tTraining batch 47 loss: 1.274744\n",
            "\tTraining batch 48 loss: 1.050575\n",
            "\tTraining batch 49 loss: 1.430903\n",
            "\tTraining batch 50 loss: 0.918302\n",
            "\tTraining batch 51 loss: 0.742990\n",
            "\tTraining batch 52 loss: 1.099647\n",
            "\tTraining batch 53 loss: 0.548435\n",
            "\tTraining batch 54 loss: 1.038219\n",
            "\tTraining batch 55 loss: 0.970260\n",
            "\tTraining batch 56 loss: 0.955282\n",
            "\tTraining batch 57 loss: 1.314894\n",
            "\tTraining batch 58 loss: 0.922057\n",
            "\tTraining batch 59 loss: 1.085092\n",
            "\tTraining batch 60 loss: 1.222676\n",
            "\tTraining batch 61 loss: 1.116439\n",
            "\tTraining batch 62 loss: 1.110607\n",
            "\tTraining batch 63 loss: 1.094917\n",
            "\tTraining batch 64 loss: 0.802806\n",
            "\tTraining batch 65 loss: 1.059026\n",
            "\tTraining batch 66 loss: 1.027906\n",
            "\tTraining batch 67 loss: 0.733499\n",
            "\tTraining batch 68 loss: 0.922013\n",
            "\tTraining batch 69 loss: 1.170080\n",
            "\tTraining batch 70 loss: 1.358487\n",
            "\tTraining batch 71 loss: 0.961235\n",
            "\tTraining batch 72 loss: 0.817968\n",
            "\tTraining batch 73 loss: 0.957135\n",
            "\tTraining batch 74 loss: 1.085976\n",
            "\tTraining batch 75 loss: 0.828253\n",
            "\tTraining batch 76 loss: 0.947987\n",
            "\tTraining batch 77 loss: 1.140417\n",
            "\tTraining batch 78 loss: 1.135314\n",
            "\tTraining batch 79 loss: 1.008899\n",
            "\tTraining batch 80 loss: 0.724345\n",
            "\tTraining batch 81 loss: 1.013231\n",
            "\tTraining batch 82 loss: 0.864301\n",
            "\tTraining batch 83 loss: 1.115986\n",
            "\tTraining batch 84 loss: 1.255711\n",
            "\tTraining batch 85 loss: 0.931074\n",
            "\tTraining batch 86 loss: 0.827494\n",
            "\tTraining batch 87 loss: 1.146387\n",
            "\tTraining batch 88 loss: 1.346348\n",
            "\tTraining batch 89 loss: 1.079710\n",
            "\tTraining batch 90 loss: 1.043808\n",
            "\tTraining batch 91 loss: 0.814699\n",
            "\tTraining batch 92 loss: 0.761135\n",
            "\tTraining batch 93 loss: 1.305603\n",
            "\tTraining batch 94 loss: 1.008500\n",
            "\tTraining batch 95 loss: 0.853916\n",
            "\tTraining batch 96 loss: 0.971261\n",
            "\tTraining batch 97 loss: 1.236536\n",
            "\tTraining batch 98 loss: 0.971587\n",
            "\tTraining batch 99 loss: 1.197457\n",
            "\tTraining batch 100 loss: 0.987458\n",
            "\tTraining batch 101 loss: 0.966617\n",
            "\tTraining batch 102 loss: 1.239575\n",
            "\tTraining batch 103 loss: 1.146605\n",
            "\tTraining batch 104 loss: 1.212071\n",
            "\tTraining batch 105 loss: 1.217046\n",
            "\tTraining batch 106 loss: 0.939399\n",
            "\tTraining batch 107 loss: 1.296463\n",
            "\tTraining batch 108 loss: 1.086885\n",
            "\tTraining batch 109 loss: 1.051583\n",
            "\tTraining batch 110 loss: 1.223384\n",
            "\tTraining batch 111 loss: 0.952398\n",
            "\tTraining batch 112 loss: 0.847464\n",
            "\tTraining batch 113 loss: 1.579331\n",
            "\tTraining batch 114 loss: 1.097263\n",
            "\tTraining batch 115 loss: 0.965092\n",
            "\tTraining batch 116 loss: 0.994669\n",
            "\tTraining batch 117 loss: 1.167762\n",
            "\tTraining batch 118 loss: 0.941472\n",
            "\tTraining batch 119 loss: 1.074066\n",
            "\tTraining batch 120 loss: 1.252006\n",
            "\tTraining batch 121 loss: 0.822902\n",
            "\tTraining batch 122 loss: 1.028228\n",
            "\tTraining batch 123 loss: 0.841318\n",
            "\tTraining batch 124 loss: 0.896999\n",
            "\tTraining batch 125 loss: 1.160260\n",
            "\tTraining batch 126 loss: 1.123224\n",
            "\tTraining batch 127 loss: 1.299694\n",
            "\tTraining batch 128 loss: 1.241386\n",
            "\tTraining batch 129 loss: 0.811429\n",
            "\tTraining batch 130 loss: 1.137620\n",
            "\tTraining batch 131 loss: 1.079858\n",
            "\tTraining batch 132 loss: 0.784298\n",
            "\tTraining batch 133 loss: 0.894194\n",
            "\tTraining batch 134 loss: 1.146329\n",
            "\tTraining batch 135 loss: 1.203901\n",
            "\tTraining batch 136 loss: 0.912415\n",
            "\tTraining batch 137 loss: 1.161325\n",
            "\tTraining batch 138 loss: 0.726247\n",
            "\tTraining batch 139 loss: 0.889874\n",
            "\tTraining batch 140 loss: 1.140625\n",
            "\tTraining batch 141 loss: 1.126305\n",
            "\tTraining batch 142 loss: 0.910450\n",
            "\tTraining batch 143 loss: 1.224704\n",
            "\tTraining batch 144 loss: 1.092638\n",
            "\tTraining batch 145 loss: 1.172863\n",
            "\tTraining batch 146 loss: 1.174647\n",
            "\tTraining batch 147 loss: 1.476394\n",
            "\tTraining batch 148 loss: 1.136536\n",
            "\tTraining batch 149 loss: 1.028102\n",
            "\tTraining batch 150 loss: 1.046361\n",
            "\tTraining batch 151 loss: 0.913724\n",
            "\tTraining batch 152 loss: 1.235646\n",
            "\tTraining batch 153 loss: 1.234732\n",
            "\tTraining batch 154 loss: 1.172443\n",
            "\tTraining batch 155 loss: 1.061908\n",
            "\tTraining batch 156 loss: 1.103714\n",
            "\tTraining batch 157 loss: 0.844355\n",
            "\tTraining batch 158 loss: 1.227810\n",
            "\tTraining batch 159 loss: 1.029912\n",
            "\tTraining batch 160 loss: 0.897887\n",
            "\tTraining batch 161 loss: 1.005835\n",
            "\tTraining batch 162 loss: 1.120445\n",
            "\tTraining batch 163 loss: 0.975573\n",
            "\tTraining batch 164 loss: 1.100682\n",
            "\tTraining batch 165 loss: 0.939431\n",
            "\tTraining batch 166 loss: 0.921664\n",
            "\tTraining batch 167 loss: 0.791530\n",
            "\tTraining batch 168 loss: 1.133320\n",
            "\tTraining batch 169 loss: 1.044002\n",
            "\tTraining batch 170 loss: 1.114926\n",
            "\tTraining batch 171 loss: 0.982554\n",
            "\tTraining batch 172 loss: 1.045965\n",
            "\tTraining batch 173 loss: 1.094077\n",
            "\tTraining batch 174 loss: 0.951487\n",
            "\tTraining batch 175 loss: 1.171438\n",
            "\tTraining batch 176 loss: 1.035285\n",
            "\tTraining batch 177 loss: 1.246811\n",
            "\tTraining batch 178 loss: 0.918593\n",
            "\tTraining batch 179 loss: 0.829287\n",
            "\tTraining batch 180 loss: 1.293167\n",
            "\tTraining batch 181 loss: 0.848827\n",
            "\tTraining batch 182 loss: 1.136417\n",
            "\tTraining batch 183 loss: 1.069799\n",
            "\tTraining batch 184 loss: 1.268660\n",
            "\tTraining batch 185 loss: 0.657525\n",
            "\tTraining batch 186 loss: 1.138004\n",
            "\tTraining batch 187 loss: 0.923741\n",
            "\tTraining batch 188 loss: 0.691238\n",
            "\tTraining batch 189 loss: 1.178762\n",
            "\tTraining batch 190 loss: 0.944563\n",
            "\tTraining batch 191 loss: 1.050592\n",
            "\tTraining batch 192 loss: 1.057259\n",
            "\tTraining batch 193 loss: 1.163146\n",
            "\tTraining batch 194 loss: 0.647905\n",
            "\tTraining batch 195 loss: 0.992600\n",
            "\tTraining batch 196 loss: 1.333521\n",
            "\tTraining batch 197 loss: 0.977901\n",
            "\tTraining batch 198 loss: 1.010734\n",
            "\tTraining batch 199 loss: 1.343436\n",
            "\tTraining batch 200 loss: 0.837528\n",
            "\tTraining batch 201 loss: 1.748074\n",
            "\tTraining batch 202 loss: 1.382223\n",
            "\tTraining batch 203 loss: 1.253008\n",
            "\tTraining batch 204 loss: 1.038031\n",
            "\tTraining batch 205 loss: 0.999124\n",
            "\tTraining batch 206 loss: 1.006611\n",
            "\tTraining batch 207 loss: 1.107083\n",
            "\tTraining batch 208 loss: 1.002749\n",
            "\tTraining batch 209 loss: 0.819459\n",
            "\tTraining batch 210 loss: 0.985677\n",
            "\tTraining batch 211 loss: 1.072884\n",
            "\tTraining batch 212 loss: 1.081538\n",
            "\tTraining batch 213 loss: 1.139596\n",
            "\tTraining batch 214 loss: 0.892543\n",
            "\tTraining batch 215 loss: 1.253028\n",
            "\tTraining batch 216 loss: 0.823054\n",
            "\tTraining batch 217 loss: 0.924509\n",
            "\tTraining batch 218 loss: 0.925238\n",
            "\tTraining batch 219 loss: 0.998210\n",
            "\tTraining batch 220 loss: 0.701750\n",
            "\tTraining batch 221 loss: 1.037751\n",
            "\tTraining batch 222 loss: 0.773354\n",
            "\tTraining batch 223 loss: 1.174973\n",
            "\tTraining batch 224 loss: 0.901042\n",
            "\tTraining batch 225 loss: 1.293003\n",
            "\tTraining batch 226 loss: 0.996902\n",
            "\tTraining batch 227 loss: 1.083222\n",
            "\tTraining batch 228 loss: 0.841681\n",
            "\tTraining batch 229 loss: 0.790131\n",
            "\tTraining batch 230 loss: 0.648390\n",
            "\tTraining batch 231 loss: 1.061702\n",
            "\tTraining batch 232 loss: 1.182582\n",
            "\tTraining batch 233 loss: 1.092937\n",
            "\tTraining batch 234 loss: 0.829831\n",
            "\tTraining batch 235 loss: 1.211407\n",
            "\tTraining batch 236 loss: 0.808462\n",
            "\tTraining batch 237 loss: 0.922277\n",
            "\tTraining batch 238 loss: 0.898549\n",
            "\tTraining batch 239 loss: 1.064677\n",
            "\tTraining batch 240 loss: 0.818090\n",
            "\tTraining batch 241 loss: 1.150557\n",
            "\tTraining batch 242 loss: 0.857350\n",
            "\tTraining batch 243 loss: 1.102457\n",
            "\tTraining batch 244 loss: 0.857188\n",
            "\tTraining batch 245 loss: 0.784646\n",
            "\tTraining batch 246 loss: 0.885972\n",
            "\tTraining batch 247 loss: 1.284861\n",
            "\tTraining batch 248 loss: 0.891988\n",
            "\tTraining batch 249 loss: 1.015502\n",
            "\tTraining batch 250 loss: 0.743124\n",
            "\tTraining batch 251 loss: 0.812830\n",
            "\tTraining batch 252 loss: 1.077864\n",
            "\tTraining batch 253 loss: 0.731241\n",
            "\tTraining batch 254 loss: 1.257447\n",
            "\tTraining batch 255 loss: 0.852009\n",
            "\tTraining batch 256 loss: 1.099099\n",
            "\tTraining batch 257 loss: 0.769653\n",
            "\tTraining batch 258 loss: 0.967906\n",
            "\tTraining batch 259 loss: 0.744664\n",
            "\tTraining batch 260 loss: 1.096865\n",
            "\tTraining batch 261 loss: 1.016738\n",
            "\tTraining batch 262 loss: 0.918980\n",
            "\tTraining batch 263 loss: 0.885445\n",
            "\tTraining batch 264 loss: 1.213597\n",
            "\tTraining batch 265 loss: 1.232484\n",
            "\tTraining batch 266 loss: 0.859923\n",
            "\tTraining batch 267 loss: 1.011746\n",
            "\tTraining batch 268 loss: 1.100115\n",
            "\tTraining batch 269 loss: 1.059031\n",
            "\tTraining batch 270 loss: 0.900240\n",
            "\tTraining batch 271 loss: 1.076844\n",
            "\tTraining batch 272 loss: 1.199929\n",
            "\tTraining batch 273 loss: 0.832146\n",
            "\tTraining batch 274 loss: 1.164611\n",
            "\tTraining batch 275 loss: 0.805006\n",
            "\tTraining batch 276 loss: 1.024392\n",
            "\tTraining batch 277 loss: 0.731832\n",
            "\tTraining batch 278 loss: 0.977686\n",
            "\tTraining batch 279 loss: 0.775209\n",
            "\tTraining batch 280 loss: 0.832434\n",
            "\tTraining batch 281 loss: 1.139282\n",
            "\tTraining batch 282 loss: 0.763008\n",
            "\tTraining batch 283 loss: 1.139318\n",
            "\tTraining batch 284 loss: 0.909053\n",
            "\tTraining batch 285 loss: 1.036248\n",
            "\tTraining batch 286 loss: 1.275267\n",
            "\tTraining batch 287 loss: 0.775896\n",
            "\tTraining batch 288 loss: 1.256022\n",
            "\tTraining batch 289 loss: 0.951042\n",
            "\tTraining batch 290 loss: 0.873329\n",
            "\tTraining batch 291 loss: 0.895576\n",
            "\tTraining batch 292 loss: 0.989749\n",
            "\tTraining batch 293 loss: 0.745931\n",
            "\tTraining batch 294 loss: 1.022253\n",
            "\tTraining batch 295 loss: 1.512581\n",
            "\tTraining batch 296 loss: 0.856382\n",
            "\tTraining batch 297 loss: 1.077819\n",
            "\tTraining batch 298 loss: 0.789142\n",
            "\tTraining batch 299 loss: 1.527490\n",
            "\tTraining batch 300 loss: 0.884610\n",
            "\tTraining batch 301 loss: 0.975455\n",
            "\tTraining batch 302 loss: 1.008955\n",
            "\tTraining batch 303 loss: 1.052131\n",
            "\tTraining batch 304 loss: 1.259734\n",
            "\tTraining batch 305 loss: 1.004486\n",
            "\tTraining batch 306 loss: 0.947121\n",
            "\tTraining batch 307 loss: 0.889124\n",
            "\tTraining batch 308 loss: 1.185165\n",
            "\tTraining batch 309 loss: 1.060417\n",
            "\tTraining batch 310 loss: 0.542012\n",
            "\tTraining batch 311 loss: 0.953421\n",
            "\tTraining batch 312 loss: 0.804938\n",
            "\tTraining batch 313 loss: 0.985649\n",
            "\tTraining batch 314 loss: 1.042377\n",
            "\tTraining batch 315 loss: 1.005216\n",
            "\tTraining batch 316 loss: 1.188191\n",
            "\tTraining batch 317 loss: 1.007724\n",
            "\tTraining batch 318 loss: 0.887983\n",
            "\tTraining batch 319 loss: 0.942918\n",
            "\tTraining batch 320 loss: 0.982274\n",
            "\tTraining batch 321 loss: 0.960201\n",
            "\tTraining batch 322 loss: 1.143794\n",
            "\tTraining batch 323 loss: 0.714692\n",
            "\tTraining batch 324 loss: 0.904622\n",
            "\tTraining batch 325 loss: 0.807602\n",
            "\tTraining batch 326 loss: 1.116450\n",
            "\tTraining batch 327 loss: 1.045537\n",
            "\tTraining batch 328 loss: 1.067588\n",
            "\tTraining batch 329 loss: 1.207081\n",
            "\tTraining batch 330 loss: 1.195757\n",
            "\tTraining batch 331 loss: 0.823447\n",
            "\tTraining batch 332 loss: 0.928960\n",
            "\tTraining batch 333 loss: 0.818695\n",
            "\tTraining batch 334 loss: 1.052820\n",
            "\tTraining batch 335 loss: 0.810186\n",
            "\tTraining batch 336 loss: 1.025180\n",
            "\tTraining batch 337 loss: 0.866568\n",
            "\tTraining batch 338 loss: 1.050728\n",
            "\tTraining batch 339 loss: 0.738450\n",
            "\tTraining batch 340 loss: 0.776087\n",
            "\tTraining batch 341 loss: 0.854053\n",
            "\tTraining batch 342 loss: 1.077012\n",
            "\tTraining batch 343 loss: 0.660989\n",
            "\tTraining batch 344 loss: 0.912683\n",
            "\tTraining batch 345 loss: 0.911449\n",
            "\tTraining batch 346 loss: 1.209431\n",
            "\tTraining batch 347 loss: 0.918167\n",
            "\tTraining batch 348 loss: 1.102050\n",
            "\tTraining batch 349 loss: 1.085892\n",
            "\tTraining batch 350 loss: 1.050623\n",
            "\tTraining batch 351 loss: 0.904206\n",
            "\tTraining batch 352 loss: 0.938232\n",
            "\tTraining batch 353 loss: 0.865811\n",
            "\tTraining batch 354 loss: 0.730179\n",
            "\tTraining batch 355 loss: 1.094595\n",
            "\tTraining batch 356 loss: 1.385618\n",
            "\tTraining batch 357 loss: 1.155325\n",
            "\tTraining batch 358 loss: 0.810047\n",
            "\tTraining batch 359 loss: 0.767021\n",
            "\tTraining batch 360 loss: 1.015769\n",
            "\tTraining batch 361 loss: 0.968879\n",
            "\tTraining batch 362 loss: 1.027426\n",
            "\tTraining batch 363 loss: 0.819187\n",
            "\tTraining batch 364 loss: 0.998473\n",
            "\tTraining batch 365 loss: 1.247078\n",
            "\tTraining batch 366 loss: 1.095287\n",
            "\tTraining batch 367 loss: 0.969143\n",
            "\tTraining batch 368 loss: 0.902587\n",
            "\tTraining batch 369 loss: 0.902657\n",
            "\tTraining batch 370 loss: 1.014711\n",
            "\tTraining batch 371 loss: 1.214552\n",
            "\tTraining batch 372 loss: 1.425287\n",
            "\tTraining batch 373 loss: 1.251512\n",
            "\tTraining batch 374 loss: 0.832143\n",
            "\tTraining batch 375 loss: 0.791582\n",
            "\tTraining batch 376 loss: 0.900768\n",
            "\tTraining batch 377 loss: 1.049240\n",
            "\tTraining batch 378 loss: 1.160580\n",
            "\tTraining batch 379 loss: 1.004340\n",
            "\tTraining batch 380 loss: 1.087677\n",
            "\tTraining batch 381 loss: 0.973846\n",
            "\tTraining batch 382 loss: 0.856153\n",
            "\tTraining batch 383 loss: 0.913156\n",
            "\tTraining batch 384 loss: 0.762408\n",
            "\tTraining batch 385 loss: 0.919374\n",
            "\tTraining batch 386 loss: 0.850704\n",
            "\tTraining batch 387 loss: 0.818392\n",
            "\tTraining batch 388 loss: 1.198968\n",
            "\tTraining batch 389 loss: 1.087778\n",
            "\tTraining batch 390 loss: 1.302000\n",
            "\tTraining batch 391 loss: 0.892796\n",
            "\tTraining batch 392 loss: 0.838024\n",
            "\tTraining batch 393 loss: 1.202713\n",
            "\tTraining batch 394 loss: 0.930081\n",
            "\tTraining batch 395 loss: 0.613721\n",
            "\tTraining batch 396 loss: 1.106106\n",
            "\tTraining batch 397 loss: 1.326813\n",
            "\tTraining batch 398 loss: 1.264304\n",
            "\tTraining batch 399 loss: 1.057759\n",
            "\tTraining batch 400 loss: 1.388588\n",
            "\tTraining batch 401 loss: 1.146408\n",
            "\tTraining batch 402 loss: 0.949171\n",
            "\tTraining batch 403 loss: 1.079901\n",
            "\tTraining batch 404 loss: 0.850487\n",
            "\tTraining batch 405 loss: 0.902971\n",
            "\tTraining batch 406 loss: 0.992532\n",
            "\tTraining batch 407 loss: 0.721490\n",
            "\tTraining batch 408 loss: 0.903831\n",
            "\tTraining batch 409 loss: 0.916906\n",
            "\tTraining batch 410 loss: 1.003443\n",
            "\tTraining batch 411 loss: 0.891860\n",
            "\tTraining batch 412 loss: 1.214200\n",
            "\tTraining batch 413 loss: 1.195250\n",
            "\tTraining batch 414 loss: 0.968886\n",
            "\tTraining batch 415 loss: 0.955872\n",
            "\tTraining batch 416 loss: 0.940118\n",
            "\tTraining batch 417 loss: 0.841297\n",
            "\tTraining batch 418 loss: 1.023462\n",
            "\tTraining batch 419 loss: 0.957510\n",
            "\tTraining batch 420 loss: 0.911522\n",
            "\tTraining batch 421 loss: 1.050659\n",
            "\tTraining batch 422 loss: 0.748717\n",
            "\tTraining batch 423 loss: 0.930770\n",
            "Training set: Average loss: 1.020169\n",
            "Validation set: Average loss: 0.828474, Accuracy: 3734/5276 (71%)\n",
            "\n",
            "Epoch 10\n",
            "\tTraining batch 1 loss: 0.779742\n",
            "\tTraining batch 2 loss: 1.147979\n",
            "\tTraining batch 3 loss: 0.803436\n",
            "\tTraining batch 4 loss: 1.097565\n",
            "\tTraining batch 5 loss: 0.965304\n",
            "\tTraining batch 6 loss: 1.187819\n",
            "\tTraining batch 7 loss: 0.945894\n",
            "\tTraining batch 8 loss: 1.235035\n",
            "\tTraining batch 9 loss: 1.327440\n",
            "\tTraining batch 10 loss: 1.017505\n",
            "\tTraining batch 11 loss: 0.900445\n",
            "\tTraining batch 12 loss: 1.532616\n",
            "\tTraining batch 13 loss: 1.142605\n",
            "\tTraining batch 14 loss: 0.948711\n",
            "\tTraining batch 15 loss: 1.445011\n",
            "\tTraining batch 16 loss: 0.934069\n",
            "\tTraining batch 17 loss: 0.797438\n",
            "\tTraining batch 18 loss: 1.329395\n",
            "\tTraining batch 19 loss: 1.056677\n",
            "\tTraining batch 20 loss: 1.120503\n",
            "\tTraining batch 21 loss: 1.269522\n",
            "\tTraining batch 22 loss: 0.948790\n",
            "\tTraining batch 23 loss: 0.694269\n",
            "\tTraining batch 24 loss: 0.996199\n",
            "\tTraining batch 25 loss: 1.066030\n",
            "\tTraining batch 26 loss: 0.958390\n",
            "\tTraining batch 27 loss: 0.874727\n",
            "\tTraining batch 28 loss: 1.204284\n",
            "\tTraining batch 29 loss: 0.983405\n",
            "\tTraining batch 30 loss: 0.838270\n",
            "\tTraining batch 31 loss: 1.086944\n",
            "\tTraining batch 32 loss: 0.998041\n",
            "\tTraining batch 33 loss: 1.385433\n",
            "\tTraining batch 34 loss: 1.035469\n",
            "\tTraining batch 35 loss: 0.833691\n",
            "\tTraining batch 36 loss: 1.198238\n",
            "\tTraining batch 37 loss: 1.095174\n",
            "\tTraining batch 38 loss: 0.778767\n",
            "\tTraining batch 39 loss: 1.233495\n",
            "\tTraining batch 40 loss: 0.738711\n",
            "\tTraining batch 41 loss: 0.828201\n",
            "\tTraining batch 42 loss: 1.319201\n",
            "\tTraining batch 43 loss: 0.765848\n",
            "\tTraining batch 44 loss: 1.088485\n",
            "\tTraining batch 45 loss: 0.955895\n",
            "\tTraining batch 46 loss: 1.010507\n",
            "\tTraining batch 47 loss: 1.272829\n",
            "\tTraining batch 48 loss: 1.104541\n",
            "\tTraining batch 49 loss: 1.189829\n",
            "\tTraining batch 50 loss: 0.909546\n",
            "\tTraining batch 51 loss: 0.875974\n",
            "\tTraining batch 52 loss: 1.124613\n",
            "\tTraining batch 53 loss: 0.532201\n",
            "\tTraining batch 54 loss: 1.006549\n",
            "\tTraining batch 55 loss: 1.009741\n",
            "\tTraining batch 56 loss: 0.859673\n",
            "\tTraining batch 57 loss: 1.197855\n",
            "\tTraining batch 58 loss: 0.799192\n",
            "\tTraining batch 59 loss: 1.424428\n",
            "\tTraining batch 60 loss: 1.340610\n",
            "\tTraining batch 61 loss: 0.949224\n",
            "\tTraining batch 62 loss: 1.085586\n",
            "\tTraining batch 63 loss: 0.962196\n",
            "\tTraining batch 64 loss: 0.926093\n",
            "\tTraining batch 65 loss: 1.221147\n",
            "\tTraining batch 66 loss: 0.933198\n",
            "\tTraining batch 67 loss: 0.731839\n",
            "\tTraining batch 68 loss: 0.930819\n",
            "\tTraining batch 69 loss: 1.089006\n",
            "\tTraining batch 70 loss: 1.082068\n",
            "\tTraining batch 71 loss: 1.034513\n",
            "\tTraining batch 72 loss: 0.848503\n",
            "\tTraining batch 73 loss: 0.982145\n",
            "\tTraining batch 74 loss: 1.090946\n",
            "\tTraining batch 75 loss: 0.893582\n",
            "\tTraining batch 76 loss: 0.869517\n",
            "\tTraining batch 77 loss: 1.157434\n",
            "\tTraining batch 78 loss: 1.162099\n",
            "\tTraining batch 79 loss: 1.033530\n",
            "\tTraining batch 80 loss: 0.625288\n",
            "\tTraining batch 81 loss: 1.006114\n",
            "\tTraining batch 82 loss: 0.780191\n",
            "\tTraining batch 83 loss: 0.998062\n",
            "\tTraining batch 84 loss: 1.166393\n",
            "\tTraining batch 85 loss: 0.993155\n",
            "\tTraining batch 86 loss: 0.787461\n",
            "\tTraining batch 87 loss: 0.983049\n",
            "\tTraining batch 88 loss: 1.388792\n",
            "\tTraining batch 89 loss: 1.008609\n",
            "\tTraining batch 90 loss: 1.098812\n",
            "\tTraining batch 91 loss: 0.877713\n",
            "\tTraining batch 92 loss: 0.823890\n",
            "\tTraining batch 93 loss: 1.163728\n",
            "\tTraining batch 94 loss: 0.989037\n",
            "\tTraining batch 95 loss: 0.912976\n",
            "\tTraining batch 96 loss: 0.902048\n",
            "\tTraining batch 97 loss: 1.241105\n",
            "\tTraining batch 98 loss: 0.996584\n",
            "\tTraining batch 99 loss: 1.163279\n",
            "\tTraining batch 100 loss: 0.945909\n",
            "\tTraining batch 101 loss: 0.995240\n",
            "\tTraining batch 102 loss: 1.044576\n",
            "\tTraining batch 103 loss: 1.028769\n",
            "\tTraining batch 104 loss: 0.984283\n",
            "\tTraining batch 105 loss: 1.149426\n",
            "\tTraining batch 106 loss: 0.927969\n",
            "\tTraining batch 107 loss: 1.266670\n",
            "\tTraining batch 108 loss: 1.208629\n",
            "\tTraining batch 109 loss: 0.807385\n",
            "\tTraining batch 110 loss: 1.223434\n",
            "\tTraining batch 111 loss: 0.857557\n",
            "\tTraining batch 112 loss: 0.817944\n",
            "\tTraining batch 113 loss: 1.471567\n",
            "\tTraining batch 114 loss: 0.994816\n",
            "\tTraining batch 115 loss: 0.845324\n",
            "\tTraining batch 116 loss: 0.917945\n",
            "\tTraining batch 117 loss: 1.027995\n",
            "\tTraining batch 118 loss: 0.824892\n",
            "\tTraining batch 119 loss: 1.106756\n",
            "\tTraining batch 120 loss: 1.054608\n",
            "\tTraining batch 121 loss: 0.680387\n",
            "\tTraining batch 122 loss: 0.880314\n",
            "\tTraining batch 123 loss: 0.977772\n",
            "\tTraining batch 124 loss: 0.797402\n",
            "\tTraining batch 125 loss: 1.211923\n",
            "\tTraining batch 126 loss: 0.893551\n",
            "\tTraining batch 127 loss: 1.246561\n",
            "\tTraining batch 128 loss: 1.189122\n",
            "\tTraining batch 129 loss: 0.843466\n",
            "\tTraining batch 130 loss: 1.044973\n",
            "\tTraining batch 131 loss: 1.089152\n",
            "\tTraining batch 132 loss: 0.771525\n",
            "\tTraining batch 133 loss: 0.988402\n",
            "\tTraining batch 134 loss: 1.093851\n",
            "\tTraining batch 135 loss: 1.254643\n",
            "\tTraining batch 136 loss: 1.147844\n",
            "\tTraining batch 137 loss: 1.165368\n",
            "\tTraining batch 138 loss: 0.859258\n",
            "\tTraining batch 139 loss: 0.947970\n",
            "\tTraining batch 140 loss: 1.009933\n",
            "\tTraining batch 141 loss: 1.100625\n",
            "\tTraining batch 142 loss: 0.957373\n",
            "\tTraining batch 143 loss: 1.065759\n",
            "\tTraining batch 144 loss: 1.065168\n",
            "\tTraining batch 145 loss: 1.191592\n",
            "\tTraining batch 146 loss: 1.349150\n",
            "\tTraining batch 147 loss: 1.335950\n",
            "\tTraining batch 148 loss: 0.845544\n",
            "\tTraining batch 149 loss: 0.636850\n",
            "\tTraining batch 150 loss: 0.691661\n",
            "\tTraining batch 151 loss: 0.783792\n",
            "\tTraining batch 152 loss: 0.896578\n",
            "\tTraining batch 153 loss: 1.210782\n",
            "\tTraining batch 154 loss: 1.095862\n",
            "\tTraining batch 155 loss: 0.933976\n",
            "\tTraining batch 156 loss: 0.766508\n",
            "\tTraining batch 157 loss: 0.655734\n",
            "\tTraining batch 158 loss: 0.860093\n",
            "\tTraining batch 159 loss: 0.796010\n",
            "\tTraining batch 160 loss: 0.926484\n",
            "\tTraining batch 161 loss: 0.753493\n",
            "\tTraining batch 162 loss: 1.001839\n",
            "\tTraining batch 163 loss: 0.804233\n",
            "\tTraining batch 164 loss: 1.093539\n",
            "\tTraining batch 165 loss: 0.762972\n",
            "\tTraining batch 166 loss: 0.844989\n",
            "\tTraining batch 167 loss: 0.825532\n",
            "\tTraining batch 168 loss: 1.031829\n",
            "\tTraining batch 169 loss: 1.119612\n",
            "\tTraining batch 170 loss: 0.974870\n",
            "\tTraining batch 171 loss: 1.042573\n",
            "\tTraining batch 172 loss: 0.980089\n",
            "\tTraining batch 173 loss: 1.095550\n",
            "\tTraining batch 174 loss: 0.932285\n",
            "\tTraining batch 175 loss: 0.956255\n",
            "\tTraining batch 176 loss: 1.051156\n",
            "\tTraining batch 177 loss: 1.214541\n",
            "\tTraining batch 178 loss: 0.779365\n",
            "\tTraining batch 179 loss: 0.983115\n",
            "\tTraining batch 180 loss: 1.293432\n",
            "\tTraining batch 181 loss: 1.029166\n",
            "\tTraining batch 182 loss: 1.095164\n",
            "\tTraining batch 183 loss: 1.018503\n",
            "\tTraining batch 184 loss: 1.137801\n",
            "\tTraining batch 185 loss: 0.758554\n",
            "\tTraining batch 186 loss: 1.161654\n",
            "\tTraining batch 187 loss: 0.909895\n",
            "\tTraining batch 188 loss: 0.699585\n",
            "\tTraining batch 189 loss: 0.891818\n",
            "\tTraining batch 190 loss: 1.212989\n",
            "\tTraining batch 191 loss: 1.036456\n",
            "\tTraining batch 192 loss: 1.336203\n",
            "\tTraining batch 193 loss: 1.143685\n",
            "\tTraining batch 194 loss: 0.792966\n",
            "\tTraining batch 195 loss: 1.263937\n",
            "\tTraining batch 196 loss: 1.618572\n",
            "\tTraining batch 197 loss: 1.124989\n",
            "\tTraining batch 198 loss: 1.100886\n",
            "\tTraining batch 199 loss: 1.286316\n",
            "\tTraining batch 200 loss: 1.069375\n",
            "\tTraining batch 201 loss: 1.837507\n",
            "\tTraining batch 202 loss: 1.392221\n",
            "\tTraining batch 203 loss: 1.714807\n",
            "\tTraining batch 204 loss: 1.337728\n",
            "\tTraining batch 205 loss: 1.469655\n",
            "\tTraining batch 206 loss: 1.272034\n",
            "\tTraining batch 207 loss: 1.422108\n",
            "\tTraining batch 208 loss: 1.287527\n",
            "\tTraining batch 209 loss: 1.208396\n",
            "\tTraining batch 210 loss: 1.146798\n",
            "\tTraining batch 211 loss: 1.243885\n",
            "\tTraining batch 212 loss: 1.230724\n",
            "\tTraining batch 213 loss: 1.341107\n",
            "\tTraining batch 214 loss: 1.360624\n",
            "\tTraining batch 215 loss: 1.198357\n",
            "\tTraining batch 216 loss: 0.980636\n",
            "\tTraining batch 217 loss: 1.014070\n",
            "\tTraining batch 218 loss: 1.522617\n",
            "\tTraining batch 219 loss: 1.373521\n",
            "\tTraining batch 220 loss: 1.494826\n",
            "\tTraining batch 221 loss: 1.040836\n",
            "\tTraining batch 222 loss: 1.333852\n",
            "\tTraining batch 223 loss: 1.199942\n",
            "\tTraining batch 224 loss: 1.233906\n",
            "\tTraining batch 225 loss: 1.216747\n",
            "\tTraining batch 226 loss: 1.314220\n",
            "\tTraining batch 227 loss: 1.545152\n",
            "\tTraining batch 228 loss: 1.072839\n",
            "\tTraining batch 229 loss: 1.089072\n",
            "\tTraining batch 230 loss: 1.095398\n",
            "\tTraining batch 231 loss: 1.678366\n",
            "\tTraining batch 232 loss: 1.417333\n",
            "\tTraining batch 233 loss: 1.508285\n",
            "\tTraining batch 234 loss: 1.126345\n",
            "\tTraining batch 235 loss: 1.592266\n",
            "\tTraining batch 236 loss: 0.953201\n",
            "\tTraining batch 237 loss: 1.128571\n",
            "\tTraining batch 238 loss: 1.380995\n",
            "\tTraining batch 239 loss: 1.566381\n",
            "\tTraining batch 240 loss: 1.123525\n",
            "\tTraining batch 241 loss: 1.479248\n",
            "\tTraining batch 242 loss: 0.952320\n",
            "\tTraining batch 243 loss: 1.171744\n",
            "\tTraining batch 244 loss: 1.092995\n",
            "\tTraining batch 245 loss: 1.586003\n",
            "\tTraining batch 246 loss: 1.256093\n",
            "\tTraining batch 247 loss: 1.408647\n",
            "\tTraining batch 248 loss: 1.196531\n",
            "\tTraining batch 249 loss: 1.335516\n",
            "\tTraining batch 250 loss: 1.260131\n",
            "\tTraining batch 251 loss: 0.956406\n",
            "\tTraining batch 252 loss: 1.297969\n",
            "\tTraining batch 253 loss: 1.267577\n",
            "\tTraining batch 254 loss: 1.290730\n",
            "\tTraining batch 255 loss: 1.498010\n",
            "\tTraining batch 256 loss: 1.312101\n",
            "\tTraining batch 257 loss: 1.521336\n",
            "\tTraining batch 258 loss: 1.265936\n",
            "\tTraining batch 259 loss: 1.048634\n",
            "\tTraining batch 260 loss: 1.344589\n",
            "\tTraining batch 261 loss: 1.335386\n",
            "\tTraining batch 262 loss: 1.256924\n",
            "\tTraining batch 263 loss: 1.313966\n",
            "\tTraining batch 264 loss: 1.441295\n",
            "\tTraining batch 265 loss: 1.515414\n",
            "\tTraining batch 266 loss: 1.139145\n",
            "\tTraining batch 267 loss: 1.266859\n",
            "\tTraining batch 268 loss: 1.536384\n",
            "\tTraining batch 269 loss: 1.537250\n",
            "\tTraining batch 270 loss: 0.926322\n",
            "\tTraining batch 271 loss: 1.138994\n",
            "\tTraining batch 272 loss: 1.462121\n",
            "\tTraining batch 273 loss: 1.200271\n",
            "\tTraining batch 274 loss: 1.509503\n",
            "\tTraining batch 275 loss: 0.992842\n",
            "\tTraining batch 276 loss: 1.401450\n",
            "\tTraining batch 277 loss: 1.105846\n",
            "\tTraining batch 278 loss: 1.181263\n",
            "\tTraining batch 279 loss: 1.035624\n",
            "\tTraining batch 280 loss: 1.306567\n",
            "\tTraining batch 281 loss: 1.237075\n",
            "\tTraining batch 282 loss: 0.876622\n",
            "\tTraining batch 283 loss: 1.332651\n",
            "\tTraining batch 284 loss: 1.218983\n",
            "\tTraining batch 285 loss: 1.303083\n",
            "\tTraining batch 286 loss: 1.737162\n",
            "\tTraining batch 287 loss: 1.102229\n",
            "\tTraining batch 288 loss: 1.289830\n",
            "\tTraining batch 289 loss: 1.193772\n",
            "\tTraining batch 290 loss: 1.245604\n",
            "\tTraining batch 291 loss: 1.265916\n",
            "\tTraining batch 292 loss: 1.083257\n",
            "\tTraining batch 293 loss: 1.036840\n",
            "\tTraining batch 294 loss: 1.402273\n",
            "\tTraining batch 295 loss: 1.442112\n",
            "\tTraining batch 296 loss: 0.907014\n",
            "\tTraining batch 297 loss: 1.245631\n",
            "\tTraining batch 298 loss: 1.080006\n",
            "\tTraining batch 299 loss: 1.164092\n",
            "\tTraining batch 300 loss: 1.229177\n",
            "\tTraining batch 301 loss: 1.309732\n",
            "\tTraining batch 302 loss: 1.122657\n",
            "\tTraining batch 303 loss: 1.150438\n",
            "\tTraining batch 304 loss: 1.263866\n",
            "\tTraining batch 305 loss: 1.120252\n",
            "\tTraining batch 306 loss: 1.164974\n",
            "\tTraining batch 307 loss: 1.024100\n",
            "\tTraining batch 308 loss: 1.318923\n",
            "\tTraining batch 309 loss: 1.302067\n",
            "\tTraining batch 310 loss: 0.795240\n",
            "\tTraining batch 311 loss: 1.271186\n",
            "\tTraining batch 312 loss: 0.905614\n",
            "\tTraining batch 313 loss: 1.168581\n",
            "\tTraining batch 314 loss: 1.011029\n",
            "\tTraining batch 315 loss: 1.169764\n",
            "\tTraining batch 316 loss: 1.285805\n",
            "\tTraining batch 317 loss: 1.035657\n",
            "\tTraining batch 318 loss: 1.035750\n",
            "\tTraining batch 319 loss: 1.273131\n",
            "\tTraining batch 320 loss: 1.205892\n",
            "\tTraining batch 321 loss: 0.954254\n",
            "\tTraining batch 322 loss: 1.187445\n",
            "\tTraining batch 323 loss: 0.838913\n",
            "\tTraining batch 324 loss: 0.968042\n",
            "\tTraining batch 325 loss: 0.864227\n",
            "\tTraining batch 326 loss: 1.193894\n",
            "\tTraining batch 327 loss: 1.212911\n",
            "\tTraining batch 328 loss: 1.170833\n",
            "\tTraining batch 329 loss: 1.159297\n",
            "\tTraining batch 330 loss: 1.198032\n",
            "\tTraining batch 331 loss: 0.824246\n",
            "\tTraining batch 332 loss: 0.871617\n",
            "\tTraining batch 333 loss: 0.918795\n",
            "\tTraining batch 334 loss: 0.957227\n",
            "\tTraining batch 335 loss: 0.688336\n",
            "\tTraining batch 336 loss: 1.106825\n",
            "\tTraining batch 337 loss: 0.767967\n",
            "\tTraining batch 338 loss: 1.090989\n",
            "\tTraining batch 339 loss: 0.735103\n",
            "\tTraining batch 340 loss: 1.140710\n",
            "\tTraining batch 341 loss: 0.854710\n",
            "\tTraining batch 342 loss: 1.418540\n",
            "\tTraining batch 343 loss: 0.592602\n",
            "\tTraining batch 344 loss: 0.859411\n",
            "\tTraining batch 345 loss: 0.939572\n",
            "\tTraining batch 346 loss: 1.003151\n",
            "\tTraining batch 347 loss: 0.961142\n",
            "\tTraining batch 348 loss: 1.220606\n",
            "\tTraining batch 349 loss: 0.957207\n",
            "\tTraining batch 350 loss: 1.059280\n",
            "\tTraining batch 351 loss: 0.976355\n",
            "\tTraining batch 352 loss: 1.058059\n",
            "\tTraining batch 353 loss: 0.881677\n",
            "\tTraining batch 354 loss: 0.725568\n",
            "\tTraining batch 355 loss: 0.912497\n",
            "\tTraining batch 356 loss: 1.371960\n",
            "\tTraining batch 357 loss: 1.162526\n",
            "\tTraining batch 358 loss: 0.746228\n",
            "\tTraining batch 359 loss: 0.864365\n",
            "\tTraining batch 360 loss: 1.024320\n",
            "\tTraining batch 361 loss: 0.866827\n",
            "\tTraining batch 362 loss: 0.833125\n",
            "\tTraining batch 363 loss: 0.915704\n",
            "\tTraining batch 364 loss: 1.017515\n",
            "\tTraining batch 365 loss: 1.163965\n",
            "\tTraining batch 366 loss: 1.000051\n",
            "\tTraining batch 367 loss: 0.909889\n",
            "\tTraining batch 368 loss: 1.004127\n",
            "\tTraining batch 369 loss: 1.051537\n",
            "\tTraining batch 370 loss: 1.017713\n",
            "\tTraining batch 371 loss: 0.944842\n",
            "\tTraining batch 372 loss: 1.178970\n",
            "\tTraining batch 373 loss: 1.085238\n",
            "\tTraining batch 374 loss: 0.898231\n",
            "\tTraining batch 375 loss: 0.936250\n",
            "\tTraining batch 376 loss: 0.978428\n",
            "\tTraining batch 377 loss: 1.006921\n",
            "\tTraining batch 378 loss: 1.009799\n",
            "\tTraining batch 379 loss: 1.206713\n",
            "\tTraining batch 380 loss: 1.000498\n",
            "\tTraining batch 381 loss: 0.892137\n",
            "\tTraining batch 382 loss: 0.829379\n",
            "\tTraining batch 383 loss: 1.011482\n",
            "\tTraining batch 384 loss: 0.889709\n",
            "\tTraining batch 385 loss: 0.946801\n",
            "\tTraining batch 386 loss: 0.676657\n",
            "\tTraining batch 387 loss: 0.940223\n",
            "\tTraining batch 388 loss: 1.087644\n",
            "\tTraining batch 389 loss: 1.099045\n",
            "\tTraining batch 390 loss: 1.185821\n",
            "\tTraining batch 391 loss: 0.988066\n",
            "\tTraining batch 392 loss: 0.826218\n",
            "\tTraining batch 393 loss: 0.989592\n",
            "\tTraining batch 394 loss: 0.776403\n",
            "\tTraining batch 395 loss: 0.659631\n",
            "\tTraining batch 396 loss: 1.022499\n",
            "\tTraining batch 397 loss: 1.127080\n",
            "\tTraining batch 398 loss: 1.288659\n",
            "\tTraining batch 399 loss: 0.996284\n",
            "\tTraining batch 400 loss: 1.449836\n",
            "\tTraining batch 401 loss: 1.188218\n",
            "\tTraining batch 402 loss: 0.785638\n",
            "\tTraining batch 403 loss: 1.230118\n",
            "\tTraining batch 404 loss: 0.973228\n",
            "\tTraining batch 405 loss: 0.976931\n",
            "\tTraining batch 406 loss: 0.910966\n",
            "\tTraining batch 407 loss: 0.924493\n",
            "\tTraining batch 408 loss: 1.262714\n",
            "\tTraining batch 409 loss: 1.005544\n",
            "\tTraining batch 410 loss: 1.093362\n",
            "\tTraining batch 411 loss: 0.748799\n",
            "\tTraining batch 412 loss: 1.270971\n",
            "\tTraining batch 413 loss: 0.951273\n",
            "\tTraining batch 414 loss: 1.139709\n",
            "\tTraining batch 415 loss: 0.973585\n",
            "\tTraining batch 416 loss: 1.008659\n",
            "\tTraining batch 417 loss: 1.254707\n",
            "\tTraining batch 418 loss: 1.082199\n",
            "\tTraining batch 419 loss: 1.021176\n",
            "\tTraining batch 420 loss: 0.973153\n",
            "\tTraining batch 421 loss: 1.131173\n",
            "\tTraining batch 422 loss: 0.822150\n",
            "\tTraining batch 423 loss: 0.520858\n",
            "Training set: Average loss: 1.079829\n",
            "Validation set: Average loss: 0.844023, Accuracy: 3743/5276 (71%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "plt.plot(epoch_nums, training_loss)\n",
        "plt.plot(epoch_nums, validation_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "id": "9Mj1F5yKHT4y",
        "outputId": "a082877a-391c-4ac5-d6c0-87b99b3c9479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "truelabels = []\n",
        "predictions = []\n",
        "model.eval()\n",
        "for data, target in test_loader:\n",
        "  for label in target.data.numpy():\n",
        "    truelabels.append(label)\n",
        "  for prediction in model(data).data.numpy().argmax(1):\n",
        "    predictions.append(prediction)\n",
        "\n",
        "cm = confusion_matrix(truelabels, predictions)\n",
        "tick_marks = np.arange(len(classes))\n",
        "\n",
        "df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n",
        "plt.figure(figsize = (7,7))\n",
        "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n",
        "plt.xlabel(\"Predicted Shape\", fontsize = 20)\n",
        "plt.ylabel(\"True Shape\", fontsize = 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "yHHwxWYDDwQY",
        "outputId": "5e318c22-12a2-404c-fad5-c64c669db6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_pred=predictions, y_true=truelabels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aHh1i6-E6Mp",
        "outputId": "c4bc6a1c-5205-4c2a-fecc-17cb69762017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7130401819560273"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "krihastyMz0D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}